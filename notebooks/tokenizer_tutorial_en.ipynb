{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5edc60c8-150d-4f94-8616-067147afcb0a",
   "metadata": {
    "id": "5edc60c8-150d-4f94-8616-067147afcb0a"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/may-/joeynmt/blob/main/notebooks/tokenizer_tutorial_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "# JoeyNMT v2 tokenizer tutorial\n",
    "\n",
    "In this notebook, we explain how to integrate a new tokenizer to JoeyNMT.\n",
    "\n",
    "Author: Mayumi Ohta  \n",
    "Date: 21. July 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce9effd-ce63-4a8a-ab79-eee11dae2dbb",
   "metadata": {
    "id": "8ce9effd-ce63-4a8a-ab79-eee11dae2dbb"
   },
   "source": [
    "> :warning: **important:** Before you start, set runtime type to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff94d15-8b31-498e-a56a-007279124738",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dff94d15-8b31-498e-a56a-007279124738",
    "outputId": "6fac3b8c-06a4-4189-e9d3-bee6a52653d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 16 07:11:40 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   42C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d3011-d6d9-4833-bf79-3fad1f50fc80",
   "metadata": {
    "id": "9a6d3011-d6d9-4833-bf79-3fad1f50fc80"
   },
   "source": [
    "Make sure that you have a compatible PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d09165-32b7-4e73-b622-f8c42b0cfcc0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "02d09165-32b7-4e73-b622-f8c42b0cfcc0",
    "outputId": "6b8dec83-a2a3-4100-961e-41dc611040dc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.12.1+cu113'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a3338-aef6-4658-ad9b-e3c87bb0ab17",
   "metadata": {
    "id": "150a3338-aef6-4658-ad9b-e3c87bb0ab17"
   },
   "source": [
    "Mount your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c232f9d-c5fc-409c-9743-517bab88de63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c232f9d-c5fc-409c-9743-517bab88de63",
    "outputId": "ea8dd689-b19e-4e05-c14a-53070c2a9afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76163cbc-f80b-4cae-9f3a-d6519bab05c1",
   "metadata": {
    "id": "76163cbc-f80b-4cae-9f3a-d6519bab05c1"
   },
   "source": [
    "Set root dir path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d8b2980-db19-4f3b-ba02-7d515fa508d0",
   "metadata": {
    "id": "4d8b2980-db19-4f3b-ba02-7d515fa508d0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "root_dir = '/content/drive/MyDrive' # for Google Colab\n",
    "#root_dir = os.environ[\"WORK_DIR\"] # '/home/studio-lab-user' for Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc8b4c4a-cac6-4e60-8778-825c5dfcf7c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc8b4c4a-cac6-4e60-8778-825c5dfcf7c8",
    "outputId": "32a301db-b066-48a3-d9b9-335f804bd448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_DATASETS_CACHE=/content/drive/MyDrive/.cache\n"
     ]
    }
   ],
   "source": [
    "%env HF_DATASETS_CACHE={root_dir}/.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11646367-bf49-44e7-aa57-48626856b1c2",
   "metadata": {
    "id": "11646367-bf49-44e7-aa57-48626856b1c2"
   },
   "source": [
    "Install joeynmt for tokenizer integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba4d9a30-9c9a-4919-b9b1-540a5422feaa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba4d9a30-9c9a-4919-b9b1-540a5422feaa",
    "outputId": "b1862150-15d7-4773-9a4b-c3c2bfed2b24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 1.0 MB 4.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 116 kB 71.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 55.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 596 kB 60.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 488 kB 69.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 190 kB 72.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 61 kB 561 kB/s \n",
      "\u001b[K     |████████████████████████████████| 365 kB 76.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 212 kB 68.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 115 kB 74.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 141 kB 72.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 101 kB 15.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 127 kB 71.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 66 kB 5.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 61 kB 602 kB/s \n",
      "\u001b[K     |████████████████████████████████| 61 kB 601 kB/s \n",
      "\u001b[K     |████████████████████████████████| 61 kB 370 kB/s \n",
      "\u001b[K     |████████████████████████████████| 61 kB 375 kB/s \n",
      "\u001b[K     |████████████████████████████████| 64 kB 3.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 64 kB 3.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 73 kB 2.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 68 kB 9.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 41 kB 817 kB/s \n",
      "\u001b[K     |████████████████████████████████| 251 kB 71.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 103 kB 76.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 843 kB 54.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 57 kB 6.7 MB/s \n",
      "\u001b[?25h  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.8.2+zzzcolab20220719082949 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/may-/joeynmt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YVXwRyMzCn93",
   "metadata": {
    "id": "YVXwRyMzCn93"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff545798-786d-427e-b4ef-625aed9f3587",
   "metadata": {
    "id": "ff545798-786d-427e-b4ef-625aed9f3587"
   },
   "source": [
    "## fastBPE\n",
    "\n",
    "JoeyNMT v2 contains `subword-nmt` and `sentencepiece` subword tokenizers. What should we do, when we want to use a different tokenizer?\n",
    "\n",
    "Let's implement [fastBPE](https://github.com/glample/fastBPE) tokenizer, for example.\n",
    "\n",
    "First, install fastBPE python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c9622d-4e9f-45b9-9d41-9426828490d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51c9622d-4e9f-45b9-9d41-9426828490d4",
    "outputId": "0084ddd5-1eff-48c6-9097-a650f27ad3f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 880 kB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Building wheel for fastbpe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q fastbpe sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd098ad6-2629-4f46-a747-673f25fac6f3",
   "metadata": {
    "id": "bd098ad6-2629-4f46-a747-673f25fac6f3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecff22d-f7e5-4f48-bd6c-7bf16dec0ede",
   "metadata": {
    "id": "4ecff22d-f7e5-4f48-bd6c-7bf16dec0ede"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06346609-64d1-44eb-8dd1-2f15c5a2a78b",
   "metadata": {
    "id": "06346609-64d1-44eb-8dd1-2f15c5a2a78b"
   },
   "source": [
    "The tokenizers are defined in `joeynmt/tokenizers.py`. We add a new class for fastBPE here.\n",
    "\n",
    "In principle, you can inherit `BasicTokenizer` class and override `__call__()` function to tokenize and `post_process()` function to detokenize.\n",
    "\n",
    "fastBPE is a library which implements subword-nmt algorithms in C++. So, we inherit here `SubwordNMTTokenizer` class, instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b3ab9-7ec4-4d29-9b76-8de0a180d2c0",
   "metadata": {
    "id": "df6b3ab9-7ec4-4d29-9b76-8de0a180d2c0"
   },
   "outputs": [],
   "source": [
    "from joeynmt.tokenizers import BasicTokenizer, SubwordNMTTokenizer\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "class FastBPETokenizer(SubwordNMTTokenizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        level: str = \"bpe\",\n",
    "        lowercase: bool = False,\n",
    "        normalize: bool = False,\n",
    "        max_length: int = -1,\n",
    "        min_length: int = -1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        try:\n",
    "            import fastBPE\n",
    "        except ImportError as e:\n",
    "            #logger.error(e)\n",
    "            raise ImportError from e\n",
    "        super(SubwordNMTTokenizer, self).__init__(level, lowercase, normalize, max_length, min_length, **kwargs)\n",
    "        assert self.level == \"bpe\"\n",
    "\n",
    "        # get codes file path\n",
    "        self.codes: Path = Path(kwargs[\"codes\"])\n",
    "        assert self.codes.is_file(), f\"codes file {self.codes} not found.\"\n",
    "\n",
    "        # instantiate fastBPE object\n",
    "        self.bpe = fastBPE.fastBPE(self.codes.as_posix())\n",
    "        self.separator = \"@@\"\n",
    "        self.dropout = 0.0\n",
    "\n",
    "    def __call__(self, raw_input: str, is_train: bool = False) -> List[str]:\n",
    "        # fastBPE.apply()\n",
    "        tokenized = self.bpe.apply([raw_input])\n",
    "        tokenized = tokenized[0].strip().split()\n",
    "\n",
    "        # check if the input sequence length stays within the valid length range\n",
    "        if is_train and self._filter_by_length(len(tokenized)):\n",
    "            return None\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97316c77-bdec-4dc1-8bad-9ab00f949121",
   "metadata": {
    "id": "97316c77-bdec-4dc1-8bad-9ab00f949121"
   },
   "source": [
    "The `FastBPETokenizer` class defined above will be instantiated in `_build_tokenizer()` function.\n",
    "\n",
    "Change `_build_tokenizer()` function so that `FastBPETokenizer` class will be called when config file specifies `tokenizer_type: \"fastbpe\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ba19a-10f1-4b7f-a882-22e5b4750285",
   "metadata": {
    "id": "072ba19a-10f1-4b7f-a882-22e5b4750285"
   },
   "outputs": [],
   "source": [
    "def _build_tokenizer(cfg):\n",
    "    # [...]\n",
    "    tokenizer_cfg = cfg.get(\"tokenizer_cfg\", {})\n",
    "    \n",
    "    if cfg[\"level\"] in [\"word\", \"char\"]:\n",
    "        tokenizer = BasicTokenizer(\n",
    "            # [...]\n",
    "        )\n",
    "        \n",
    "    elif cfg[\"level\"] == \"bpe\":\n",
    "        tokenizer_type = cfg.get(\"tokenizer_type\", cfg.get(\"bpe_type\", \"sentencepiece\"))\n",
    "        if tokenizer_type == \"sentencepiece\":\n",
    "            assert \"model_file\" in tokenizer_cfg\n",
    "            # [...]\n",
    "        elif tokenizer_type == \"subword-nmt\":\n",
    "            assert \"codes\" in tokenizer_cfg\n",
    "            # [...]\n",
    "        elif tokenizer_type == \"fastbpe\":\n",
    "            assert \"codes\" in tokenizer_cfg\n",
    "            tokenizer = FastBPETokenizer(\n",
    "                level=cfg[\"level\"],\n",
    "                lowercase=cfg.get(\"lowercase\", False),\n",
    "                normalize=cfg.get(\"normalize\", False),\n",
    "                max_length=cfg.get(\"max_length\", -1),\n",
    "                min_length=cfg.get(\"min_length\", -1),\n",
    "                **tokenizer_cfg,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f4fbc-cf7b-470d-ab7c-6a4fa0f69a8d",
   "metadata": {
    "id": "741f4fbc-cf7b-470d-ab7c-6a4fa0f69a8d"
   },
   "source": [
    "In the config file `config.yaml`, you can select \"fastbpe\" as follows. `codes` attribute is necessary.\n",
    "\n",
    "```yaml\n",
    "name: \"transformer_iwslt14_deen_fastbpe\"\n",
    "joeynmt_version: \"2.0.0\"\n",
    "\n",
    "data:\n",
    "    train: \"iwslt14\"\n",
    "    dev: \"iwslt14\"\n",
    "    test: \"iwslt14\"\n",
    "    dataset_type: \"huggingface\"\n",
    "    dataset_cfg:\n",
    "        name: \"de-en\"\n",
    "    src:\n",
    "        lang: \"de\"\n",
    "        max_length: 128\n",
    "        lowercase: True\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"data/iwslt14/vocab.32000\"\n",
    "        tokenizer_type: \"fastbpe\"\n",
    "        tokenizer_cfg:\n",
    "            codes: \"data/iwslt14/bpe.32000\"  # necessary\n",
    "            pretokenizer: \"moses\"\n",
    "    trg:\n",
    "        lang: \"en\"\n",
    "        max_length: 128\n",
    "        lowercase: True\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"data/iwslt14/vocab.32000\"\n",
    "        tokenizer_type: \"fastbpe\"\n",
    "        tokenizer_cfg:\n",
    "            codes: \"data/iwslt14/bpe.32000\"  # necessary\n",
    "            pretokenizer: \"moses\"\n",
    "[...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e510a8e-3abc-459d-8b95-5cb0a64a5f4c",
   "metadata": {
    "id": "8e510a8e-3abc-459d-8b95-5cb0a64a5f4c"
   },
   "source": [
    "### use in pretrained models\n",
    "\n",
    "Let's check if it works on a real dataset, i.e. iwslt14 en-de.\n",
    "\n",
    "Download a pretrained model. (The codes file here was trained on subword-nmt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216547f1-df1b-43b9-8528-3f89d7eb185d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "216547f1-df1b-43b9-8528-3f89d7eb185d",
    "outputId": "8fc82fb0-9151-4c8d-f7d5-c0ae1c96e908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-15 20:15:23--  https://www.cl.uni-heidelberg.de/statnlpgroup/joeynmt2/transformer_iwslt14_deen_bpe.tar.gz\n",
      "Resolving www.cl.uni-heidelberg.de (www.cl.uni-heidelberg.de)... 147.142.207.78\n",
      "Connecting to www.cl.uni-heidelberg.de (www.cl.uni-heidelberg.de)|147.142.207.78|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 220750239 (211M) [application/x-gzip]\n",
      "Saving to: ‘/content/drive/MyDrive/transformer_iwslt14_deen_bpe.tar.gz’\n",
      "\n",
      "/content/drive/MyDr 100%[===================>] 210.52M  14.8MB/s    in 16s     \n",
      "\n",
      "2022-08-15 20:15:42 (13.0 MB/s) - ‘/content/drive/MyDrive/transformer_iwslt14_deen_bpe.tar.gz’ saved [220750239/220750239]\n",
      "\n",
      "transformer_iwslt14_deen_bpe/\n",
      "transformer_iwslt14_deen_bpe/best.ckpt\n",
      "transformer_iwslt14_deen_bpe/trg_vocab.txt\n",
      "transformer_iwslt14_deen_bpe/config_v1.yaml\n",
      "transformer_iwslt14_deen_bpe/config_v2.yaml\n",
      "transformer_iwslt14_deen_bpe/train.log\n",
      "transformer_iwslt14_deen_bpe/bpe.32000\n",
      "transformer_iwslt14_deen_bpe/test.log\n",
      "transformer_iwslt14_deen_bpe/hyp.test\n",
      "transformer_iwslt14_deen_bpe/hyp.dev\n",
      "transformer_iwslt14_deen_bpe/src_vocab.txt\n",
      "best.ckpt  config_v1.yaml  hyp.dev   src_vocab.txt  train.log\n",
      "bpe.32000  config_v2.yaml  hyp.test  test.log\t    trg_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!wget -O {root_dir}/transformer_iwslt14_deen_bpe.tar.gz https://www.cl.uni-heidelberg.de/statnlpgroup/joeynmt2/transformer_iwslt14_deen_bpe.tar.gz\n",
    "!cd {root_dir} && tar -xvf transformer_iwslt14_deen_bpe.tar.gz\n",
    "!ls {root_dir}/transformer_iwslt14_deen_bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YxpO9FIgJhix",
   "metadata": {
    "id": "YxpO9FIgJhix"
   },
   "source": [
    "Copy codes and vocab files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199904be-6707-4970-91a8-5589bd1bb144",
   "metadata": {
    "id": "199904be-6707-4970-91a8-5589bd1bb144"
   },
   "outputs": [],
   "source": [
    "!mkdir {root_dir}/data\n",
    "!mkdir {root_dir}/data/iwslt14\n",
    "!cp {root_dir}/transformer_iwslt14_deen_bpe/bpe.32000 {root_dir}/data/iwslt14/codes.32000\n",
    "!cp {root_dir}/transformer_iwslt14_deen_bpe/trg_vocab.txt {root_dir}/data/iwslt14/vocab.32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0278565-e7c3-4b45-931f-60f8fab8090c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0278565-e7c3-4b45-931f-60f8fab8090c",
    "outputId": "e0cfe609-f01d-4f68-d014-97e25ef4f1df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastBPETokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=fastBPE, separator=@@, dropout=0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastbpe_tokenizer = FastBPETokenizer(codes=f\"{root_dir}/data/iwslt14/codes.32000\")\n",
    "fastbpe_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae54431-6a06-4ecf-a34d-b009550a2eb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ae54431-6a06-4ecf-a34d-b009550a2eb5",
    "outputId": "9ce696af-54d6-4a4c-b343-73827faf8286"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T@@', 'his', 'is', 'a', 'test', '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastbpe_tokenizer(\"This is a test .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d73a0-c750-4289-91dc-a6bab6d57c15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "419d73a0-c750-4289-91dc-a6bab6d57c15",
    "outputId": "4b36914c-f3ae-435e-dd48-b9c286a00804"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D@@', 'as', 'ist', 'ein', 'B@@', 'ei@@', 'spiel', '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastbpe_tokenizer(\"Das ist ein Beispiel .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Xju9ngXKBLx",
   "metadata": {
    "id": "1Xju9ngXKBLx"
   },
   "source": [
    "Decode using fastBPE tokenizer. Specify \"fastbpe\" in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dea653-9c89-47f4-95d3-b04693c55900",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0dea653-9c89-47f4-95d3-b04693c55900",
    "outputId": "6a00ab51-ac76-41c9-dd81-5133f071c23c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2675"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_config = \"\"\"\n",
    "name: \"transformer_iwslt14_deen_fastbpe\"\n",
    "joeynmt_version: \"2.0.0\"\n",
    "\n",
    "data:\n",
    "    train: \"iwslt14\"\n",
    "    dev: \"iwslt14\"\n",
    "    test: \"iwslt14\"\n",
    "    dataset_type: \"huggingface\"\n",
    "    dataset_cfg:\n",
    "        name: \"de-en\"\n",
    "    src:\n",
    "        lang: \"de\"\n",
    "        max_length: 128\n",
    "        lowercase: True\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"data/iwslt14/vocab.32000\"\n",
    "        tokenizer_type: \"fastbpe\"\n",
    "        tokenizer_cfg:\n",
    "            codes: \"data/iwslt14/codes.32000\" #必須\n",
    "            pretokenizer: \"moses\"\n",
    "    trg:\n",
    "        lang: \"en\"\n",
    "        max_length: 128\n",
    "        lowercase: True\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"data/iwslt14/vocab.32000\"\n",
    "        tokenizer_type: \"fastbpe\"\n",
    "        tokenizer_cfg:\n",
    "            codes: \"data/iwslt14/codes.32000\" #必須\n",
    "            pretokenizer: \"moses\"\n",
    "\n",
    "testing:\n",
    "    n_best: 1\n",
    "    beam_size: 5\n",
    "    beam_alpha: 1.0\n",
    "    batch_size: 1024\n",
    "    batch_type: \"token\"\n",
    "    max_output_length: 100\n",
    "    eval_metrics: [\"bleu\"]\n",
    "    return_prob: \"none\"\n",
    "    return_attention: False\n",
    "    sacrebleu_cfg:\n",
    "        tokenize: \"13a\"\n",
    "        lowercase: True\n",
    "\n",
    "training:\n",
    "    load_model: \"transformer_iwslt14_deen_bpe/best.ckpt\"\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999]\n",
    "    scheduling: \"plateau\"\n",
    "    patience: 5\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    early_stopping_metric: \"bleu\"\n",
    "    epochs: 100\n",
    "    validation_freq: 1000\n",
    "    logging_freq: 100\n",
    "    model_dir: \"transformer_iwslt14_deen_bpe\"\n",
    "    overwrite: False\n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    print_valid_sents: [0, 1, 2, 3, 4]\n",
    "    keep_best_ckpts: 5\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier_uniform\"\n",
    "    embed_initializer: \"xavier_uniform\"\n",
    "    embed_init_gain: 1.0\n",
    "    init_gain: 1.0\n",
    "    bias_initializer: \"zeros\"\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.3\n",
    "        layer_norm: \"pre\"\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.3\n",
    "        layer_norm: \"pre\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "(Path(root_dir) / 'data/iwslt14/config.yaml').write_text(new_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZF2_N_c6KOnQ",
   "metadata": {
    "id": "ZF2_N_c6KOnQ"
   },
   "source": [
    "Generate translations in interactive mode. (Press [Ctrl]+[C] to stop the cell execution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10343372-a8ab-4f5c-90d5-34aa8ca6105b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10343372-a8ab-4f5c-90d5-34aa8ca6105b",
    "outputId": "6f550576-476b-47fb-882e-72448800724e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-15 20:31:51,569 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-08-15 20:32:06,238 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-15 20:32:06,480 - WARNING - joeynmt.initialization - `xavier` option is obsolete. Please use `xavier_uniform`, instead.\n",
      "2022-08-15 20:32:06,480 - WARNING - joeynmt.initialization - `xavier` option is obsolete. Please use `xavier_uniform`, instead.\n",
      "2022-08-15 20:32:06,605 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-15 20:32:07,196 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/transformer_iwslt14_deen_bpe/best.ckpt.\n",
      "Loading codes from data/iwslt14/codes.32000 ...\n",
      "Read 32001 codes from the codes file.\n",
      "Loading codes from data/iwslt14/codes.32000 ...\n",
      "Read 32001 codes from the codes file.\n",
      "2022-08-15 20:32:09,415 - INFO - joeynmt.tokenizers - de tokenizer: FastBPETokenizer(level=bpe, lowercase=True, normalize=False, filter_by_length=(-1, 128), pretokenizer=moses, tokenizer=fastBPE, separator=@@, dropout=0.0)\n",
      "2022-08-15 20:32:09,415 - INFO - joeynmt.tokenizers - en tokenizer: FastBPETokenizer(level=bpe, lowercase=True, normalize=False, filter_by_length=(-1, 128), pretokenizer=moses, tokenizer=fastBPE, separator=@@, dropout=0.0)\n",
      "\n",
      "Please enter a source sentence:\n",
      "Maschinelle Übersetzung macht Spaß!\n",
      "2022-08-15 20:32:36,812 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 20:32:39,412 - INFO - joeynmt.prediction - Generation took 2.5870[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: machine translation is fun!\n",
      "\n",
      "Please enter a source sentence:\n",
      "Wann macht maschinelle Übersetzung Sinn?\n",
      "2022-08-15 20:33:03,723 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 20:33:03,813 - INFO - joeynmt.prediction - Generation took 0.0876[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: when does machine translation make sense?\n",
      "\n",
      "Please enter a source sentence:\n",
      "\n",
      "Bye.Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/prediction.py\", line 557, in translate\n",
      "    src_input = input(\"\\nPlease enter a source sentence:\\n\")\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/__main__.py\", line 64, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/__main__.py\", line 57, in main\n",
      "    output_path=args.output_path,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/prediction.py\", line 581, in translate\n",
      "    print(\"\\nBye.\")\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cd {root_dir} && python -m joeynmt translate data/iwslt14/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd6350-8368-48e1-ae60-d081c67f750f",
   "metadata": {
    "id": "cdbd6350-8368-48e1-ae60-d081c67f750f"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec686022-55ae-436b-b77a-c8c2eb736971",
   "metadata": {
    "id": "ec686022-55ae-436b-b77a-c8c2eb736971"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947a672-d73e-40de-a50c-07d296027492",
   "metadata": {
    "id": "6947a672-d73e-40de-a50c-07d296027492"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e73b3-c54e-4092-8522-3acf0d8a12d7",
   "metadata": {
    "id": "9b6e73b3-c54e-4092-8522-3acf0d8a12d7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b360e7-c50b-42c8-824f-3ded7173b23d",
   "metadata": {
    "id": "16b360e7-c50b-42c8-824f-3ded7173b23d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c50f77-1252-4010-8ce3-f6c97e78aeb0",
   "metadata": {
    "id": "c7c50f77-1252-4010-8ce3-f6c97e78aeb0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce06a850-a137-4819-824f-d6af278dd594",
   "metadata": {
    "id": "ce06a850-a137-4819-824f-d6af278dd594"
   },
   "source": [
    "### use in training from scratch\n",
    "\n",
    "In this section, we demonstrate how to use a new tokenizer in training from scratch.\n",
    "\n",
    "First, we need to prepare codes file. We train a tokenizer model using fastBPE's `fast` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc618bdb-f249-403b-8cf8-29d50f6b5a32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc618bdb-f249-403b-8cf8-29d50f6b5a32",
    "outputId": "58254c58-354d-40b2-9fd4-8e472d87de70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: fastbpe <command> <args>\n",
      "\n",
      "The commands supported by fastBPE are:\n",
      "\n",
      "getvocab input1 [input2]             extract the vocabulary from one or two text files\n",
      "learnbpe nCodes input1 [input2]      learn BPE codes from one or two text files\n",
      "applybpe output input codes [vocab]  apply BPE codes to a text file\n",
      "applybpe_stream codes [vocab]        apply BPE codes to stdin and output to stdout\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/glample/fastBPE.git {root_dir}/fastBPE\n",
    "!cd {root_dir}/fastBPE && g++ -std=c++11 -pthread -O3 fastBPE/main.cc -IfastBPE -o fast\n",
    "!cd {root_dir}/fastBPE && ./fast -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e29244-553d-4dce-a266-727e52815f14",
   "metadata": {
    "id": "63e29244-553d-4dce-a266-727e52815f14"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "QGSprnObN0et",
   "metadata": {
    "id": "QGSprnObN0et"
   },
   "source": [
    "We use iwslt14 en-de dataset for this purpose.\n",
    "\n",
    "Download the script and call it from huggingface's datasets package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f791d484-2f28-4d58-967e-be07074e44be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f791d484-2f28-4d58-967e-be07074e44be",
    "outputId": "1ac13e0f-a80c-4c96-f952-b62ba21e6d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-15 20:34:59--  https://raw.githubusercontent.com/may-/datasets/master/datasets/iwslt14/iwslt14.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7614 (7.4K) [text/plain]\n",
      "Saving to: ‘/content/drive/MyDrive/data/iwslt14/iwslt14.py’\n",
      "\n",
      "/content/drive/MyDr 100%[===================>]   7.44K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2022-08-15 20:34:59 (5.56 MB/s) - ‘/content/drive/MyDrive/data/iwslt14/iwslt14.py’ saved [7614/7614]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O {root_dir}/data/iwslt14/iwslt14.py https://raw.githubusercontent.com/may-/datasets/master/datasets/iwslt14/iwslt14.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5dcf34-2da7-4d3a-90fa-514f72d94afe",
   "metadata": {
    "id": "6f5dcf34-2da7-4d3a-90fa-514f72d94afe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "avZQGyvUN7yg",
   "metadata": {
    "id": "avZQGyvUN7yg"
   },
   "source": [
    "Before subword tokenization, we pretokenize the text by MosesTokenizer in order to handle punctuations better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55890b99-9ffb-4360-ac02-e4b6cfc73354",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240,
     "referenced_widgets": [
      "c1f9fa74a4284eb388375f2479fa64c4",
      "298eaf75350e4a918dc7e859ada452b3",
      "587548fc703142a78f10a394ceb37597",
      "1f6be19d0d4a48118220d1a880d0745d",
      "d8cfbf6182774960a9b1024519afc926",
      "9fa81f2f29ef462bb7114509546e3fc4",
      "16bec58461cd4974ab503fc844c407b7",
      "db082aaeff6e4f9db14c580993e859f4",
      "e99eafed62824d1aa6cc17c5c3f6a8c6",
      "f6c2958faa4e45c784eb60165f674a71",
      "417b45ffccbf4fb5874c056d7ba30b5c",
      "2f5a3572fbc7412190a0be7158be3988",
      "5841ded11a864b6e87863f6e545ffa23",
      "2128acc04eab49a6a99b5486a317d5e3",
      "4c7c78770d2041e2a66c99e49bad7f01",
      "0427037fa8cc4f87830440f1ca0c49e7",
      "ec5bac39bb254f7d8690d7aef2984b86",
      "42c431d43f414396b17b0fa740557e02",
      "e6d5e11a804e4fd7a01fc7c1e7700a5b",
      "2359486a3c9041e2a7e2e03238aa817f",
      "b5ed3f6d70ee4608ab38ac2fbe78e943",
      "9fbf5bbe8bb646239b2196faf41283af",
      "a6a47180d0424330b5efb8d81129ce01",
      "7c4465ec5e544906aa41a7e6cc82451e",
      "55333d675b42426bb1f5bc1ccf82827f",
      "0058e56d30cb40598b62e2798d57dd4c",
      "b3fca54d08eb4439989d098413ac777e",
      "76686bd0636a4401ad2b8debedd39e8c",
      "3f7095b4a86e40ecb6740c12fd53a99f",
      "aacf253901d14d58b5eabf27a80a131e",
      "8967c4fc0eca4fbb9892afd07f5d2619",
      "13da6ffd78964675b699f544510947bb",
      "0743dbb4232a4143bf3d22f10a606006",
      "53a1d3e2d5754413b77dd2f5c5e1ae90",
      "62821e35851043569ba3fcc822c235ed",
      "79c9b2b64c4e48f1905b425db44e2db9",
      "39cc1007bf714de6abe9505ef637175c",
      "85542cc8c06944f9b5e8090ef35a9407",
      "296f06e92f824c10b2bd9b772c65a3b2",
      "085a74d291524e66a2ba6af109bc1f29",
      "f696be6a65924089b85589ecc2cddf61",
      "8d4f1a442df5476da052df622c56498d",
      "d2343705bce74db382439479af490d7c",
      "50651d8566c14893a12ec28b3f7c44df",
      "0f7a938d874a42cb917a486bdb096315",
      "3080c8063942424b8374f593c971a2df",
      "5c5bc58dcb794bbbac54defd763f179d",
      "c33ff826c05b4ec6aa87edfb75228364",
      "a7758b23eabf41c5a4fa948f90405b73",
      "47d839270a46420b9b18893ed0805344",
      "24a96bde324642369009617fcb6c9e16",
      "ff5a3824f6e94689be3bf83eed88302e",
      "636c2c57cb3b44c0a8fe8033459f3bb5",
      "acba547bead4495ca0cb675070d18ce4",
      "23f4f10e6a4a400cafecba96e89384a7",
      "da4d46bd68264cd29bb8ec539b7bb9ba",
      "e440076625164c259a1467279035e1c5",
      "a20a447b39f84312a0eff1887bb9ed44",
      "d082f074220f4f96a2f4f401d06fa5f9",
      "e2127613ba9c407b84c9e3ddd48d9313",
      "eb24103b1bbf48289de19c0f05acea1a",
      "72ad75a111624935ae72eb30d8e2243a",
      "7ccc882eaf30450abd88f05a435ce670",
      "49918e2f66224c75b4cd1b154d1d48f8",
      "c14ec55597d9413a93670c0523baf8d3",
      "3197e91e6e234c18b5b9822de3d1f231"
     ]
    },
    "id": "55890b99-9ffb-4360-ac02-e4b6cfc73354",
    "outputId": "64303e6f-71ed-4217-c697-25787c4a00b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset iwslt14/de-en to /content/drive/MyDrive/.cache/iwslt14/de-en/1.0.0/ddd017b0ab639227607efd17fdf9687b1c4f06edf13c78a00314d7ea682d408d...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f9fa74a4284eb388375f2479fa64c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5a3572fbc7412190a0be7158be3988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a47180d0424330b5efb8d81129ce01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a1d3e2d5754413b77dd2f5c5e1ae90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset iwslt14 downloaded and prepared to /content/drive/MyDrive/.cache/iwslt14/de-en/1.0.0/ddd017b0ab639227607efd17fdf9687b1c4f06edf13c78a00314d7ea682d408d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7a938d874a42cb917a486bdb096315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4d46bd68264cd29bb8ec539b7bb9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174443 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation.de', 'translation.en', 'en', 'de'],\n",
       "    num_rows: 174443\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sacremoses import MosesTokenizer\n",
    "\n",
    "moses_tokenizer = {\n",
    "    \"en\": MosesTokenizer(lang=\"en\"),\n",
    "    \"de\": MosesTokenizer(lang=\"de\"),\n",
    "}\n",
    "iwslt14 = load_dataset(f\"{root_dir}/data/iwslt14\", name=\"de-en\")\n",
    "iwslt14_train = iwslt14['train'].flatten()\n",
    "iwslt14_train = iwslt14_train.map(\n",
    "    lambda item: {\"en\": moses_tokenizer[\"en\"].tokenize(item[\"translation.en\"], return_str=True),\n",
    "                  \"de\": moses_tokenizer[\"de\"].tokenize(item[\"translation.de\"], return_str=True)}\n",
    ")\n",
    "iwslt14_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2903e-b7c3-4add-85a2-af249602a034",
   "metadata": {
    "id": "14e2903e-b7c3-4add-85a2-af249602a034"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56f0bfb7-ba83-4743-909e-211abff412b4",
   "metadata": {
    "id": "56f0bfb7-ba83-4743-909e-211abff412b4"
   },
   "source": [
    "We create a joint vocaburaly in size 40,000 shared both in source and target.\n",
    "\n",
    "JoeyNMT expects vocab file in one-token-per-line format. The vocab file generated by fastBPE has two coloumns, which is imcompatible with JoeyNMT. So we drop the second column and make it reusable in JoeyNMT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78962cc-550e-4921-b85d-d5f6c2e5fda0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a78962cc-550e-4921-b85d-d5f6c2e5fda0",
    "outputId": "6c4668ce-c59b-4ca9-d809-d05dfb0c5c21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36662010"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ende = iwslt14_train['en'] + iwslt14_train['de']\n",
    "(Path(root_dir) / 'data/iwslt14/train.ende').write_text('\\n'.join(train_ende))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd3a51-2ca0-47c5-b9fc-2c6f9b8559aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8cd3a51-2ca0-47c5-b9fc-2c6f9b8559aa",
    "outputId": "0499062d-1ba8-46b6-832d-211a0ec6597d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary from train.ende ...\n",
      "Read 6865852 words (173116 unique) from text file.\n",
      "tcmalloc: large alloc 12000002048 bytes == 0x556bb453a000 @  0x7f99f1a4a887 0x556baef878f3 0x556baef7c78f 0x7f99f0e85c87 0x556baef7ca1a\n",
      "Loading codes from codes.40000 ...\n",
      "Read 40000 codes from the codes file.\n",
      "Loading vocabulary from train.ende ...\n",
      "Read 6865852 words (173116 unique) from text file.\n",
      "Applying BPE to train.ende ...\n",
      "Modified 6865852 words from text file.\n",
      "Loading vocabulary from train.ende.40000 ...\n",
      "Read 7421753 words (39687 unique) from text file.\n"
     ]
    }
   ],
   "source": [
    "!cd {root_dir}/data/iwslt14 && {root_dir}/fastBPE/fast learnbpe 40000 train.ende > codes.40000\n",
    "!cd {root_dir}/data/iwslt14 && {root_dir}/fastBPE/fast applybpe train.ende.40000 train.ende codes.40000\n",
    "!cd {root_dir}/data/iwslt14 && {root_dir}/fastBPE/fast getvocab train.ende.40000 > vocab_freq.40000\n",
    "!cd {root_dir}/data/iwslt14 && cut -d \" \" -f 1 vocab_freq.40000 > vocab.40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee15b89-15c5-4929-b630-e91b1fc69567",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fee15b89-15c5-4929-b630-e91b1fc69567",
    "outputId": "b85eee59-971c-4798-d45b-797232211a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      ".\n",
      "the\n",
      "in\n",
      "to\n",
      "of\n",
      "die\n",
      "a\n",
      "and\n",
      "und\n"
     ]
    }
   ],
   "source": [
    "!head -10 {root_dir}/data/iwslt14/vocab.40000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835453c5-f05e-4e5f-bcf4-f0aba51122f3",
   "metadata": {
    "id": "835453c5-f05e-4e5f-bcf4-f0aba51122f3"
   },
   "source": [
    "Now we've prepared a codes file, and a vocab file in one-toke-per-line format.\n",
    "\n",
    "Let's try to tokenize a text using the codes file trained aboeve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921bba2-0b9a-4f9d-9d57-f5492cd00319",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f921bba2-0b9a-4f9d-9d57-f5492cd00319",
    "outputId": "3f64c34e-19b1-49bb-8a9d-471402528332"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastBPETokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=fastBPE, separator=@@, dropout=0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastbpe_tokenizer = FastBPETokenizer(codes=f\"{root_dir}/data/iwslt14/codes.40000\")\n",
    "fastbpe_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73eea62-6c10-4f7e-bf3b-5aa7ed755ad8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d73eea62-6c10-4f7e-bf3b-5aa7ed755ad8",
    "outputId": "340776fa-4980-4e5a-9371-1b5c2431cf99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastbpe_tokenizer(\"This is a test .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b41024-f28c-44f2-a37e-6e1fcba3d1b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08b41024-f28c-44f2-a37e-6e1fcba3d1b2",
    "outputId": "55a83596-4796-4204-b6c6-0951bd180e13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Das', 'ist', 'ein', 'Beispiel', '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastbpe_tokenizer(\"Das ist ein Beispiel .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f8ac4-042e-4292-99d2-9ab50c2ffa48",
   "metadata": {
    "id": "4b5f8ac4-042e-4292-99d2-9ab50c2ffa48"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75Cmg5zSQ4mi",
   "metadata": {
    "id": "75Cmg5zSQ4mi"
   },
   "source": [
    "Rewrite the config file so that the new codes and vocab file will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12bddd-286b-4c77-a960-6f7536f1b5f6",
   "metadata": {
    "id": "4c12bddd-286b-4c77-a960-6f7536f1b5f6"
   },
   "outputs": [],
   "source": [
    "fastbpe_config = new_config\\\n",
    "  .replace('vocab.32000', 'vocab.40000')\\\n",
    "  .replace('codes.32000', 'codes.40000')\\\n",
    "  .replace('model_dir: \"transformer_iwslt14_deen_bpe\"', 'model_dir: \"iwslt14_deen_fastbpe\"')\\\n",
    "  .replace('load_model:', '#load_model:')\n",
    "\n",
    "with (Path(root_dir) / \"data/iwslt14/fastbpe_config.yaml\").open('w') as f:\n",
    "    f.write(fastbpe_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91212482-c24c-499a-b8f2-a8fc1c0eb1a5",
   "metadata": {
    "id": "91212482-c24c-499a-b8f2-a8fc1c0eb1a5"
   },
   "source": [
    "Start training from scratch using fastBPE tokenizer with the newly created codes file.\n",
    "\n",
    "Please check the line with \"INFO - joeynmt.tokenizers\" below. You will see \"FastBPETokenizer\" in the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eecb6ef-80e8-4dad-9e41-4d7f747c9a1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eecb6ef-80e8-4dad-9e41-4d7f747c9a1d",
    "outputId": "be5d28f8-bfa2-4f20-d62e-4111c3acc6d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-15 20:46:16,536 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-08-15 20:46:16,537 - INFO - joeynmt.helpers -                           cfg.name : transformer_iwslt14_deen_fastbpe\n",
      "2022-08-15 20:46:16,538 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0\n",
      "2022-08-15 20:46:16,538 - INFO - joeynmt.helpers -                     cfg.data.train : iwslt14\n",
      "2022-08-15 20:46:16,538 - INFO - joeynmt.helpers -                       cfg.data.dev : iwslt14\n",
      "2022-08-15 20:46:16,538 - INFO - joeynmt.helpers -                      cfg.data.test : iwslt14\n",
      "2022-08-15 20:46:16,538 - INFO - joeynmt.helpers -              cfg.data.dataset_type : huggingface\n",
      "2022-08-15 20:46:16,538 - INFO - joeynmt.helpers -          cfg.data.dataset_cfg.name : de-en\n",
      "2022-08-15 20:46:16,538 - INFO - joeynmt.helpers -                  cfg.data.src.lang : de\n",
      "2022-08-15 20:46:16,539 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 128\n",
      "2022-08-15 20:46:16,539 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : True\n",
      "2022-08-15 20:46:16,539 - INFO - joeynmt.helpers -             cfg.data.src.normalize : False\n",
      "2022-08-15 20:46:16,539 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe\n",
      "2022-08-15 20:46:16,539 - INFO - joeynmt.helpers -          cfg.data.src.voc_min_freq : 1\n",
      "2022-08-15 20:46:16,539 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : data/iwslt14/vocab.40000\n",
      "2022-08-15 20:46:16,539 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : fastbpe\n",
      "2022-08-15 20:46:16,539 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : data/iwslt14/codes.40000\n",
      "2022-08-15 20:46:16,540 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : moses\n",
      "2022-08-15 20:46:16,540 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : en\n",
      "2022-08-15 20:46:16,540 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 128\n",
      "2022-08-15 20:46:16,540 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : True\n",
      "2022-08-15 20:46:16,540 - INFO - joeynmt.helpers -             cfg.data.trg.normalize : False\n",
      "2022-08-15 20:46:16,540 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe\n",
      "2022-08-15 20:46:16,540 - INFO - joeynmt.helpers -          cfg.data.trg.voc_min_freq : 1\n",
      "2022-08-15 20:46:16,540 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : data/iwslt14/vocab.40000\n",
      "2022-08-15 20:46:16,540 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : fastbpe\n",
      "2022-08-15 20:46:16,541 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : data/iwslt14/codes.40000\n",
      "2022-08-15 20:46:16,541 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : moses\n",
      "2022-08-15 20:46:16,541 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1\n",
      "2022-08-15 20:46:16,541 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\n",
      "2022-08-15 20:46:16,541 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0\n",
      "2022-08-15 20:46:16,541 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 1024\n",
      "2022-08-15 20:46:16,541 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token\n",
      "2022-08-15 20:46:16,541 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 100\n",
      "2022-08-15 20:46:16,542 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']\n",
      "2022-08-15 20:46:16,542 - INFO - joeynmt.helpers -            cfg.testing.return_prob : none\n",
      "2022-08-15 20:46:16,542 - INFO - joeynmt.helpers -       cfg.testing.return_attention : False\n",
      "2022-08-15 20:46:16,542 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a\n",
      "2022-08-15 20:46:16,542 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.lowercase : True\n",
      "2022-08-15 20:46:16,542 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\n",
      "2022-08-15 20:46:16,542 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\n",
      "2022-08-15 20:46:16,542 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\n",
      "2022-08-15 20:46:16,543 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]\n",
      "2022-08-15 20:46:16,543 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau\n",
      "2022-08-15 20:46:16,543 - INFO - joeynmt.helpers -              cfg.training.patience : 5\n",
      "2022-08-15 20:46:16,543 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7\n",
      "2022-08-15 20:46:16,543 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\n",
      "2022-08-15 20:46:16,543 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003\n",
      "2022-08-15 20:46:16,543 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08\n",
      "2022-08-15 20:46:16,543 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\n",
      "2022-08-15 20:46:16,544 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1\n",
      "2022-08-15 20:46:16,544 - INFO - joeynmt.helpers -            cfg.training.batch_size : 4096\n",
      "2022-08-15 20:46:16,544 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\n",
      "2022-08-15 20:46:16,544 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : bleu\n",
      "2022-08-15 20:46:16,544 - INFO - joeynmt.helpers -                cfg.training.epochs : 100\n",
      "2022-08-15 20:46:16,544 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000\n",
      "2022-08-15 20:46:16,544 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100\n",
      "2022-08-15 20:46:16,544 - INFO - joeynmt.helpers -             cfg.training.model_dir : iwslt14_deen_fastbpe\n",
      "2022-08-15 20:46:16,544 - INFO - joeynmt.helpers -             cfg.training.overwrite : False\n",
      "2022-08-15 20:46:16,545 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\n",
      "2022-08-15 20:46:16,545 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\n",
      "2022-08-15 20:46:16,545 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]\n",
      "2022-08-15 20:46:16,545 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 5\n",
      "2022-08-15 20:46:16,545 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform\n",
      "2022-08-15 20:46:16,545 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform\n",
      "2022-08-15 20:46:16,545 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\n",
      "2022-08-15 20:46:16,545 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\n",
      "2022-08-15 20:46:16,546 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\n",
      "2022-08-15 20:46:16,546 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True\n",
      "2022-08-15 20:46:16,546 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True\n",
      "2022-08-15 20:46:16,546 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\n",
      "2022-08-15 20:46:16,546 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\n",
      "2022-08-15 20:46:16,546 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4\n",
      "2022-08-15 20:46:16,546 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2022-08-15 20:46:16,546 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2022-08-15 20:46:16,547 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0\n",
      "2022-08-15 20:46:16,547 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256\n",
      "2022-08-15 20:46:16,547 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024\n",
      "2022-08-15 20:46:16,547 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.3\n",
      "2022-08-15 20:46:16,547 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre\n",
      "2022-08-15 20:46:16,547 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\n",
      "2022-08-15 20:46:16,547 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\n",
      "2022-08-15 20:46:16,547 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4\n",
      "2022-08-15 20:46:16,548 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2022-08-15 20:46:16,548 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2022-08-15 20:46:16,548 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0\n",
      "2022-08-15 20:46:16,548 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256\n",
      "2022-08-15 20:46:16,548 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024\n",
      "2022-08-15 20:46:16,548 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.3\n",
      "2022-08-15 20:46:16,549 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre\n",
      "2022-08-15 20:46:16,559 - INFO - joeynmt.data - Building tokenizer...\n",
      "Loading codes from data/iwslt14/codes.40000 ...\n",
      "Read 40000 codes from the codes file.\n",
      "Loading codes from data/iwslt14/codes.40000 ...\n",
      "Read 40000 codes from the codes file.\n",
      "2022-08-15 20:46:16,665 - INFO - joeynmt.tokenizers - de tokenizer: FastBPETokenizer(level=bpe, lowercase=True, normalize=False, filter_by_length=(-1, 128), pretokenizer=moses, tokenizer=fastBPE, separator=@@, dropout=0.0)\n",
      "2022-08-15 20:46:16,665 - INFO - joeynmt.tokenizers - en tokenizer: FastBPETokenizer(level=bpe, lowercase=True, normalize=False, filter_by_length=(-1, 128), pretokenizer=moses, tokenizer=fastBPE, separator=@@, dropout=0.0)\n",
      "2022-08-15 20:46:16,665 - INFO - joeynmt.data - Loading train set...\n",
      "2022-08-15 20:46:16,727 - INFO - numexpr.utils - NumExpr defaulting to 2 threads.\n",
      "2022-08-15 20:46:18,790 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/iwslt14/ddd017b0ab639227607efd17fdf9687b1c4f06edf13c78a00314d7ea682d408d (last modified on Mon Aug 15 20:35:28 2022) since it couldn't be found locally at iwslt14., or remotely on the Hugging Face Hub.\n",
      "2022-08-15 20:46:18,907 - WARNING - datasets.builder - Reusing dataset iwslt14 (/content/drive/MyDrive/.cache/iwslt14/de-en/1.0.0/ddd017b0ab639227607efd17fdf9687b1c4f06edf13c78a00314d7ea682d408d)\n",
      "2022-08-15 20:46:18,928 - WARNING - datasets.fingerprint - Parameter 'function'=<function HuggingfaceDataset.load_data.<locals>._drop_nan at 0x7f4f7830eb00> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Dropping NaN...: 100% 175/175 [00:01<00:00, 167.38ba/s]\n",
      "Preprocessing...: 100% 174419/174419 [01:11<00:00, 2424.52ex/s]\n",
      "2022-08-15 20:47:31,995 - INFO - joeynmt.data - Building vocabulary...\n",
      "2022-08-15 20:47:58,965 - INFO - joeynmt.data - Loading dev set...\n",
      "2022-08-15 20:48:00,581 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/iwslt14/ddd017b0ab639227607efd17fdf9687b1c4f06edf13c78a00314d7ea682d408d (last modified on Mon Aug 15 20:35:28 2022) since it couldn't be found locally at iwslt14., or remotely on the Hugging Face Hub.\n",
      "2022-08-15 20:48:00,597 - WARNING - datasets.builder - Reusing dataset iwslt14 (/content/drive/MyDrive/.cache/iwslt14/de-en/1.0.0/ddd017b0ab639227607efd17fdf9687b1c4f06edf13c78a00314d7ea682d408d)\n",
      "Dropping NaN...: 100% 3/3 [00:00<00:00, 152.12ba/s]\n",
      "Preprocessing...: 100% 2052/2052 [00:01<00:00, 1971.72ex/s]\n",
      "2022-08-15 20:48:01,715 - INFO - joeynmt.data - Loading test set...\n",
      "2022-08-15 20:48:02,685 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/iwslt14/ddd017b0ab639227607efd17fdf9687b1c4f06edf13c78a00314d7ea682d408d (last modified on Mon Aug 15 20:35:28 2022) since it couldn't be found locally at iwslt14., or remotely on the Hugging Face Hub.\n",
      "2022-08-15 20:48:02,701 - WARNING - datasets.builder - Reusing dataset iwslt14 (/content/drive/MyDrive/.cache/iwslt14/de-en/1.0.0/ddd017b0ab639227607efd17fdf9687b1c4f06edf13c78a00314d7ea682d408d)\n",
      "Dropping NaN...: 100% 5/5 [00:00<00:00, 146.44ba/s]\n",
      "Preprocessing...: 100% 4698/4698 [00:01<00:00, 2587.19ex/s]\n",
      "2022-08-15 20:48:04,589 - INFO - joeynmt.data - Data loaded.\n",
      "2022-08-15 20:48:04,589 - INFO - joeynmt.helpers - Train dataset: HuggingfaceDataset(len=174419, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1, name=de-en, split=train)\n",
      "2022-08-15 20:48:04,590 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=2052, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1, name=de-en, split=validation)\n",
      "2022-08-15 20:48:04,590 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=4698, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1, name=de-en, split=test)\n",
      "2022-08-15 20:48:04,591 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] das meer kann ziemlich kompliziert sein .\n",
      "\t[TRG] it can be a very complicated thing , the ocean .\n",
      "2022-08-15 20:48:04,591 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) to (9) of\n",
      "2022-08-15 20:48:04,591 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) to (9) of\n",
      "2022-08-15 20:48:04,591 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 39691\n",
      "2022-08-15 20:48:04,591 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 39691\n",
      "2022-08-15 20:48:04,695 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-15 20:48:05,187 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-15 20:48:07,357 - INFO - joeynmt.model - Total params: 21221120\n",
      "2022-08-15 20:48:07,359 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=39691),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=39691),\n",
      "\tloss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))\n",
      "2022-08-15 20:48:11,951 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=[0.9, 0.999])\n",
      "2022-08-15 20:48:11,951 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=max, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=5)\n",
      "2022-08-15 20:48:11,952 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\teffective batch size (w. parallel & accumulation): 4096\n",
      "2022-08-15 20:48:11,952 - INFO - joeynmt.training - EPOCH 1\n",
      "2022-08-15 20:48:34,620 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.880674, Batch Acc: 0.054343, Tokens per Sec:     5311, Lr: 0.000300\n",
      "2022-08-15 20:48:56,237 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.677292, Batch Acc: 0.112785, Tokens per Sec:     5640, Lr: 0.000300\n",
      "2022-08-15 20:49:17,951 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     5.397653, Batch Acc: 0.140154, Tokens per Sec:     5687, Lr: 0.000300\n",
      "2022-08-15 20:49:40,106 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.228195, Batch Acc: 0.151167, Tokens per Sec:     5311, Lr: 0.000300\n",
      "2022-08-15 20:50:01,760 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     5.199869, Batch Acc: 0.156545, Tokens per Sec:     5538, Lr: 0.000300\n",
      "2022-08-15 20:50:23,185 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     5.223935, Batch Acc: 0.162795, Tokens per Sec:     5708, Lr: 0.000300\n",
      "2022-08-15 20:50:45,157 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     4.993643, Batch Acc: 0.166710, Tokens per Sec:     5582, Lr: 0.000300\n",
      "2022-08-15 20:51:06,903 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     5.116451, Batch Acc: 0.170197, Tokens per Sec:     5598, Lr: 0.000300\n",
      "2022-08-15 20:51:28,955 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     4.882935, Batch Acc: 0.177789, Tokens per Sec:     5569, Lr: 0.000300\n",
      "2022-08-15 20:51:50,770 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     5.032339, Batch Acc: 0.180973, Tokens per Sec:     5502, Lr: 0.000300\n",
      "2022-08-15 20:51:50,771 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 20:52:46,145 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 20:52:46,145 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   0.44, loss:   5.05, ppl: 156.41, acc:   0.18, generation: 54.3013[sec], evaluation: 0.3114[sec]\n",
      "2022-08-15 20:52:46,146 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 20:52:47,350 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 20:52:47,356 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 20:52:47,356 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 20:52:47,357 - INFO - joeynmt.training - \tHypothesis: and i can have a world, and i can be the world, and i can be the world, and i can be a world, and the world, and i can be a world, and i can be a world, and i can be a world, and i can be a world, and i can be a world, and i can be a world, and i can be a world, and i can be to be a world, and i can be a world, and i can\n",
      "2022-08-15 20:52:47,357 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 20:52:47,361 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 20:52:47,361 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 20:52:47,361 - INFO - joeynmt.training - \tHypothesis: and i can have a world, and the world, and i can be the world, and the world, and i can be a world, and the world, and i can be a world, and i can be a world, and the world, and i can be a world, and i can be a world, and i can be a world, and i can be a world, and i can have to be to be a world, and i can be a world, and i can\n",
      "2022-08-15 20:52:47,361 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 20:52:47,364 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 20:52:47,364 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 20:52:47,364 - INFO - joeynmt.training - \tHypothesis: and i're a world, and i can be the world.\n",
      "2022-08-15 20:52:47,365 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 20:52:47,369 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 20:52:47,369 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 20:52:47,369 - INFO - joeynmt.training - \tHypothesis: and i're a world, and i can be the world.\n",
      "2022-08-15 20:52:47,369 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 20:52:47,372 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 20:52:47,372 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 20:52:47,373 - INFO - joeynmt.training - \tHypothesis: and i're a world.\n",
      "2022-08-15 20:53:09,746 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     4.941198, Batch Acc: 0.185912, Tokens per Sec:     5097, Lr: 0.000300\n",
      "2022-08-15 20:53:31,867 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.879086, Batch Acc: 0.191796, Tokens per Sec:     5383, Lr: 0.000300\n",
      "2022-08-15 20:53:53,584 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.809684, Batch Acc: 0.195018, Tokens per Sec:     5266, Lr: 0.000300\n",
      "2022-08-15 20:54:15,589 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.988866, Batch Acc: 0.201077, Tokens per Sec:     5527, Lr: 0.000300\n",
      "2022-08-15 20:54:37,378 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     4.804751, Batch Acc: 0.202341, Tokens per Sec:     5479, Lr: 0.000300\n",
      "2022-08-15 20:54:59,698 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.907604, Batch Acc: 0.210161, Tokens per Sec:     5500, Lr: 0.000300\n",
      "2022-08-15 20:55:21,413 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     4.808315, Batch Acc: 0.214050, Tokens per Sec:     5535, Lr: 0.000300\n",
      "2022-08-15 20:55:43,484 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     4.868003, Batch Acc: 0.221030, Tokens per Sec:     5474, Lr: 0.000300\n",
      "2022-08-15 20:56:04,998 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     4.664507, Batch Acc: 0.225919, Tokens per Sec:     5482, Lr: 0.000300\n",
      "2022-08-15 20:56:26,855 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     4.805121, Batch Acc: 0.228759, Tokens per Sec:     5582, Lr: 0.000300\n",
      "2022-08-15 20:56:26,856 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 20:57:44,435 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 20:57:44,436 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   0.49, loss:   4.91, ppl: 136.00, acc:   0.22, generation: 76.1058[sec], evaluation: 0.4137[sec]\n",
      "2022-08-15 20:57:44,437 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 20:57:45,544 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 20:57:45,551 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 20:57:45,551 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 20:57:45,551 - INFO - joeynmt.training - \tHypothesis: you're a lot of the world, and the first of the world, and the world, the world, the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world, and we're been been a lot of the world, and we're been been been been been been been been been been been been been been been been been a lot of the world, and we're a lot of the world, and\n",
      "2022-08-15 20:57:45,552 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 20:57:45,555 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 20:57:45,555 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 20:57:45,556 - INFO - joeynmt.training - \tHypothesis: it's a lot of the world, the first, the first thing, the first, the first of the world, and the world, the world, the world, the world, the world, the world, the world, and we're been a lot of the world, and we're been a lot of the world, and we're been been been a lot of the world, and we're been a lot of the world, and we're a lot of the world, and we're\n",
      "2022-08-15 20:57:45,556 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 20:57:45,558 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 20:57:45,559 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 20:57:45,559 - INFO - joeynmt.training - \tHypothesis: and we're been been been been been been been to be a lot of the world.\n",
      "2022-08-15 20:57:45,559 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 20:57:45,562 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 20:57:45,562 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 20:57:45,562 - INFO - joeynmt.training - \tHypothesis: we're been been been been been been been to be a lot of the world.\n",
      "2022-08-15 20:57:45,562 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 20:57:45,565 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 20:57:45,565 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 20:57:45,565 - INFO - joeynmt.training - \tHypothesis: we're a lot of the world.\n",
      "2022-08-15 20:58:07,974 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     4.685781, Batch Acc: 0.233105, Tokens per Sec:     5244, Lr: 0.000300\n",
      "2022-08-15 20:58:29,907 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     4.820754, Batch Acc: 0.234361, Tokens per Sec:     5699, Lr: 0.000300\n",
      "2022-08-15 20:58:51,932 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     4.840821, Batch Acc: 0.239613, Tokens per Sec:     5431, Lr: 0.000300\n",
      "2022-08-15 20:59:14,053 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     4.741670, Batch Acc: 0.243512, Tokens per Sec:     5452, Lr: 0.000300\n",
      "2022-08-15 20:59:35,971 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     4.716740, Batch Acc: 0.249906, Tokens per Sec:     5473, Lr: 0.000300\n",
      "2022-08-15 20:59:58,088 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     4.590816, Batch Acc: 0.251332, Tokens per Sec:     5422, Lr: 0.000300\n",
      "2022-08-15 21:00:19,968 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     4.549999, Batch Acc: 0.259681, Tokens per Sec:     5598, Lr: 0.000300\n",
      "2022-08-15 21:00:42,119 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     4.845673, Batch Acc: 0.263996, Tokens per Sec:     5419, Lr: 0.000300\n",
      "2022-08-15 21:01:04,448 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     4.512650, Batch Acc: 0.266672, Tokens per Sec:     5507, Lr: 0.000300\n",
      "2022-08-15 21:01:26,195 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     4.525964, Batch Acc: 0.270714, Tokens per Sec:     5644, Lr: 0.000300\n",
      "2022-08-15 21:01:26,196 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:02:36,955 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:02:36,956 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   1.34, loss:   4.79, ppl: 120.61, acc:   0.27, generation: 69.4175[sec], evaluation: 0.4189[sec]\n",
      "2022-08-15 21:02:36,957 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:02:38,205 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:02:38,210 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:02:38,210 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:02:38,212 - INFO - joeynmt.training - \tHypothesis: you know, the first of the first of the first of the first, and the first of the first of the first, the other people are the other people that are the other people are in the world, and they're going to be able to be able to be able to be able to be able to be able to the way to the way to the way to the way to the way to the way to the way to the way.\n",
      "2022-08-15 21:02:38,212 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:02:38,216 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:02:38,216 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:02:38,216 - INFO - joeynmt.training - \tHypothesis: so, this is a \"the\" in the u.s., the first of the first, the first of the first, the first of the first, the first of the first, the first of the first of the first, the first of the world, the world, the world, the world, the world, the world, the world, the first is a little bit of the world, and we've got to be a little bit of the world, and the world, the world\n",
      "2022-08-15 21:02:38,216 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:02:38,219 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:02:38,220 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:02:38,220 - INFO - joeynmt.training - \tHypothesis: and we've been able to be a lot of the way.\n",
      "2022-08-15 21:02:38,220 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:02:38,223 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:02:38,223 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:02:38,223 - INFO - joeynmt.training - \tHypothesis: we're going to be a lot of our world.\n",
      "2022-08-15 21:02:38,223 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:02:38,227 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:02:38,227 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:02:38,227 - INFO - joeynmt.training - \tHypothesis: we're going to go in the u.s.\n",
      "2022-08-15 21:03:01,137 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     4.676382, Batch Acc: 0.270064, Tokens per Sec:     5069, Lr: 0.000300\n",
      "2022-08-15 21:03:22,905 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     4.587523, Batch Acc: 0.278222, Tokens per Sec:     5532, Lr: 0.000300\n",
      "2022-08-15 21:03:31,465 - INFO - joeynmt.training - Epoch   1: total training loss 16150.82\n",
      "2022-08-15 21:03:31,466 - INFO - joeynmt.training - EPOCH 2\n",
      "2022-08-15 21:03:45,046 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     4.374036, Batch Acc: 0.281193, Tokens per Sec:     5580, Lr: 0.000300\n",
      "2022-08-15 21:04:07,215 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     4.462623, Batch Acc: 0.290606, Tokens per Sec:     5553, Lr: 0.000300\n",
      "2022-08-15 21:04:29,490 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     4.710729, Batch Acc: 0.290887, Tokens per Sec:     5535, Lr: 0.000300\n",
      "2022-08-15 21:04:51,433 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     4.382256, Batch Acc: 0.294545, Tokens per Sec:     5438, Lr: 0.000300\n",
      "2022-08-15 21:05:13,830 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     4.739389, Batch Acc: 0.297108, Tokens per Sec:     5488, Lr: 0.000300\n",
      "2022-08-15 21:05:35,909 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     4.395713, Batch Acc: 0.299795, Tokens per Sec:     5449, Lr: 0.000300\n",
      "2022-08-15 21:05:58,101 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     4.610826, Batch Acc: 0.305082, Tokens per Sec:     5433, Lr: 0.000300\n",
      "2022-08-15 21:06:19,916 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     4.571481, Batch Acc: 0.306679, Tokens per Sec:     5713, Lr: 0.000300\n",
      "2022-08-15 21:06:19,916 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:07:34,980 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:07:34,980 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   2.58, loss:   4.61, ppl: 100.03, acc:   0.30, generation: 73.6958[sec], evaluation: 0.4221[sec]\n",
      "2022-08-15 21:07:34,982 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:07:36,058 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:07:36,063 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:07:36,063 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:07:36,063 - INFO - joeynmt.training - \tHypothesis: you know, you're a few of the same thing, and you're a few years of the world, and the most of the people can see the people that are going to be able to be able to be able to be able to be in the way to be in the way that they can be able to be able to be able to be able to be able to be able to be able to be able to be in the way to be in the way to be able to be in the\n",
      "2022-08-15 21:07:36,064 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:07:36,067 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:07:36,067 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:07:36,067 - INFO - joeynmt.training - \tHypothesis: so that's why the \"the\" the \"is the only, the most of the end of the sun, or the most of the most of the earth, and the most of the most of the most of the world, but the most of the world is that we're going to be able to be able to be able to be able to be able to be able to be able to be able to be a world, but that's the world, and that's a year, and that\n",
      "2022-08-15 21:07:36,068 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:07:36,070 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:07:36,071 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:07:36,071 - INFO - joeynmt.training - \tHypothesis: and we have to have the same thing that we're in the same system.\n",
      "2022-08-15 21:07:36,071 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:07:36,074 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:07:36,074 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:07:36,074 - INFO - joeynmt.training - \tHypothesis: we're going to see that we're going to use our world.\n",
      "2022-08-15 21:07:36,074 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:07:36,077 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:07:36,077 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:07:36,077 - INFO - joeynmt.training - \tHypothesis: we're going to be the same way.\n",
      "2022-08-15 21:07:58,680 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     4.580179, Batch Acc: 0.306722, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2022-08-15 21:08:20,557 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     4.199121, Batch Acc: 0.312268, Tokens per Sec:     5551, Lr: 0.000300\n",
      "2022-08-15 21:08:42,594 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     4.333919, Batch Acc: 0.315899, Tokens per Sec:     5541, Lr: 0.000300\n",
      "2022-08-15 21:09:04,581 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     4.289984, Batch Acc: 0.317346, Tokens per Sec:     5481, Lr: 0.000300\n",
      "2022-08-15 21:09:26,679 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     4.157608, Batch Acc: 0.321376, Tokens per Sec:     5560, Lr: 0.000300\n",
      "2022-08-15 21:09:49,005 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     4.506279, Batch Acc: 0.320244, Tokens per Sec:     5524, Lr: 0.000300\n",
      "2022-08-15 21:10:10,896 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     4.264665, Batch Acc: 0.323196, Tokens per Sec:     5511, Lr: 0.000300\n",
      "2022-08-15 21:10:33,095 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     4.257266, Batch Acc: 0.325990, Tokens per Sec:     5727, Lr: 0.000300\n",
      "2022-08-15 21:10:54,633 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     4.271552, Batch Acc: 0.330576, Tokens per Sec:     5647, Lr: 0.000300\n",
      "2022-08-15 21:11:17,082 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     3.999480, Batch Acc: 0.331134, Tokens per Sec:     5394, Lr: 0.000300\n",
      "2022-08-15 21:11:17,083 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:12:19,427 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:12:19,428 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   4.29, loss:   4.39, ppl:  80.28, acc:   0.33, generation: 61.1767[sec], evaluation: 0.3865[sec]\n",
      "2022-08-15 21:12:19,429 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:12:20,477 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:12:20,481 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:12:20,482 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:12:20,482 - INFO - joeynmt.training - \tHypothesis: you know, you know, a big idea of the big and the way of the data is the data of the world, the people can see the next time, you can see the next time, you can see the way to the way of the way.\n",
      "2022-08-15 21:12:20,482 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:12:20,486 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:12:20,486 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:12:20,486 - INFO - joeynmt.training - \tHypothesis: so, the reason that the michael is actually just the first way to the u.s. and the right of the top of the one of the the u.s., the world is the world that we're going to be able to be able to be able to be able to be a world, but we're going to be able to be able to be a world.\n",
      "2022-08-15 21:12:20,486 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:12:20,489 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:12:20,490 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:12:20,490 - INFO - joeynmt.training - \tHypothesis: and we're going to do all the same thing.\n",
      "2022-08-15 21:12:20,490 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:12:20,493 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:12:20,493 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:12:20,493 - INFO - joeynmt.training - \tHypothesis: we're all all all all. we're going to do our world.\n",
      "2022-08-15 21:12:20,493 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:12:20,496 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:12:20,496 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:12:20,496 - INFO - joeynmt.training - \tHypothesis: we're going to use the kind of.\n",
      "2022-08-15 21:12:43,111 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     4.056942, Batch Acc: 0.339114, Tokens per Sec:     5152, Lr: 0.000300\n",
      "2022-08-15 21:13:04,792 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     3.914767, Batch Acc: 0.339165, Tokens per Sec:     5650, Lr: 0.000300\n",
      "2022-08-15 21:13:26,577 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     4.064254, Batch Acc: 0.339246, Tokens per Sec:     5463, Lr: 0.000300\n",
      "2022-08-15 21:13:48,933 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     4.078518, Batch Acc: 0.343964, Tokens per Sec:     5532, Lr: 0.000300\n",
      "2022-08-15 21:14:10,974 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     4.143125, Batch Acc: 0.344857, Tokens per Sec:     5507, Lr: 0.000300\n",
      "2022-08-15 21:14:32,749 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     3.975497, Batch Acc: 0.346138, Tokens per Sec:     5529, Lr: 0.000300\n",
      "2022-08-15 21:14:54,817 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     4.204741, Batch Acc: 0.347570, Tokens per Sec:     5431, Lr: 0.000300\n",
      "2022-08-15 21:15:16,512 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     4.046185, Batch Acc: 0.355694, Tokens per Sec:     5478, Lr: 0.000300\n",
      "2022-08-15 21:15:39,141 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     4.131404, Batch Acc: 0.353767, Tokens per Sec:     5377, Lr: 0.000300\n",
      "2022-08-15 21:16:01,087 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     4.224904, Batch Acc: 0.357343, Tokens per Sec:     5584, Lr: 0.000300\n",
      "2022-08-15 21:16:01,088 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:16:58,388 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:16:58,389 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.03, loss:   4.08, ppl:  59.39, acc:   0.36, generation: 56.0596[sec], evaluation: 0.4051[sec]\n",
      "2022-08-15 21:16:58,390 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:16:59,436 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/1000.ckpt\n",
      "2022-08-15 21:16:59,468 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:16:59,473 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:16:59,473 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:16:59,473 - INFO - joeynmt.training - \tHypothesis: you know, one of the big, and a large thing that is a kind of the key of the most of the world is to say, to say, the first time, the first time you can see in the last year, you can see the way to get the back in the air.\n",
      "2022-08-15 21:16:59,473 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:16:59,477 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:16:59,477 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:16:59,477 - INFO - joeynmt.training - \tHypothesis: just that's why johnson's still all the sun, and the sun are also the right, or the most of the most of the most of the most of the most of the u.s., the u.s., the world, the world is that we know, but the world is that we're not a world that we're going to have been able to have a lot of the world.\n",
      "2022-08-15 21:16:59,477 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:16:59,480 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:16:59,481 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:16:59,481 - INFO - joeynmt.training - \tHypothesis: and of course, we all do all all all of these same things.\n",
      "2022-08-15 21:16:59,481 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:16:59,484 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:16:59,484 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:16:59,484 - INFO - joeynmt.training - \tHypothesis: we're all all -- we're going to do our children.\n",
      "2022-08-15 21:16:59,484 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:16:59,487 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:16:59,488 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:16:59,488 - INFO - joeynmt.training - \tHypothesis: we're going to go to the key.\n",
      "2022-08-15 21:17:21,850 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     3.812346, Batch Acc: 0.356634, Tokens per Sec:     5141, Lr: 0.000300\n",
      "2022-08-15 21:17:43,730 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     3.749814, Batch Acc: 0.360398, Tokens per Sec:     5364, Lr: 0.000300\n",
      "2022-08-15 21:18:05,559 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     3.849966, Batch Acc: 0.360956, Tokens per Sec:     5495, Lr: 0.000300\n",
      "2022-08-15 21:18:27,939 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     3.755217, Batch Acc: 0.365542, Tokens per Sec:     5248, Lr: 0.000300\n",
      "2022-08-15 21:18:42,844 - INFO - joeynmt.training - Epoch   2: total training loss 13702.39\n",
      "2022-08-15 21:18:42,844 - INFO - joeynmt.training - EPOCH 3\n",
      "2022-08-15 21:18:49,600 - INFO - joeynmt.training - Epoch   3, Step:     6500, Batch Loss:     3.775943, Batch Acc: 0.367885, Tokens per Sec:     5636, Lr: 0.000300\n",
      "2022-08-15 21:19:11,771 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     3.483340, Batch Acc: 0.372378, Tokens per Sec:     5403, Lr: 0.000300\n",
      "2022-08-15 21:19:33,284 - INFO - joeynmt.training - Epoch   3, Step:     6700, Batch Loss:     3.779709, Batch Acc: 0.374133, Tokens per Sec:     5410, Lr: 0.000300\n",
      "2022-08-15 21:19:55,070 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     3.873618, Batch Acc: 0.376277, Tokens per Sec:     5430, Lr: 0.000300\n",
      "2022-08-15 21:20:17,595 - INFO - joeynmt.training - Epoch   3, Step:     6900, Batch Loss:     3.764743, Batch Acc: 0.376675, Tokens per Sec:     5249, Lr: 0.000300\n",
      "2022-08-15 21:20:40,019 - INFO - joeynmt.training - Epoch   3, Step:     7000, Batch Loss:     3.724370, Batch Acc: 0.374720, Tokens per Sec:     5474, Lr: 0.000300\n",
      "2022-08-15 21:20:40,019 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:21:34,581 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:21:34,581 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.52, loss:   3.83, ppl:  45.93, acc:   0.38, generation: 53.3585[sec], evaluation: 0.4125[sec]\n",
      "2022-08-15 21:21:34,582 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:21:36,065 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/2000.ckpt\n",
      "2022-08-15 21:21:36,095 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:21:36,099 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:21:36,099 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:21:36,099 - INFO - joeynmt.training - \tHypothesis: you know, one of the big, the big way and the most of the most of the most of the most of the early war is the first time to remember the day, you can see the last day in the end of the air, you can see the air, you can see the way you're going to get the more.\n",
      "2022-08-15 21:21:36,099 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:21:36,103 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:21:36,103 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:21:36,103 - INFO - joeynmt.training - \tHypothesis: just know that james's still going to be able to be able to the middle of the street or the u.s. or the most important thing that are going to be in the u.k. or the u.s., the most important generation of the world that we've seen the world that we've seen the most important part of the world that we've seen in the world.\n",
      "2022-08-15 21:21:36,104 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:21:36,106 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:21:36,107 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:21:36,107 - INFO - joeynmt.training - \tHypothesis: and of course, we all have all kinds of tools.\n",
      "2022-08-15 21:21:36,107 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:21:36,110 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:21:36,110 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:21:36,110 - INFO - joeynmt.training - \tHypothesis: we're going to get all -- we're going to get children to live.\n",
      "2022-08-15 21:21:36,110 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:21:36,113 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:21:36,113 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:21:36,113 - INFO - joeynmt.training - \tHypothesis: we're going to go through the end of the field.\n",
      "2022-08-15 21:21:58,918 - INFO - joeynmt.training - Epoch   3, Step:     7100, Batch Loss:     3.671573, Batch Acc: 0.386507, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2022-08-15 21:22:21,243 - INFO - joeynmt.training - Epoch   3, Step:     7200, Batch Loss:     3.271559, Batch Acc: 0.382036, Tokens per Sec:     5467, Lr: 0.000300\n",
      "2022-08-15 21:22:43,249 - INFO - joeynmt.training - Epoch   3, Step:     7300, Batch Loss:     3.696375, Batch Acc: 0.386749, Tokens per Sec:     5565, Lr: 0.000300\n",
      "2022-08-15 21:23:05,337 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     3.304845, Batch Acc: 0.384207, Tokens per Sec:     5461, Lr: 0.000300\n",
      "2022-08-15 21:23:28,335 - INFO - joeynmt.training - Epoch   3, Step:     7500, Batch Loss:     3.739311, Batch Acc: 0.388199, Tokens per Sec:     5422, Lr: 0.000300\n",
      "2022-08-15 21:23:50,528 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     3.655953, Batch Acc: 0.390666, Tokens per Sec:     5424, Lr: 0.000300\n",
      "2022-08-15 21:24:12,747 - INFO - joeynmt.training - Epoch   3, Step:     7700, Batch Loss:     3.545297, Batch Acc: 0.392199, Tokens per Sec:     5497, Lr: 0.000300\n",
      "2022-08-15 21:24:34,792 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     3.268258, Batch Acc: 0.392414, Tokens per Sec:     5469, Lr: 0.000300\n",
      "2022-08-15 21:24:56,960 - INFO - joeynmt.training - Epoch   3, Step:     7900, Batch Loss:     3.383134, Batch Acc: 0.393331, Tokens per Sec:     5498, Lr: 0.000300\n",
      "2022-08-15 21:25:19,141 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     3.424212, Batch Acc: 0.396721, Tokens per Sec:     5469, Lr: 0.000300\n",
      "2022-08-15 21:25:19,142 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:26:25,389 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:26:25,389 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.47, loss:   3.61, ppl:  36.97, acc:   0.40, generation: 65.0265[sec], evaluation: 0.4012[sec]\n",
      "2022-08-15 21:26:25,390 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:26:26,409 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/3000.ckpt\n",
      "2022-08-15 21:26:26,439 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:26:26,444 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:26:26,444 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:26:26,444 - INFO - joeynmt.training - \tHypothesis: you know, one of the big, and a great thing that is the most of the ngos, is to remember the technology that people can remember the old life, and you can see the past the past, you can see the same, you know, they're going to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able\n",
      "2022-08-15 21:26:26,444 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:26:26,448 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:26:26,448 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:26:26,448 - INFO - joeynmt.training - \tHypothesis: just that means that jos-bow is still going to go through the black and the reason of the president of the most of the most of the most of the most of the president or the u.s., or the u.s., the u.s., the universe is not a real idea of the world that we've got a real idea of the world, but we've got a real idea that we've been able to do is that we have a real idea of the world\n",
      "2022-08-15 21:26:26,449 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:26:26,451 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:26:26,452 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:26:26,452 - INFO - joeynmt.training - \tHypothesis: and of course, we all have all kinds of creatures.\n",
      "2022-08-15 21:26:26,452 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:26:26,455 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:26:26,455 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:26:26,455 - INFO - joeynmt.training - \tHypothesis: we're going to get all of them. we're going to live in the world.\n",
      "2022-08-15 21:26:26,455 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:26:26,459 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:26:26,459 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:26:26,459 - INFO - joeynmt.training - \tHypothesis: we're going to go through the production.\n",
      "2022-08-15 21:26:49,268 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     3.556317, Batch Acc: 0.397136, Tokens per Sec:     5151, Lr: 0.000300\n",
      "2022-08-15 21:27:11,940 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     3.351020, Batch Acc: 0.402226, Tokens per Sec:     5314, Lr: 0.000300\n",
      "2022-08-15 21:27:34,069 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     3.791606, Batch Acc: 0.404905, Tokens per Sec:     5519, Lr: 0.000300\n",
      "2022-08-15 21:27:56,270 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     3.582573, Batch Acc: 0.398790, Tokens per Sec:     5418, Lr: 0.000300\n",
      "2022-08-15 21:28:18,410 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     3.286088, Batch Acc: 0.405690, Tokens per Sec:     5649, Lr: 0.000300\n",
      "2022-08-15 21:28:40,468 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     3.296792, Batch Acc: 0.406351, Tokens per Sec:     5654, Lr: 0.000300\n",
      "2022-08-15 21:29:02,792 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     3.483942, Batch Acc: 0.405972, Tokens per Sec:     5480, Lr: 0.000300\n",
      "2022-08-15 21:29:24,916 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     3.203691, Batch Acc: 0.411056, Tokens per Sec:     5440, Lr: 0.000300\n",
      "2022-08-15 21:29:46,873 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     3.022296, Batch Acc: 0.405877, Tokens per Sec:     5555, Lr: 0.000300\n",
      "2022-08-15 21:30:08,876 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     3.367140, Batch Acc: 0.408976, Tokens per Sec:     5513, Lr: 0.000300\n",
      "2022-08-15 21:30:08,876 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:31:19,756 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:31:19,756 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.73, loss:   3.40, ppl:  29.84, acc:   0.42, generation: 69.4907[sec], evaluation: 0.5820[sec]\n",
      "2022-08-15 21:31:19,757 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:31:21,215 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/4000.ckpt\n",
      "2022-08-15 21:31:21,245 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:31:21,249 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:31:21,250 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:31:21,250 - INFO - joeynmt.training - \tHypothesis: you know, one of the great wisdom and a jews in the iran's science of the ngos is working with people who can remind them to the first days, and then the first day they can see the air, they're still going to feel more and they're going to be able to be able to be able to be able to be able to be able to be able to be able to be able to be more and more and more and more and more and more.\n",
      "2022-08-15 21:31:21,250 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:31:21,254 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:31:21,254 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:31:21,254 - INFO - joeynmt.training - \tHypothesis: just the way that james is still going to go through the milky street or the fact of the oldest and the oldest, the oldest are very important, or the u.s., or the u.s., who's still the u.s., but the fact that we've got a lot of the world, but we've got a lot of the world that we've got to call the world, but we have a very important idea of the world, but we have a lot of\n",
      "2022-08-15 21:31:21,254 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:31:21,257 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:31:21,258 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:31:21,258 - INFO - joeynmt.training - \tHypothesis: and of course, we all do all the same need.\n",
      "2022-08-15 21:31:21,258 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:31:21,261 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:31:21,261 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:31:21,261 - INFO - joeynmt.training - \tHypothesis: we're going to be born. we're going to bring kids to the world.\n",
      "2022-08-15 21:31:21,262 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:31:21,264 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:31:21,265 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:31:21,265 - INFO - joeynmt.training - \tHypothesis: we're going to start with the distribution of the.\n",
      "2022-08-15 21:31:43,319 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     3.261004, Batch Acc: 0.412224, Tokens per Sec:     5045, Lr: 0.000300\n",
      "2022-08-15 21:32:05,486 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     3.528770, Batch Acc: 0.412698, Tokens per Sec:     5189, Lr: 0.000300\n",
      "2022-08-15 21:32:27,698 - INFO - joeynmt.training - Epoch   3, Step:     9300, Batch Loss:     3.406806, Batch Acc: 0.411768, Tokens per Sec:     5403, Lr: 0.000300\n",
      "2022-08-15 21:32:49,558 - INFO - joeynmt.training - Epoch   3, Step:     9400, Batch Loss:     3.045623, Batch Acc: 0.415168, Tokens per Sec:     5550, Lr: 0.000300\n",
      "2022-08-15 21:33:12,241 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     2.983365, Batch Acc: 0.422055, Tokens per Sec:     5343, Lr: 0.000300\n",
      "2022-08-15 21:33:33,977 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     2.990468, Batch Acc: 0.419582, Tokens per Sec:     5681, Lr: 0.000300\n",
      "2022-08-15 21:33:56,350 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     3.155892, Batch Acc: 0.418602, Tokens per Sec:     5411, Lr: 0.000300\n",
      "2022-08-15 21:33:56,756 - INFO - joeynmt.training - Epoch   3: total training loss 11139.49\n",
      "2022-08-15 21:33:56,757 - INFO - joeynmt.training - EPOCH 4\n",
      "2022-08-15 21:34:17,997 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     3.158708, Batch Acc: 0.430311, Tokens per Sec:     5659, Lr: 0.000300\n",
      "2022-08-15 21:34:39,622 - INFO - joeynmt.training - Epoch   4, Step:     9900, Batch Loss:     3.116088, Batch Acc: 0.432989, Tokens per Sec:     5594, Lr: 0.000300\n",
      "2022-08-15 21:35:01,904 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     2.982947, Batch Acc: 0.432674, Tokens per Sec:     5600, Lr: 0.000300\n",
      "2022-08-15 21:35:01,905 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:35:59,355 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:35:59,356 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  11.58, loss:   3.27, ppl:  26.27, acc:   0.43, generation: 56.3319[sec], evaluation: 0.3874[sec]\n",
      "2022-08-15 21:35:59,356 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:36:00,322 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/5000.ckpt\n",
      "2022-08-15 21:36:00,356 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:36:00,361 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:36:00,361 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:36:00,361 - INFO - joeynmt.training - \tHypothesis: you know, one of the big pleasure, and a funny thing about the malomas a research research of the people who can remember the old days, the old days, the first day, the time you can see the sun, they can still feel in the past, they can actually feel in the streets of the streets.\n",
      "2022-08-15 21:36:00,361 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:36:00,366 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:36:00,366 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:36:00,366 - INFO - joeynmt.training - \tHypothesis: just the idea that jonke's still over the bottom of the milky street or the idea of the indian devil, or the most of the most of the neanderthals, or the u.k., but the sun is still in the u.s., the u.s. '70s, the fact, the fact, the fact, the idea of the world is not just a very important thing that we're going to call, but we're not a very familiar with a very\n",
      "2022-08-15 21:36:00,366 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:36:00,369 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:36:00,370 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:36:00,370 - INFO - joeynmt.training - \tHypothesis: and of course, we all share all the same need of the same needs.\n",
      "2022-08-15 21:36:00,370 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:36:00,373 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:36:00,373 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:36:00,373 - INFO - joeynmt.training - \tHypothesis: we're going to get all right. we're going to get children on the world.\n",
      "2022-08-15 21:36:00,373 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:36:00,376 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:36:00,376 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:36:00,376 - INFO - joeynmt.training - \tHypothesis: we're working through the foundation.\n",
      "2022-08-15 21:36:22,416 - INFO - joeynmt.training - Epoch   4, Step:    10100, Batch Loss:     3.087469, Batch Acc: 0.430470, Tokens per Sec:     5300, Lr: 0.000300\n",
      "2022-08-15 21:36:44,023 - INFO - joeynmt.training - Epoch   4, Step:    10200, Batch Loss:     2.973831, Batch Acc: 0.431801, Tokens per Sec:     5435, Lr: 0.000300\n",
      "2022-08-15 21:37:06,426 - INFO - joeynmt.training - Epoch   4, Step:    10300, Batch Loss:     3.062661, Batch Acc: 0.436626, Tokens per Sec:     5279, Lr: 0.000300\n",
      "2022-08-15 21:37:29,713 - INFO - joeynmt.training - Epoch   4, Step:    10400, Batch Loss:     2.994776, Batch Acc: 0.435365, Tokens per Sec:     5146, Lr: 0.000300\n",
      "2022-08-15 21:37:51,381 - INFO - joeynmt.training - Epoch   4, Step:    10500, Batch Loss:     3.039037, Batch Acc: 0.437670, Tokens per Sec:     5631, Lr: 0.000300\n",
      "2022-08-15 21:38:13,177 - INFO - joeynmt.training - Epoch   4, Step:    10600, Batch Loss:     3.000211, Batch Acc: 0.433456, Tokens per Sec:     5573, Lr: 0.000300\n",
      "2022-08-15 21:38:34,874 - INFO - joeynmt.training - Epoch   4, Step:    10700, Batch Loss:     2.796664, Batch Acc: 0.436913, Tokens per Sec:     5543, Lr: 0.000300\n",
      "2022-08-15 21:38:57,183 - INFO - joeynmt.training - Epoch   4, Step:    10800, Batch Loss:     2.933029, Batch Acc: 0.441572, Tokens per Sec:     5451, Lr: 0.000300\n",
      "2022-08-15 21:39:19,134 - INFO - joeynmt.training - Epoch   4, Step:    10900, Batch Loss:     3.165184, Batch Acc: 0.438605, Tokens per Sec:     5397, Lr: 0.000300\n",
      "2022-08-15 21:39:41,506 - INFO - joeynmt.training - Epoch   4, Step:    11000, Batch Loss:     3.002946, Batch Acc: 0.441249, Tokens per Sec:     5242, Lr: 0.000300\n",
      "2022-08-15 21:39:41,507 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:40:47,213 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:40:47,213 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  12.47, loss:   3.16, ppl:  23.58, acc:   0.44, generation: 64.5295[sec], evaluation: 0.4085[sec]\n",
      "2022-08-15 21:40:47,214 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:40:48,183 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/6000.ckpt\n",
      "2022-08-15 21:40:48,215 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:40:48,219 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:40:48,219 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:40:48,219 - INFO - joeynmt.training - \tHypothesis: you know, one of the big pleasure in the world and a strange deal with the nobel research is working with people who can remember the old days, and they can remember the first time, they can be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able\n",
      "2022-08-15 21:40:48,220 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:40:48,224 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:40:48,224 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:40:48,224 - INFO - joeynmt.training - \tHypothesis: just the idea that jesers still still go beyond the milky road or the importance of the military who are still in the world, or the sweden of the mars of the world, the world is still still in the world, the world, the most important idea of the world, but we've never seen in the world, but we've been a lot of the world, in the world, but we've been able to call the world, in the world, we have a\n",
      "2022-08-15 21:40:48,224 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:40:48,227 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:40:48,228 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:40:48,228 - INFO - joeynmt.training - \tHypothesis: and of course, we all share the same need of the diversity.\n",
      "2022-08-15 21:40:48,228 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:40:48,231 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:40:48,231 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:40:48,231 - INFO - joeynmt.training - \tHypothesis: we're going to be born. we're going to bring kids to the world.\n",
      "2022-08-15 21:40:48,231 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:40:48,234 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:40:48,234 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:40:48,234 - INFO - joeynmt.training - \tHypothesis: we're going to go through the headlines.\n",
      "2022-08-15 21:41:10,635 - INFO - joeynmt.training - Epoch   4, Step:    11100, Batch Loss:     2.589802, Batch Acc: 0.447735, Tokens per Sec:     5281, Lr: 0.000300\n",
      "2022-08-15 21:41:33,187 - INFO - joeynmt.training - Epoch   4, Step:    11200, Batch Loss:     2.740834, Batch Acc: 0.444151, Tokens per Sec:     5439, Lr: 0.000300\n",
      "2022-08-15 21:41:56,124 - INFO - joeynmt.training - Epoch   4, Step:    11300, Batch Loss:     2.693206, Batch Acc: 0.445249, Tokens per Sec:     5364, Lr: 0.000300\n",
      "2022-08-15 21:42:18,391 - INFO - joeynmt.training - Epoch   4, Step:    11400, Batch Loss:     2.957885, Batch Acc: 0.449637, Tokens per Sec:     5429, Lr: 0.000300\n",
      "2022-08-15 21:42:40,352 - INFO - joeynmt.training - Epoch   4, Step:    11500, Batch Loss:     2.676431, Batch Acc: 0.447427, Tokens per Sec:     5471, Lr: 0.000300\n",
      "2022-08-15 21:43:02,672 - INFO - joeynmt.training - Epoch   4, Step:    11600, Batch Loss:     3.224316, Batch Acc: 0.448411, Tokens per Sec:     5467, Lr: 0.000300\n",
      "2022-08-15 21:43:24,594 - INFO - joeynmt.training - Epoch   4, Step:    11700, Batch Loss:     2.850996, Batch Acc: 0.454248, Tokens per Sec:     5657, Lr: 0.000300\n",
      "2022-08-15 21:43:46,800 - INFO - joeynmt.training - Epoch   4, Step:    11800, Batch Loss:     2.938967, Batch Acc: 0.451869, Tokens per Sec:     5543, Lr: 0.000300\n",
      "2022-08-15 21:44:09,146 - INFO - joeynmt.training - Epoch   4, Step:    11900, Batch Loss:     3.267295, Batch Acc: 0.451995, Tokens per Sec:     5389, Lr: 0.000300\n",
      "2022-08-15 21:44:31,416 - INFO - joeynmt.training - Epoch   4, Step:    12000, Batch Loss:     2.866504, Batch Acc: 0.457164, Tokens per Sec:     5485, Lr: 0.000300\n",
      "2022-08-15 21:44:31,417 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:45:27,108 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:45:27,109 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  13.47, loss:   3.05, ppl:  21.03, acc:   0.46, generation: 54.5433[sec], evaluation: 0.4129[sec]\n",
      "2022-08-15 21:45:27,109 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:45:28,584 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/7000.ckpt\n",
      "2022-08-15 21:45:28,616 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:45:28,620 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:45:28,620 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:45:28,620 - INFO - joeynmt.training - \tHypothesis: you know, one of the big pleasure and a jewish of the ceology research is to be in the human life, to remember the old days, the past days, and the past days, they can still feel in the air, and they feel in the air, they're going to feel more and more and more and more and more and more of the weight.\n",
      "2022-08-15 21:45:28,620 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:45:28,624 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:45:28,624 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:45:28,624 - INFO - joeynmt.training - \tHypothesis: just the way that the knowledge of jony is still beyond the milky street or the meaning of the oldest indian indian, or that the oldest leaders are still in the himalayas, but also the buddhist, the fact, the reality of the world, the nature of the world, is not a sense of the world, but we have a very important idea of the impact, but we have a kind of view of view of the world, but we have a kind of\n",
      "2022-08-15 21:45:28,624 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:45:28,627 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:45:28,627 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:45:28,627 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same dynamic.\n",
      "2022-08-15 21:45:28,628 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:45:28,630 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:45:28,630 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:45:28,631 - INFO - joeynmt.training - \tHypothesis: we're going to be born. we're going to bring kids to the world.\n",
      "2022-08-15 21:45:28,631 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:45:28,633 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:45:28,634 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:45:28,634 - INFO - joeynmt.training - \tHypothesis: we're working through the discovery of crime.\n",
      "2022-08-15 21:45:50,434 - INFO - joeynmt.training - Epoch   4, Step:    12100, Batch Loss:     2.836247, Batch Acc: 0.453478, Tokens per Sec:     5255, Lr: 0.000300\n",
      "2022-08-15 21:46:13,547 - INFO - joeynmt.training - Epoch   4, Step:    12200, Batch Loss:     3.160439, Batch Acc: 0.459634, Tokens per Sec:     5206, Lr: 0.000300\n",
      "2022-08-15 21:46:35,860 - INFO - joeynmt.training - Epoch   4, Step:    12300, Batch Loss:     2.982396, Batch Acc: 0.457087, Tokens per Sec:     5391, Lr: 0.000300\n",
      "2022-08-15 21:46:57,863 - INFO - joeynmt.training - Epoch   4, Step:    12400, Batch Loss:     3.052760, Batch Acc: 0.456943, Tokens per Sec:     5489, Lr: 0.000300\n",
      "2022-08-15 21:47:20,641 - INFO - joeynmt.training - Epoch   4, Step:    12500, Batch Loss:     2.793053, Batch Acc: 0.459972, Tokens per Sec:     5393, Lr: 0.000300\n",
      "2022-08-15 21:47:42,653 - INFO - joeynmt.training - Epoch   4, Step:    12600, Batch Loss:     2.657018, Batch Acc: 0.460994, Tokens per Sec:     5503, Lr: 0.000300\n",
      "2022-08-15 21:48:04,484 - INFO - joeynmt.training - Epoch   4, Step:    12700, Batch Loss:     3.030612, Batch Acc: 0.460315, Tokens per Sec:     5618, Lr: 0.000300\n",
      "2022-08-15 21:48:26,917 - INFO - joeynmt.training - Epoch   4, Step:    12800, Batch Loss:     2.888421, Batch Acc: 0.465402, Tokens per Sec:     5515, Lr: 0.000300\n",
      "2022-08-15 21:48:49,031 - INFO - joeynmt.training - Epoch   4, Step:    12900, Batch Loss:     2.641906, Batch Acc: 0.464181, Tokens per Sec:     5366, Lr: 0.000300\n",
      "2022-08-15 21:48:55,767 - INFO - joeynmt.training - Epoch   4: total training loss 9456.12\n",
      "2022-08-15 21:48:55,768 - INFO - joeynmt.training - EPOCH 5\n",
      "2022-08-15 21:49:10,819 - INFO - joeynmt.training - Epoch   5, Step:    13000, Batch Loss:     2.947210, Batch Acc: 0.477996, Tokens per Sec:     5445, Lr: 0.000300\n",
      "2022-08-15 21:49:10,819 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:50:12,019 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:50:12,020 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  14.32, loss:   2.96, ppl:  19.38, acc:   0.47, generation: 60.0792[sec], evaluation: 0.3882[sec]\n",
      "2022-08-15 21:50:12,021 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:50:12,963 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/8000.ckpt\n",
      "2022-08-15 21:50:12,966 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:50:12,971 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:50:12,971 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:50:12,972 - INFO - joeynmt.training - \tHypothesis: you know, one of the big pleasure and a funny thing about the ethician research is to see the people who can still remember the old days. the past days, they can still feel their oceans, and they're still in the wind, they're still feeling of the streets of the animals, and they're going to touch the animals.\n",
      "2022-08-15 21:50:12,972 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:50:12,976 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:50:12,977 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:50:12,977 - INFO - joeynmt.training - \tHypothesis: just the idea that jonny lockates still beyond the milky street or the importance of the oldest indian inese inese inese, or that the himalayas are still in the world, or the world is still the same part of the buddhist, the fact that the reality is that we've ever been a sense of the world, but that we've just been a sense of a sense of view of view of view of a sense of reality, but that we have\n",
      "2022-08-15 21:50:12,977 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:50:12,981 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:50:12,981 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:50:12,981 - INFO - joeynmt.training - \tHypothesis: and of course, we all share the same need of communication.\n",
      "2022-08-15 21:50:12,981 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:50:12,984 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:50:12,985 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:50:12,985 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to get kids to the world.\n",
      "2022-08-15 21:50:12,985 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:50:12,988 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:50:12,988 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:50:12,988 - INFO - joeynmt.training - \tHypothesis: we're working through the crime.\n",
      "2022-08-15 21:50:39,367 - INFO - joeynmt.training - Epoch   5, Step:    13100, Batch Loss:     3.010553, Batch Acc: 0.475602, Tokens per Sec:     4414, Lr: 0.000300\n",
      "2022-08-15 21:51:00,944 - INFO - joeynmt.training - Epoch   5, Step:    13200, Batch Loss:     2.685570, Batch Acc: 0.477885, Tokens per Sec:     5760, Lr: 0.000300\n",
      "2022-08-15 21:51:23,162 - INFO - joeynmt.training - Epoch   5, Step:    13300, Batch Loss:     2.581715, Batch Acc: 0.476177, Tokens per Sec:     5493, Lr: 0.000300\n",
      "2022-08-15 21:51:45,658 - INFO - joeynmt.training - Epoch   5, Step:    13400, Batch Loss:     2.625708, Batch Acc: 0.479749, Tokens per Sec:     5494, Lr: 0.000300\n",
      "2022-08-15 21:52:08,370 - INFO - joeynmt.training - Epoch   5, Step:    13500, Batch Loss:     2.722531, Batch Acc: 0.479676, Tokens per Sec:     5298, Lr: 0.000300\n",
      "2022-08-15 21:52:30,280 - INFO - joeynmt.training - Epoch   5, Step:    13600, Batch Loss:     2.566567, Batch Acc: 0.475308, Tokens per Sec:     5666, Lr: 0.000300\n",
      "2022-08-15 21:52:52,784 - INFO - joeynmt.training - Epoch   5, Step:    13700, Batch Loss:     2.569329, Batch Acc: 0.479477, Tokens per Sec:     5440, Lr: 0.000300\n",
      "2022-08-15 21:53:14,715 - INFO - joeynmt.training - Epoch   5, Step:    13800, Batch Loss:     2.730641, Batch Acc: 0.478944, Tokens per Sec:     5681, Lr: 0.000300\n",
      "2022-08-15 21:53:36,751 - INFO - joeynmt.training - Epoch   5, Step:    13900, Batch Loss:     2.778728, Batch Acc: 0.480192, Tokens per Sec:     5423, Lr: 0.000300\n",
      "2022-08-15 21:53:58,905 - INFO - joeynmt.training - Epoch   5, Step:    14000, Batch Loss:     2.473256, Batch Acc: 0.484083, Tokens per Sec:     5449, Lr: 0.000300\n",
      "2022-08-15 21:53:58,905 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:54:52,353 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:54:52,354 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  15.29, loss:   2.89, ppl:  17.99, acc:   0.48, generation: 52.3452[sec], evaluation: 0.3976[sec]\n",
      "2022-08-15 21:54:52,355 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:54:53,430 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/9000.ckpt\n",
      "2022-08-15 21:54:53,433 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:54:53,439 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:54:53,439 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:54:53,439 - INFO - joeynmt.training - \tHypothesis: you know, one of the big pleasure of the time and a freudian research is working with the ethicic research, which can remember the old days, and the time you can still feel in the wind, they're still feeling on the air, they're going to touch the back of the plants.\n",
      "2022-08-15 21:54:53,439 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:54:53,443 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:54:53,443 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:54:53,443 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jupiter is still beyond the milky street or the meaning of the western inese inese inese, is still concerned about the ialayas or the ii still, the ii still has always been to follow the central, the fact that we call a part of the world, but that is a very important idea that we're going to call a part of the world, but that we're not a very important thing that is a very important thing that we\n",
      "2022-08-15 21:54:53,443 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:54:53,446 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:54:53,447 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:54:53,447 - INFO - joeynmt.training - \tHypothesis: and of course, we all share the same adaptation.\n",
      "2022-08-15 21:54:53,447 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:54:53,450 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:54:53,450 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:54:53,450 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring kids to the world.\n",
      "2022-08-15 21:54:53,450 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:54:53,453 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:54:53,453 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:54:53,454 - INFO - joeynmt.training - \tHypothesis: we're moving through observations.\n",
      "2022-08-15 21:55:15,179 - INFO - joeynmt.training - Epoch   5, Step:    14100, Batch Loss:     2.654949, Batch Acc: 0.481355, Tokens per Sec:     5300, Lr: 0.000300\n",
      "2022-08-15 21:55:37,524 - INFO - joeynmt.training - Epoch   5, Step:    14200, Batch Loss:     2.759406, Batch Acc: 0.481488, Tokens per Sec:     5401, Lr: 0.000300\n",
      "2022-08-15 21:55:59,430 - INFO - joeynmt.training - Epoch   5, Step:    14300, Batch Loss:     2.631889, Batch Acc: 0.481540, Tokens per Sec:     5506, Lr: 0.000300\n",
      "2022-08-15 21:56:21,304 - INFO - joeynmt.training - Epoch   5, Step:    14400, Batch Loss:     2.480169, Batch Acc: 0.485077, Tokens per Sec:     5515, Lr: 0.000300\n",
      "2022-08-15 21:56:43,403 - INFO - joeynmt.training - Epoch   5, Step:    14500, Batch Loss:     2.671555, Batch Acc: 0.484180, Tokens per Sec:     5488, Lr: 0.000300\n",
      "2022-08-15 21:57:06,785 - INFO - joeynmt.training - Epoch   5, Step:    14600, Batch Loss:     2.648245, Batch Acc: 0.485594, Tokens per Sec:     5161, Lr: 0.000300\n",
      "2022-08-15 21:57:28,848 - INFO - joeynmt.training - Epoch   5, Step:    14700, Batch Loss:     2.684588, Batch Acc: 0.489061, Tokens per Sec:     5524, Lr: 0.000300\n",
      "2022-08-15 21:57:51,062 - INFO - joeynmt.training - Epoch   5, Step:    14800, Batch Loss:     2.628777, Batch Acc: 0.487802, Tokens per Sec:     5469, Lr: 0.000300\n",
      "2022-08-15 21:58:12,972 - INFO - joeynmt.training - Epoch   5, Step:    14900, Batch Loss:     2.587084, Batch Acc: 0.487766, Tokens per Sec:     5649, Lr: 0.000300\n",
      "2022-08-15 21:58:35,018 - INFO - joeynmt.training - Epoch   5, Step:    15000, Batch Loss:     2.786029, Batch Acc: 0.485204, Tokens per Sec:     5391, Lr: 0.000300\n",
      "2022-08-15 21:58:35,019 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 21:59:27,357 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 21:59:27,358 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  15.74, loss:   2.83, ppl:  16.95, acc:   0.49, generation: 51.0550[sec], evaluation: 0.5946[sec]\n",
      "2022-08-15 21:59:27,358 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 21:59:28,462 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/10000.ckpt\n",
      "2022-08-15 21:59:28,477 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 21:59:28,484 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 21:59:28,485 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 21:59:28,485 - INFO - joeynmt.training - \tHypothesis: you know, one of the big pleasure in the home and a funny of the ethology research is to remember the people who can remember the old days, and their past, they're still feeling of the air, they're still comfortable with the air, and they're going to touch the hard teeth.\n",
      "2022-08-15 21:59:28,485 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 21:59:28,489 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 21:59:28,489 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 21:59:28,489 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jupiter still is over the milky street or the meaning of the oldest inese inese inese, is more important to the himalayas, or the ipoma, who still is still the central of the ima, the general idea of the environment, which is that we call a certain idea of the world, but we're just a very important idea of a very popular sense of a human life, but a very, which is a very important idea\n",
      "2022-08-15 21:59:28,489 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 21:59:28,493 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 21:59:28,493 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 21:59:28,493 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptions.\n",
      "2022-08-15 21:59:28,493 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 21:59:28,496 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 21:59:28,497 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 21:59:28,497 - INFO - joeynmt.training - \tHypothesis: we're going to be born. we get kids to the world.\n",
      "2022-08-15 21:59:28,497 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 21:59:28,500 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 21:59:28,500 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 21:59:28,501 - INFO - joeynmt.training - \tHypothesis: we're moving through rituals.\n",
      "2022-08-15 21:59:51,645 - INFO - joeynmt.training - Epoch   5, Step:    15100, Batch Loss:     2.405597, Batch Acc: 0.488766, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2022-08-15 22:00:13,954 - INFO - joeynmt.training - Epoch   5, Step:    15200, Batch Loss:     2.784413, Batch Acc: 0.492537, Tokens per Sec:     5490, Lr: 0.000300\n",
      "2022-08-15 22:00:36,206 - INFO - joeynmt.training - Epoch   5, Step:    15300, Batch Loss:     2.527466, Batch Acc: 0.489652, Tokens per Sec:     5557, Lr: 0.000300\n",
      "2022-08-15 22:00:58,081 - INFO - joeynmt.training - Epoch   5, Step:    15400, Batch Loss:     2.607036, Batch Acc: 0.491995, Tokens per Sec:     5505, Lr: 0.000300\n",
      "2022-08-15 22:01:20,966 - INFO - joeynmt.training - Epoch   5, Step:    15500, Batch Loss:     2.522867, Batch Acc: 0.494999, Tokens per Sec:     5312, Lr: 0.000300\n",
      "2022-08-15 22:01:42,962 - INFO - joeynmt.training - Epoch   5, Step:    15600, Batch Loss:     2.713599, Batch Acc: 0.493034, Tokens per Sec:     5476, Lr: 0.000300\n",
      "2022-08-15 22:02:05,629 - INFO - joeynmt.training - Epoch   5, Step:    15700, Batch Loss:     2.727018, Batch Acc: 0.491853, Tokens per Sec:     5291, Lr: 0.000300\n",
      "2022-08-15 22:02:27,492 - INFO - joeynmt.training - Epoch   5, Step:    15800, Batch Loss:     2.541044, Batch Acc: 0.494506, Tokens per Sec:     5587, Lr: 0.000300\n",
      "2022-08-15 22:02:49,497 - INFO - joeynmt.training - Epoch   5, Step:    15900, Batch Loss:     2.564834, Batch Acc: 0.498515, Tokens per Sec:     5570, Lr: 0.000300\n",
      "2022-08-15 22:03:11,350 - INFO - joeynmt.training - Epoch   5, Step:    16000, Batch Loss:     2.481617, Batch Acc: 0.498914, Tokens per Sec:     5518, Lr: 0.000300\n",
      "2022-08-15 22:03:11,351 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:04:03,442 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:04:03,442 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  16.33, loss:   2.78, ppl:  16.12, acc:   0.50, generation: 50.9923[sec], evaluation: 0.3921[sec]\n",
      "2022-08-15 22:04:03,443 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 22:04:04,510 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/11000.ckpt\n",
      "2022-08-15 22:04:04,514 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:04:04,519 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:04:04,520 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:04:04,520 - INFO - joeynmt.training - \tHypothesis: you know, one of the big pleasure in the school and a freuthy research is in the society of people who can remember the old days, and the past, they still feel in the wind, they're still touching the seeds of the dust, and they touch the animals in the water.\n",
      "2022-08-15 22:04:04,520 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:04:04,524 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:04:04,524 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:04:04,524 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jinar is still beyond the milky road or the importance of myniininese, the oldest inese inese, is still a sense of the himalayan, who is still still the fatima, who's the general principle of the reality of the world, but that we're not the fact that we're going to call a certain idea of the world, but that we're not the fact that we're not the fact that we're going to\n",
      "2022-08-15 22:04:04,524 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:04:04,527 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:04:04,527 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:04:04,527 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptive needs.\n",
      "2022-08-15 22:04:04,528 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:04:04,531 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:04:04,531 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:04:04,531 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring kids to the world.\n",
      "2022-08-15 22:04:04,531 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:04:04,534 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:04:04,534 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:04:04,534 - INFO - joeynmt.training - \tHypothesis: we're through ritual.\n",
      "2022-08-15 22:04:27,301 - INFO - joeynmt.training - Epoch   5, Step:    16100, Batch Loss:     2.555265, Batch Acc: 0.496609, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2022-08-15 22:04:40,341 - INFO - joeynmt.training - Epoch   5: total training loss 8480.15\n",
      "2022-08-15 22:04:40,341 - INFO - joeynmt.training - EPOCH 6\n",
      "2022-08-15 22:04:49,790 - INFO - joeynmt.training - Epoch   6, Step:    16200, Batch Loss:     2.365241, Batch Acc: 0.505602, Tokens per Sec:     5281, Lr: 0.000300\n",
      "2022-08-15 22:05:13,377 - INFO - joeynmt.training - Epoch   6, Step:    16300, Batch Loss:     2.530145, Batch Acc: 0.508172, Tokens per Sec:     5230, Lr: 0.000300\n",
      "2022-08-15 22:05:35,777 - INFO - joeynmt.training - Epoch   6, Step:    16400, Batch Loss:     2.457617, Batch Acc: 0.510615, Tokens per Sec:     5421, Lr: 0.000300\n",
      "2022-08-15 22:05:58,784 - INFO - joeynmt.training - Epoch   6, Step:    16500, Batch Loss:     2.460823, Batch Acc: 0.508137, Tokens per Sec:     5264, Lr: 0.000300\n",
      "2022-08-15 22:06:21,101 - INFO - joeynmt.training - Epoch   6, Step:    16600, Batch Loss:     2.317833, Batch Acc: 0.509707, Tokens per Sec:     5500, Lr: 0.000300\n",
      "2022-08-15 22:06:43,401 - INFO - joeynmt.training - Epoch   6, Step:    16700, Batch Loss:     2.481067, Batch Acc: 0.515072, Tokens per Sec:     5407, Lr: 0.000300\n",
      "2022-08-15 22:07:06,019 - INFO - joeynmt.training - Epoch   6, Step:    16800, Batch Loss:     2.569281, Batch Acc: 0.508480, Tokens per Sec:     5381, Lr: 0.000300\n",
      "2022-08-15 22:07:28,562 - INFO - joeynmt.training - Epoch   6, Step:    16900, Batch Loss:     2.314403, Batch Acc: 0.509396, Tokens per Sec:     5375, Lr: 0.000300\n",
      "2022-08-15 22:07:51,837 - INFO - joeynmt.training - Epoch   6, Step:    17000, Batch Loss:     2.651689, Batch Acc: 0.509382, Tokens per Sec:     5328, Lr: 0.000300\n",
      "2022-08-15 22:07:51,837 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:08:48,665 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:08:48,666 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  16.76, loss:   2.73, ppl:  15.38, acc:   0.50, generation: 55.7107[sec], evaluation: 0.3956[sec]\n",
      "2022-08-15 22:08:48,666 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 22:08:50,188 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/12000.ckpt\n",
      "2022-08-15 22:08:50,193 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:08:50,197 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:08:50,197 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:08:50,197 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure, and a of the freuonology research is to be involved with people who can remember the old days, and the past time they feel in the rain, they're still touched in the air, they touch the blocks of animals.\n",
      "2022-08-15 22:08:50,198 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:08:50,201 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:08:50,201 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:08:50,202 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jesloy still is still beyond the milky street or the importance of the myinininese who are still shared, or in the himalayas, still the most important part of the fbean, the central principle of the philosophy of the philosophy of view, which is that we chose to call a part of the world, but we're not only part of the world, but we're really, is a part of the world, that we're really important\n",
      "2022-08-15 22:08:50,202 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:08:50,205 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:08:50,205 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:08:50,205 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptions.\n",
      "2022-08-15 22:08:50,205 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:08:50,208 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:08:50,208 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:08:50,209 - INFO - joeynmt.training - \tHypothesis: we're going to be born. we're going to bring kids to the world.\n",
      "2022-08-15 22:08:50,209 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:08:50,211 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:08:50,212 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:08:50,212 - INFO - joeynmt.training - \tHypothesis: we're running through rituals.\n",
      "2022-08-15 22:09:12,346 - INFO - joeynmt.training - Epoch   6, Step:    17100, Batch Loss:     2.354750, Batch Acc: 0.507304, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2022-08-15 22:09:35,434 - INFO - joeynmt.training - Epoch   6, Step:    17200, Batch Loss:     2.753189, Batch Acc: 0.511938, Tokens per Sec:     5244, Lr: 0.000300\n",
      "2022-08-15 22:09:59,116 - INFO - joeynmt.training - Epoch   6, Step:    17300, Batch Loss:     2.355906, Batch Acc: 0.511925, Tokens per Sec:     5167, Lr: 0.000300\n",
      "2022-08-15 22:10:22,533 - INFO - joeynmt.training - Epoch   6, Step:    17400, Batch Loss:     2.418072, Batch Acc: 0.513417, Tokens per Sec:     5177, Lr: 0.000300\n",
      "2022-08-15 22:10:44,782 - INFO - joeynmt.training - Epoch   6, Step:    17500, Batch Loss:     2.480746, Batch Acc: 0.511657, Tokens per Sec:     5593, Lr: 0.000300\n",
      "2022-08-15 22:11:06,859 - INFO - joeynmt.training - Epoch   6, Step:    17600, Batch Loss:     2.711494, Batch Acc: 0.514629, Tokens per Sec:     5513, Lr: 0.000300\n",
      "2022-08-15 22:11:29,202 - INFO - joeynmt.training - Epoch   6, Step:    17700, Batch Loss:     2.435160, Batch Acc: 0.514499, Tokens per Sec:     5419, Lr: 0.000300\n",
      "2022-08-15 22:11:51,966 - INFO - joeynmt.training - Epoch   6, Step:    17800, Batch Loss:     2.580814, Batch Acc: 0.514350, Tokens per Sec:     5480, Lr: 0.000300\n",
      "2022-08-15 22:12:14,935 - INFO - joeynmt.training - Epoch   6, Step:    17900, Batch Loss:     2.255725, Batch Acc: 0.510096, Tokens per Sec:     5250, Lr: 0.000300\n",
      "2022-08-15 22:12:37,513 - INFO - joeynmt.training - Epoch   6, Step:    18000, Batch Loss:     2.304640, Batch Acc: 0.511561, Tokens per Sec:     5358, Lr: 0.000300\n",
      "2022-08-15 22:12:37,514 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:13:33,239 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:13:33,240 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.47, loss:   2.71, ppl:  14.96, acc:   0.51, generation: 54.6041[sec], evaluation: 0.4019[sec]\n",
      "2022-08-15 22:13:33,240 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 22:13:34,463 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/13000.ckpt\n",
      "2022-08-15 22:13:34,485 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:13:34,490 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:13:34,490 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:13:34,490 - INFO - joeynmt.training - \tHypothesis: you know, one of the great lessons about travel and a freuthy research is in the ethology research, in collaboration with people who can remember the old days, and they still feel in the past, they're feeling in the wind, they're feeling in the snow, they're feeling in the animals, they're feeling more resilient.\n",
      "2022-08-15 22:13:34,490 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:13:34,494 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:13:34,494 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:13:34,495 - INFO - joeynmt.training - \tHypothesis: just the knowledge that johanese schabed still over the milky street or the meaning of the oldest inuit is still a lot of meaning, or the himalayan kingdom still still still the end of the fatima, the center of the discovery of the discovery of the philosophy, the discovery of the reality, is that we have a sense of the world's understanding of a very powerful idea of the world, but the reality of the world of the world of the world.\n",
      "2022-08-15 22:13:34,495 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:13:34,498 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:13:34,498 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:13:34,498 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptation.\n",
      "2022-08-15 22:13:34,499 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:13:34,502 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:13:34,502 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:13:34,502 - INFO - joeynmt.training - \tHypothesis: we're all born. we bring kids to the world.\n",
      "2022-08-15 22:13:34,502 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:13:34,505 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:13:34,506 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:13:34,506 - INFO - joeynmt.training - \tHypothesis: we're starting to explore rituals.\n",
      "2022-08-15 22:13:57,249 - INFO - joeynmt.training - Epoch   6, Step:    18100, Batch Loss:     2.338006, Batch Acc: 0.515672, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2022-08-15 22:14:20,224 - INFO - joeynmt.training - Epoch   6, Step:    18200, Batch Loss:     2.342691, Batch Acc: 0.512899, Tokens per Sec:     5354, Lr: 0.000300\n",
      "2022-08-15 22:14:43,644 - INFO - joeynmt.training - Epoch   6, Step:    18300, Batch Loss:     2.505372, Batch Acc: 0.516848, Tokens per Sec:     5185, Lr: 0.000300\n",
      "2022-08-15 22:15:06,154 - INFO - joeynmt.training - Epoch   6, Step:    18400, Batch Loss:     2.318372, Batch Acc: 0.511848, Tokens per Sec:     5356, Lr: 0.000300\n",
      "2022-08-15 22:15:28,652 - INFO - joeynmt.training - Epoch   6, Step:    18500, Batch Loss:     2.618811, Batch Acc: 0.517604, Tokens per Sec:     5472, Lr: 0.000300\n",
      "2022-08-15 22:15:51,651 - INFO - joeynmt.training - Epoch   6, Step:    18600, Batch Loss:     2.360694, Batch Acc: 0.514791, Tokens per Sec:     5288, Lr: 0.000300\n",
      "2022-08-15 22:16:14,057 - INFO - joeynmt.training - Epoch   6, Step:    18700, Batch Loss:     2.335631, Batch Acc: 0.515192, Tokens per Sec:     5523, Lr: 0.000300\n",
      "2022-08-15 22:16:36,743 - INFO - joeynmt.training - Epoch   6, Step:    18800, Batch Loss:     2.406525, Batch Acc: 0.520386, Tokens per Sec:     5367, Lr: 0.000300\n",
      "2022-08-15 22:17:00,073 - INFO - joeynmt.training - Epoch   6, Step:    18900, Batch Loss:     2.422742, Batch Acc: 0.519511, Tokens per Sec:     5291, Lr: 0.000300\n",
      "2022-08-15 22:17:22,198 - INFO - joeynmt.training - Epoch   6, Step:    19000, Batch Loss:     2.362300, Batch Acc: 0.514744, Tokens per Sec:     5346, Lr: 0.000300\n",
      "2022-08-15 22:17:22,199 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:18:19,562 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:18:19,562 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.69, loss:   2.66, ppl:  14.29, acc:   0.51, generation: 55.5210[sec], evaluation: 0.7303[sec]\n",
      "2022-08-15 22:18:19,563 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 22:18:20,691 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/14000.ckpt\n",
      "2022-08-15 22:18:20,695 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:18:20,700 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:18:20,700 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:18:20,700 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of travel and a funny thing that's in ethesgraphic research, with people who can remember the old days, and the past, they're still feeling at the wind, they're moving on the snow, and they're moving into the back of the sticky blocks, and they're moving into the more and they're moving.\n",
      "2022-08-15 22:18:20,700 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:18:20,705 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:18:20,705 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:18:20,705 - INFO - joeynmt.training - \tHypothesis: just the idea that jinese schabed still is beyond the milky road or the meaning of the oldest inese inese is full of meaning, or that in the himalayas still is the buddhist of the harma, the center of the revisma, the center of the memory, the center of the reality, which is that we have a tendency to call a reality, but that we're really, in a sense of a sense of a sense of a sense of\n",
      "2022-08-15 22:18:20,705 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:18:20,709 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:18:20,709 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:18:20,709 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 22:18:20,710 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:18:20,713 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:18:20,713 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:18:20,713 - INFO - joeynmt.training - \tHypothesis: we're going to be born. we bring kids to the world.\n",
      "2022-08-15 22:18:20,713 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:18:20,716 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:18:20,716 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:18:20,717 - INFO - joeynmt.training - \tHypothesis: we're going to go through the ritual.\n",
      "2022-08-15 22:18:44,234 - INFO - joeynmt.training - Epoch   6, Step:    19100, Batch Loss:     2.142468, Batch Acc: 0.520594, Tokens per Sec:     4935, Lr: 0.000300\n",
      "2022-08-15 22:19:05,935 - INFO - joeynmt.training - Epoch   6, Step:    19200, Batch Loss:     2.663957, Batch Acc: 0.513987, Tokens per Sec:     5662, Lr: 0.000300\n",
      "2022-08-15 22:19:28,063 - INFO - joeynmt.training - Epoch   6, Step:    19300, Batch Loss:     2.366625, Batch Acc: 0.522563, Tokens per Sec:     5562, Lr: 0.000300\n",
      "2022-08-15 22:19:45,060 - INFO - joeynmt.training - Epoch   6: total training loss 7847.01\n",
      "2022-08-15 22:19:45,061 - INFO - joeynmt.training - EPOCH 7\n",
      "2022-08-15 22:19:50,392 - INFO - joeynmt.training - Epoch   7, Step:    19400, Batch Loss:     2.285515, Batch Acc: 0.525958, Tokens per Sec:     5627, Lr: 0.000300\n",
      "2022-08-15 22:20:12,706 - INFO - joeynmt.training - Epoch   7, Step:    19500, Batch Loss:     2.132035, Batch Acc: 0.534013, Tokens per Sec:     5473, Lr: 0.000300\n",
      "2022-08-15 22:20:34,882 - INFO - joeynmt.training - Epoch   7, Step:    19600, Batch Loss:     2.315292, Batch Acc: 0.530410, Tokens per Sec:     5347, Lr: 0.000300\n",
      "2022-08-15 22:20:57,770 - INFO - joeynmt.training - Epoch   7, Step:    19700, Batch Loss:     2.257561, Batch Acc: 0.531679, Tokens per Sec:     5237, Lr: 0.000300\n",
      "2022-08-15 22:21:19,751 - INFO - joeynmt.training - Epoch   7, Step:    19800, Batch Loss:     2.223460, Batch Acc: 0.529034, Tokens per Sec:     5548, Lr: 0.000300\n",
      "2022-08-15 22:21:41,942 - INFO - joeynmt.training - Epoch   7, Step:    19900, Batch Loss:     2.371094, Batch Acc: 0.532939, Tokens per Sec:     5476, Lr: 0.000300\n",
      "2022-08-15 22:22:03,763 - INFO - joeynmt.training - Epoch   7, Step:    20000, Batch Loss:     2.509251, Batch Acc: 0.528710, Tokens per Sec:     5495, Lr: 0.000300\n",
      "2022-08-15 22:22:03,764 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:22:53,328 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:22:53,328 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.74, loss:   2.66, ppl:  14.23, acc:   0.52, generation: 48.4677[sec], evaluation: 0.4115[sec]\n",
      "2022-08-15 22:22:53,329 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 22:22:54,443 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/15000.ckpt\n",
      "2022-08-15 22:22:54,448 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:22:54,453 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:22:54,453 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:22:54,453 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of travel and a freuman in the etho-graphic research is to live with people who can remember the old days. the past, they still feel in the wind, they feel on the snow, and they touch the leaves of the other plants.\n",
      "2022-08-15 22:22:54,453 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:22:54,457 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:22:54,458 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:22:54,458 - INFO - joeynmt.training - \tHypothesis: just the only knowledge that johanar schabed still are beyond the milky road or the meaning of the oldest inuit is full of meaning, or that in the himalayas, the buddhists still are at the buddhist of the field, which is the center of the philosophy of the philosophy of the memory, which is that we've been in a sense of a certain idea of life, but we've been in fact, is a sense of a sense of a world,\n",
      "2022-08-15 22:22:54,458 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:22:54,461 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:22:54,461 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:22:54,462 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 22:22:54,462 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:22:54,465 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:22:54,466 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:22:54,466 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring children to the world.\n",
      "2022-08-15 22:22:54,466 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:22:54,469 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:22:54,469 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:22:54,469 - INFO - joeynmt.training - \tHypothesis: we're starting to study rituals.\n",
      "2022-08-15 22:23:18,222 - INFO - joeynmt.training - Epoch   7, Step:    20100, Batch Loss:     2.259861, Batch Acc: 0.532773, Tokens per Sec:     4887, Lr: 0.000300\n",
      "2022-08-15 22:23:40,207 - INFO - joeynmt.training - Epoch   7, Step:    20200, Batch Loss:     2.107689, Batch Acc: 0.531545, Tokens per Sec:     5517, Lr: 0.000300\n",
      "2022-08-15 22:24:02,402 - INFO - joeynmt.training - Epoch   7, Step:    20300, Batch Loss:     2.288493, Batch Acc: 0.533763, Tokens per Sec:     5491, Lr: 0.000300\n",
      "2022-08-15 22:24:24,337 - INFO - joeynmt.training - Epoch   7, Step:    20400, Batch Loss:     2.498749, Batch Acc: 0.532429, Tokens per Sec:     5431, Lr: 0.000300\n",
      "2022-08-15 22:24:47,122 - INFO - joeynmt.training - Epoch   7, Step:    20500, Batch Loss:     2.243003, Batch Acc: 0.533325, Tokens per Sec:     5446, Lr: 0.000300\n",
      "2022-08-15 22:25:09,776 - INFO - joeynmt.training - Epoch   7, Step:    20600, Batch Loss:     2.405369, Batch Acc: 0.527620, Tokens per Sec:     5182, Lr: 0.000300\n",
      "2022-08-15 22:25:31,766 - INFO - joeynmt.training - Epoch   7, Step:    20700, Batch Loss:     2.167343, Batch Acc: 0.532494, Tokens per Sec:     5548, Lr: 0.000300\n",
      "2022-08-15 22:25:54,077 - INFO - joeynmt.training - Epoch   7, Step:    20800, Batch Loss:     2.310155, Batch Acc: 0.535646, Tokens per Sec:     5533, Lr: 0.000300\n",
      "2022-08-15 22:26:16,553 - INFO - joeynmt.training - Epoch   7, Step:    20900, Batch Loss:     2.209114, Batch Acc: 0.533669, Tokens per Sec:     5286, Lr: 0.000300\n",
      "2022-08-15 22:26:39,012 - INFO - joeynmt.training - Epoch   7, Step:    21000, Batch Loss:     2.179913, Batch Acc: 0.529816, Tokens per Sec:     5393, Lr: 0.000300\n",
      "2022-08-15 22:26:39,012 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:27:32,059 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:27:32,059 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.11, loss:   2.62, ppl:  13.68, acc:   0.52, generation: 51.9159[sec], evaluation: 0.4018[sec]\n",
      "2022-08-15 22:27:32,060 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 22:27:33,144 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/16000.ckpt\n",
      "2022-08-15 22:27:33,148 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:27:33,153 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:27:33,154 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:27:33,154 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of travel and a freuman at the ethicago, is in collaboration with people who can remember the old days, and the past, they still feel in the wind, they touch together in the wild blocks, they touch in the teeth.\n",
      "2022-08-15 22:27:33,154 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:27:33,158 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:27:33,160 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:27:33,161 - INFO - joeynmt.training - \tHypothesis: just the way that jawhogans still are still beyond the milky road or the meaning of the poet's oldest inuit are more significant or that in the himalayas, the buddhists still reddma, means the central journalism to memory, the idea that we have a lot of insight, but that we have a lot of insight in the world, but we have a very important idea of a lot of insight in the world.\n",
      "2022-08-15 22:27:33,161 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:27:33,165 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:27:33,165 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:27:33,165 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 22:27:33,165 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:27:33,168 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:27:33,169 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:27:33,169 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring kids to the world.\n",
      "2022-08-15 22:27:33,169 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:27:33,172 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:27:33,172 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:27:33,172 - INFO - joeynmt.training - \tHypothesis: we're running through rituals.\n",
      "2022-08-15 22:27:56,010 - INFO - joeynmt.training - Epoch   7, Step:    21100, Batch Loss:     2.197440, Batch Acc: 0.533460, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2022-08-15 22:28:18,250 - INFO - joeynmt.training - Epoch   7, Step:    21200, Batch Loss:     2.306550, Batch Acc: 0.534055, Tokens per Sec:     5544, Lr: 0.000300\n",
      "2022-08-15 22:28:40,657 - INFO - joeynmt.training - Epoch   7, Step:    21300, Batch Loss:     2.363750, Batch Acc: 0.532610, Tokens per Sec:     5447, Lr: 0.000300\n",
      "2022-08-15 22:29:03,049 - INFO - joeynmt.training - Epoch   7, Step:    21400, Batch Loss:     2.294992, Batch Acc: 0.536376, Tokens per Sec:     5328, Lr: 0.000300\n",
      "2022-08-15 22:29:24,984 - INFO - joeynmt.training - Epoch   7, Step:    21500, Batch Loss:     2.571896, Batch Acc: 0.532347, Tokens per Sec:     5426, Lr: 0.000300\n",
      "2022-08-15 22:29:49,154 - INFO - joeynmt.training - Epoch   7, Step:    21600, Batch Loss:     2.400170, Batch Acc: 0.536869, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2022-08-15 22:30:11,544 - INFO - joeynmt.training - Epoch   7, Step:    21700, Batch Loss:     2.198041, Batch Acc: 0.533982, Tokens per Sec:     5442, Lr: 0.000300\n",
      "2022-08-15 22:30:34,058 - INFO - joeynmt.training - Epoch   7, Step:    21800, Batch Loss:     2.247227, Batch Acc: 0.531746, Tokens per Sec:     5332, Lr: 0.000300\n",
      "2022-08-15 22:30:56,332 - INFO - joeynmt.training - Epoch   7, Step:    21900, Batch Loss:     2.322225, Batch Acc: 0.535271, Tokens per Sec:     5467, Lr: 0.000300\n",
      "2022-08-15 22:31:18,703 - INFO - joeynmt.training - Epoch   7, Step:    22000, Batch Loss:     2.309908, Batch Acc: 0.536104, Tokens per Sec:     5437, Lr: 0.000300\n",
      "2022-08-15 22:31:18,704 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:32:09,009 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:32:09,009 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.94, loss:   2.58, ppl:  13.13, acc:   0.53, generation: 48.9593[sec], evaluation: 0.6562[sec]\n",
      "2022-08-15 22:32:09,010 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 22:32:10,071 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/17000.ckpt\n",
      "2022-08-15 22:32:10,076 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:32:10,081 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:32:10,081 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:32:10,082 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure in travel and a funny of the ethicey research is in common with people who can remember the old days, and the past, they still feel in the wind, they touch the snow blocks, they touch the other blocks in the way they touch the teeth.\n",
      "2022-08-15 22:32:10,082 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:32:10,087 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:32:10,087 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:32:10,087 - INFO - joeynmt.training - \tHypothesis: just the idea that jawky chocolate still is over the milky street or the meaning of the myths of the oldest inuit are still more important, or that in the himalayas still the buddhist of the fatima, the central framework, the center of the memory of the sphere of the mind is to call the world, but the idea of the world is a lot of truth that we have been in a sense of truth of the world, but that we've been a lot\n",
      "2022-08-15 22:32:10,088 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:32:10,090 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:32:10,091 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:32:10,091 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 22:32:10,091 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:32:10,094 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:32:10,094 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:32:10,094 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring children to the world.\n",
      "2022-08-15 22:32:10,094 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:32:10,097 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:32:10,097 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:32:10,097 - INFO - joeynmt.training - \tHypothesis: we're running through rituals.\n",
      "2022-08-15 22:32:32,381 - INFO - joeynmt.training - Epoch   7, Step:    22100, Batch Loss:     2.223915, Batch Acc: 0.535186, Tokens per Sec:     5156, Lr: 0.000300\n",
      "2022-08-15 22:32:55,206 - INFO - joeynmt.training - Epoch   7, Step:    22200, Batch Loss:     2.320628, Batch Acc: 0.539331, Tokens per Sec:     5433, Lr: 0.000300\n",
      "2022-08-15 22:33:17,290 - INFO - joeynmt.training - Epoch   7, Step:    22300, Batch Loss:     2.336114, Batch Acc: 0.539909, Tokens per Sec:     5556, Lr: 0.000300\n",
      "2022-08-15 22:33:39,533 - INFO - joeynmt.training - Epoch   7, Step:    22400, Batch Loss:     2.307615, Batch Acc: 0.538844, Tokens per Sec:     5404, Lr: 0.000300\n",
      "2022-08-15 22:34:02,611 - INFO - joeynmt.training - Epoch   7, Step:    22500, Batch Loss:     2.283074, Batch Acc: 0.536709, Tokens per Sec:     5215, Lr: 0.000300\n",
      "2022-08-15 22:34:25,427 - INFO - joeynmt.training - Epoch   7, Step:    22600, Batch Loss:     2.256031, Batch Acc: 0.537488, Tokens per Sec:     5476, Lr: 0.000300\n",
      "2022-08-15 22:34:27,185 - INFO - joeynmt.training - Epoch   7: total training loss 7452.25\n",
      "2022-08-15 22:34:27,185 - INFO - joeynmt.training - EPOCH 8\n",
      "2022-08-15 22:34:47,574 - INFO - joeynmt.training - Epoch   8, Step:    22700, Batch Loss:     2.113826, Batch Acc: 0.552576, Tokens per Sec:     5476, Lr: 0.000300\n",
      "2022-08-15 22:35:09,593 - INFO - joeynmt.training - Epoch   8, Step:    22800, Batch Loss:     2.341868, Batch Acc: 0.550293, Tokens per Sec:     5491, Lr: 0.000300\n",
      "2022-08-15 22:35:31,488 - INFO - joeynmt.training - Epoch   8, Step:    22900, Batch Loss:     2.171580, Batch Acc: 0.551116, Tokens per Sec:     5483, Lr: 0.000300\n",
      "2022-08-15 22:35:53,261 - INFO - joeynmt.training - Epoch   8, Step:    23000, Batch Loss:     2.311982, Batch Acc: 0.549380, Tokens per Sec:     5592, Lr: 0.000300\n",
      "2022-08-15 22:35:53,262 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:36:43,958 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:36:43,958 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.91, loss:   2.57, ppl:  13.06, acc:   0.53, generation: 49.6195[sec], evaluation: 0.3980[sec]\n",
      "2022-08-15 22:36:44,961 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/18000.ckpt\n",
      "2022-08-15 22:36:44,964 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:36:44,969 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:36:44,969 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:36:44,969 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of travel and a funny obsession in the ethically graphic research is to live with people who can remember the old days. the past is still in the wind, they feel about the rain, they touch the rocks that they make the more blocks.\n",
      "2022-08-15 22:36:44,970 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:36:44,973 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:36:44,974 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:36:44,974 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jawky chocolate still goes beyond the milky street or the importance of the oldest inuit of the inuit, or that in the himalayas, the buddhists still follow the atm of the harma, is the central revelation of the ecology of memory, the idea that we have a sense of life, but that we have a certain sense of purpose, but that we have a certain sense of the most of the most important idea of life is a consequence of\n",
      "2022-08-15 22:36:44,974 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:36:44,977 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:36:44,977 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:36:44,977 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 22:36:44,977 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:36:44,980 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:36:44,981 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:36:44,981 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring kids to the world.\n",
      "2022-08-15 22:36:44,981 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:36:44,984 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:36:44,984 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:36:44,984 - INFO - joeynmt.training - \tHypothesis: we're using talent rituals.\n",
      "2022-08-15 22:37:07,357 - INFO - joeynmt.training - Epoch   8, Step:    23100, Batch Loss:     2.266878, Batch Acc: 0.546770, Tokens per Sec:     5155, Lr: 0.000300\n",
      "2022-08-15 22:37:29,546 - INFO - joeynmt.training - Epoch   8, Step:    23200, Batch Loss:     2.157597, Batch Acc: 0.550302, Tokens per Sec:     5479, Lr: 0.000300\n",
      "2022-08-15 22:37:51,783 - INFO - joeynmt.training - Epoch   8, Step:    23300, Batch Loss:     1.887228, Batch Acc: 0.549467, Tokens per Sec:     5450, Lr: 0.000300\n",
      "2022-08-15 22:38:15,403 - INFO - joeynmt.training - Epoch   8, Step:    23400, Batch Loss:     2.289292, Batch Acc: 0.544172, Tokens per Sec:     5106, Lr: 0.000300\n",
      "2022-08-15 22:38:37,230 - INFO - joeynmt.training - Epoch   8, Step:    23500, Batch Loss:     2.275556, Batch Acc: 0.550563, Tokens per Sec:     5644, Lr: 0.000300\n",
      "2022-08-15 22:38:58,942 - INFO - joeynmt.training - Epoch   8, Step:    23600, Batch Loss:     2.290223, Batch Acc: 0.547661, Tokens per Sec:     5473, Lr: 0.000300\n",
      "2022-08-15 22:39:21,141 - INFO - joeynmt.training - Epoch   8, Step:    23700, Batch Loss:     2.071685, Batch Acc: 0.547273, Tokens per Sec:     5420, Lr: 0.000300\n",
      "2022-08-15 22:39:43,252 - INFO - joeynmt.training - Epoch   8, Step:    23800, Batch Loss:     2.266153, Batch Acc: 0.551831, Tokens per Sec:     5514, Lr: 0.000300\n",
      "2022-08-15 22:40:05,423 - INFO - joeynmt.training - Epoch   8, Step:    23900, Batch Loss:     2.167791, Batch Acc: 0.551264, Tokens per Sec:     5447, Lr: 0.000300\n",
      "2022-08-15 22:40:28,058 - INFO - joeynmt.training - Epoch   8, Step:    24000, Batch Loss:     2.243575, Batch Acc: 0.550784, Tokens per Sec:     5328, Lr: 0.000300\n",
      "2022-08-15 22:40:28,058 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:41:16,756 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:41:16,757 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.89, loss:   2.56, ppl:  12.90, acc:   0.53, generation: 47.6234[sec], evaluation: 0.4035[sec]\n",
      "2022-08-15 22:41:17,779 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/19000.ckpt\n",
      "2022-08-15 22:41:17,782 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:41:17,788 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:41:17,788 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:41:17,788 - INFO - joeynmt.training - \tHypothesis: you know, one of the great lessons about travel and a freuather of the ethicist research is to live with people who can still remember the old days. the past, they feel in the wind, they're actually proud of the rain, they're proud of the blocks that they're confusing to the trees.\n",
      "2022-08-15 22:41:17,788 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:41:17,793 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:41:17,793 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:41:17,793 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguar schans still beyond the milky road or the importance of the myths are still full of the inuit of the himalaya, or that the buddhists still follow the atma, which means the center of the center of the center of the ecology of the memory, the idea that we have a sense of purpose of the world, but the world is not only part of the world, but the purpose of the world of the world of the world, but\n",
      "2022-08-15 22:41:17,793 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:41:17,812 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:41:17,812 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:41:17,812 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 22:41:17,812 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:41:17,817 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:41:17,817 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:41:17,817 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring children to the world.\n",
      "2022-08-15 22:41:17,817 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:41:17,821 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:41:17,821 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:41:17,821 - INFO - joeynmt.training - \tHypothesis: we're walking through the ritual.\n",
      "2022-08-15 22:41:39,947 - INFO - joeynmt.training - Epoch   8, Step:    24100, Batch Loss:     2.383371, Batch Acc: 0.546972, Tokens per Sec:     5135, Lr: 0.000300\n",
      "2022-08-15 22:42:02,018 - INFO - joeynmt.training - Epoch   8, Step:    24200, Batch Loss:     2.270133, Batch Acc: 0.546190, Tokens per Sec:     5615, Lr: 0.000300\n",
      "2022-08-15 22:42:24,670 - INFO - joeynmt.training - Epoch   8, Step:    24300, Batch Loss:     2.312779, Batch Acc: 0.548436, Tokens per Sec:     5352, Lr: 0.000300\n",
      "2022-08-15 22:42:47,354 - INFO - joeynmt.training - Epoch   8, Step:    24400, Batch Loss:     2.189844, Batch Acc: 0.546744, Tokens per Sec:     5222, Lr: 0.000300\n",
      "2022-08-15 22:43:09,599 - INFO - joeynmt.training - Epoch   8, Step:    24500, Batch Loss:     2.071889, Batch Acc: 0.544522, Tokens per Sec:     5526, Lr: 0.000300\n",
      "2022-08-15 22:43:31,960 - INFO - joeynmt.training - Epoch   8, Step:    24600, Batch Loss:     1.982830, Batch Acc: 0.553415, Tokens per Sec:     5308, Lr: 0.000300\n",
      "2022-08-15 22:43:54,530 - INFO - joeynmt.training - Epoch   8, Step:    24700, Batch Loss:     2.215418, Batch Acc: 0.545296, Tokens per Sec:     5594, Lr: 0.000300\n",
      "2022-08-15 22:44:16,367 - INFO - joeynmt.training - Epoch   8, Step:    24800, Batch Loss:     2.288803, Batch Acc: 0.550934, Tokens per Sec:     5456, Lr: 0.000300\n",
      "2022-08-15 22:44:38,287 - INFO - joeynmt.training - Epoch   8, Step:    24900, Batch Loss:     2.185628, Batch Acc: 0.552821, Tokens per Sec:     5638, Lr: 0.000300\n",
      "2022-08-15 22:45:00,629 - INFO - joeynmt.training - Epoch   8, Step:    25000, Batch Loss:     2.191793, Batch Acc: 0.553842, Tokens per Sec:     5501, Lr: 0.000300\n",
      "2022-08-15 22:45:00,630 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:45:48,159 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:45:48,160 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.39, loss:   2.52, ppl:  12.43, acc:   0.54, generation: 46.4395[sec], evaluation: 0.4057[sec]\n",
      "2022-08-15 22:45:48,161 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 22:45:49,252 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/20000.ckpt\n",
      "2022-08-15 22:45:49,256 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:45:49,262 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:45:49,262 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:45:49,262 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of travel, and one of the freugraphic research is to live with people who can remember the old days, and their past, they still feel in the wind, they're proud of the dust, they're proud of the bitter plants.\n",
      "2022-08-15 22:45:49,263 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:45:49,267 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:45:49,267 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:45:49,267 - INFO - joeynmt.training - \tHypothesis: just the idea that jaguar schans still go beyond the milky street or the meaning of myninuit are more important, or that in the himalayas still pursue the buddhist of harma, the central revelation of the center of the memory of the memory of the memory, the idea is that we have a certain idea of the world, but the tendency of purpose of the world is not a certain idea of the world of the world as a certain insight.\n",
      "2022-08-15 22:45:49,267 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:45:49,270 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:45:49,270 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:45:49,271 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 22:45:49,271 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:45:49,274 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:45:49,274 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:45:49,274 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring kids to the world.\n",
      "2022-08-15 22:45:49,274 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:45:49,280 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:45:49,280 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:45:49,280 - INFO - joeynmt.training - \tHypothesis: we're running through the reformers.\n",
      "2022-08-15 22:46:11,491 - INFO - joeynmt.training - Epoch   8, Step:    25100, Batch Loss:     2.072172, Batch Acc: 0.549819, Tokens per Sec:     5136, Lr: 0.000300\n",
      "2022-08-15 22:46:33,377 - INFO - joeynmt.training - Epoch   8, Step:    25200, Batch Loss:     2.228605, Batch Acc: 0.551006, Tokens per Sec:     5581, Lr: 0.000300\n",
      "2022-08-15 22:46:56,446 - INFO - joeynmt.training - Epoch   8, Step:    25300, Batch Loss:     2.180050, Batch Acc: 0.548799, Tokens per Sec:     5246, Lr: 0.000300\n",
      "2022-08-15 22:47:18,769 - INFO - joeynmt.training - Epoch   8, Step:    25400, Batch Loss:     2.289925, Batch Acc: 0.552126, Tokens per Sec:     5456, Lr: 0.000300\n",
      "2022-08-15 22:47:41,133 - INFO - joeynmt.training - Epoch   8, Step:    25500, Batch Loss:     2.257057, Batch Acc: 0.551493, Tokens per Sec:     5482, Lr: 0.000300\n",
      "2022-08-15 22:48:02,997 - INFO - joeynmt.training - Epoch   8, Step:    25600, Batch Loss:     2.220365, Batch Acc: 0.550479, Tokens per Sec:     5510, Lr: 0.000300\n",
      "2022-08-15 22:48:25,026 - INFO - joeynmt.training - Epoch   8, Step:    25700, Batch Loss:     2.060442, Batch Acc: 0.551252, Tokens per Sec:     5466, Lr: 0.000300\n",
      "2022-08-15 22:48:47,465 - INFO - joeynmt.training - Epoch   8, Step:    25800, Batch Loss:     2.155310, Batch Acc: 0.551478, Tokens per Sec:     5464, Lr: 0.000300\n",
      "2022-08-15 22:48:55,767 - INFO - joeynmt.training - Epoch   8: total training loss 7120.39\n",
      "2022-08-15 22:48:55,767 - INFO - joeynmt.training - EPOCH 9\n",
      "2022-08-15 22:49:10,127 - INFO - joeynmt.training - Epoch   9, Step:    25900, Batch Loss:     1.931702, Batch Acc: 0.568184, Tokens per Sec:     5474, Lr: 0.000300\n",
      "2022-08-15 22:49:31,904 - INFO - joeynmt.training - Epoch   9, Step:    26000, Batch Loss:     2.207263, Batch Acc: 0.558511, Tokens per Sec:     5531, Lr: 0.000300\n",
      "2022-08-15 22:49:31,905 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:50:23,263 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:50:23,263 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.66, loss:   2.51, ppl:  12.30, acc:   0.54, generation: 50.2545[sec], evaluation: 0.4240[sec]\n",
      "2022-08-15 22:50:23,265 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 22:50:24,279 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/21000.ckpt\n",
      "2022-08-15 22:50:24,283 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:50:24,288 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:50:24,288 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:50:24,288 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of travel, and one of the freudian research is in common with people who can remember the old days, and their past, they still feel in the wind, they touch on the snow, they touch the more strings of the teeth.\n",
      "2022-08-15 22:50:24,288 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:50:24,292 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:50:24,293 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:50:24,293 - INFO - joeynmt.training - \tHypothesis: just the idea that jaguers still still go beyond the milky street or the meaning of myredness of the oldest inuit are still more important, or that in himalayan, the buddhist still follow the atm of the harma, which means to call the central anthropology to the memory of the idea that we have a unique sense of life as a result of a certain way of life, but a very important idea of the world is only a lot of insight in a sense\n",
      "2022-08-15 22:50:24,293 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:50:24,296 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:50:24,297 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:50:24,297 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 22:50:24,297 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:50:24,300 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:50:24,300 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:50:24,300 - INFO - joeynmt.training - \tHypothesis: we're all born. we bring kids to the world.\n",
      "2022-08-15 22:50:24,300 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:50:24,303 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:50:24,304 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:50:24,304 - INFO - joeynmt.training - \tHypothesis: we're going to go through the initiation of rituals.\n",
      "2022-08-15 22:50:46,109 - INFO - joeynmt.training - Epoch   9, Step:    26100, Batch Loss:     1.967928, Batch Acc: 0.567103, Tokens per Sec:     5319, Lr: 0.000300\n",
      "2022-08-15 22:51:08,108 - INFO - joeynmt.training - Epoch   9, Step:    26200, Batch Loss:     2.172405, Batch Acc: 0.566230, Tokens per Sec:     5537, Lr: 0.000300\n",
      "2022-08-15 22:51:31,444 - INFO - joeynmt.training - Epoch   9, Step:    26300, Batch Loss:     1.948787, Batch Acc: 0.559929, Tokens per Sec:     5156, Lr: 0.000300\n",
      "2022-08-15 22:51:54,031 - INFO - joeynmt.training - Epoch   9, Step:    26400, Batch Loss:     2.193842, Batch Acc: 0.565164, Tokens per Sec:     5638, Lr: 0.000300\n",
      "2022-08-15 22:52:16,109 - INFO - joeynmt.training - Epoch   9, Step:    26500, Batch Loss:     1.994500, Batch Acc: 0.564138, Tokens per Sec:     5628, Lr: 0.000300\n",
      "2022-08-15 22:52:38,051 - INFO - joeynmt.training - Epoch   9, Step:    26600, Batch Loss:     2.341570, Batch Acc: 0.563391, Tokens per Sec:     5507, Lr: 0.000300\n",
      "2022-08-15 22:52:59,881 - INFO - joeynmt.training - Epoch   9, Step:    26700, Batch Loss:     2.187519, Batch Acc: 0.562009, Tokens per Sec:     5392, Lr: 0.000300\n",
      "2022-08-15 22:53:23,128 - INFO - joeynmt.training - Epoch   9, Step:    26800, Batch Loss:     2.149416, Batch Acc: 0.559816, Tokens per Sec:     5229, Lr: 0.000300\n",
      "2022-08-15 22:53:45,453 - INFO - joeynmt.training - Epoch   9, Step:    26900, Batch Loss:     1.976544, Batch Acc: 0.561886, Tokens per Sec:     5379, Lr: 0.000300\n",
      "2022-08-15 22:54:07,269 - INFO - joeynmt.training - Epoch   9, Step:    27000, Batch Loss:     2.522415, Batch Acc: 0.562259, Tokens per Sec:     5443, Lr: 0.000300\n",
      "2022-08-15 22:54:07,270 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:54:54,876 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:54:54,876 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.11, loss:   2.50, ppl:  12.21, acc:   0.54, generation: 45.8015[sec], evaluation: 0.7198[sec]\n",
      "2022-08-15 22:54:54,878 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 22:54:56,046 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/24000.ckpt\n",
      "2022-08-15 22:54:56,050 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:54:56,055 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:54:56,056 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:54:56,056 - INFO - joeynmt.training - \tHypothesis: you know, one of the great delight, and one of the joy of the ethics research is to live with people who can remember the old days. the past they still feel in the wind, they touch on the snow, they touch them in the more sides of the plants.\n",
      "2022-08-15 22:54:56,056 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:54:56,060 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:54:56,060 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:54:56,060 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguered chocolate still goes beyond the milky road or the meaning of myninuit is still a meaning, or that in the himalayas, the buddhists still follow the atima, which means the center of the heart of the memory of the memory of the memory, the idea that we don't have a unique idea of a certain amount of life, but a certain sense of the world of purpose of the world is a very important.\n",
      "2022-08-15 22:54:56,060 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:54:56,063 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:54:56,064 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:54:56,065 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 22:54:56,065 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:54:56,068 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:54:56,068 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:54:56,068 - INFO - joeynmt.training - \tHypothesis: we're all born. we're bringing children to the world.\n",
      "2022-08-15 22:54:56,068 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:54:56,071 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:54:56,071 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:54:56,072 - INFO - joeynmt.training - \tHypothesis: we're going through reformers.\n",
      "2022-08-15 22:55:18,096 - INFO - joeynmt.training - Epoch   9, Step:    27100, Batch Loss:     2.289779, Batch Acc: 0.564279, Tokens per Sec:     5172, Lr: 0.000300\n",
      "2022-08-15 22:55:41,741 - INFO - joeynmt.training - Epoch   9, Step:    27200, Batch Loss:     2.250328, Batch Acc: 0.560012, Tokens per Sec:     5094, Lr: 0.000300\n",
      "2022-08-15 22:56:03,651 - INFO - joeynmt.training - Epoch   9, Step:    27300, Batch Loss:     2.217027, Batch Acc: 0.561075, Tokens per Sec:     5648, Lr: 0.000300\n",
      "2022-08-15 22:56:25,668 - INFO - joeynmt.training - Epoch   9, Step:    27400, Batch Loss:     2.204995, Batch Acc: 0.561692, Tokens per Sec:     5438, Lr: 0.000300\n",
      "2022-08-15 22:56:48,608 - INFO - joeynmt.training - Epoch   9, Step:    27500, Batch Loss:     2.316441, Batch Acc: 0.558113, Tokens per Sec:     5338, Lr: 0.000300\n",
      "2022-08-15 22:57:10,795 - INFO - joeynmt.training - Epoch   9, Step:    27600, Batch Loss:     2.356738, Batch Acc: 0.562403, Tokens per Sec:     5425, Lr: 0.000300\n",
      "2022-08-15 22:57:33,197 - INFO - joeynmt.training - Epoch   9, Step:    27700, Batch Loss:     2.066304, Batch Acc: 0.557704, Tokens per Sec:     5424, Lr: 0.000300\n",
      "2022-08-15 22:57:56,070 - INFO - joeynmt.training - Epoch   9, Step:    27800, Batch Loss:     2.119267, Batch Acc: 0.560260, Tokens per Sec:     5464, Lr: 0.000300\n",
      "2022-08-15 22:58:18,658 - INFO - joeynmt.training - Epoch   9, Step:    27900, Batch Loss:     2.050601, Batch Acc: 0.560120, Tokens per Sec:     5272, Lr: 0.000300\n",
      "2022-08-15 22:58:40,359 - INFO - joeynmt.training - Epoch   9, Step:    28000, Batch Loss:     2.145123, Batch Acc: 0.559864, Tokens per Sec:     5500, Lr: 0.000300\n",
      "2022-08-15 22:58:40,359 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 22:59:28,083 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 22:59:28,084 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.95, loss:   2.47, ppl:  11.85, acc:   0.54, generation: 46.6441[sec], evaluation: 0.4040[sec]\n",
      "2022-08-15 22:59:29,048 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/23000.ckpt\n",
      "2022-08-15 22:59:29,051 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 22:59:29,058 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 22:59:29,058 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 22:59:29,058 - INFO - joeynmt.training - \tHypothesis: you know, one of the big pleasure of travel and one of the freudian research is in common with people who can remember the old days, and the past in the wind, they still feel in the wind, they touch the snow, they touch the holes in the side of the pack.\n",
      "2022-08-15 22:59:29,058 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 22:59:29,064 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 22:59:29,064 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 22:59:29,065 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguers still go beyond the milky way or the importance of the myreformers of the oldest inuit are more significant, or in the himalayas, the buddhist of the buddhist's most of the harma, is the central revelation of the memory, the idea that we have a lot of insight in the world, but that we've got a sense of understanding of the world, but that is a certain sense of purpose of purpose of purpose.\n",
      "2022-08-15 22:59:29,065 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 22:59:29,068 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 22:59:29,069 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 22:59:29,069 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptions.\n",
      "2022-08-15 22:59:29,069 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 22:59:29,072 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 22:59:29,072 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 22:59:29,072 - INFO - joeynmt.training - \tHypothesis: we're all born. we're bringing kids to the world.\n",
      "2022-08-15 22:59:29,072 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 22:59:29,075 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 22:59:29,075 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 22:59:29,076 - INFO - joeynmt.training - \tHypothesis: we're going to go through an investigative ritual.\n",
      "2022-08-15 22:59:52,156 - INFO - joeynmt.training - Epoch   9, Step:    28100, Batch Loss:     1.970172, Batch Acc: 0.561655, Tokens per Sec:     5174, Lr: 0.000300\n",
      "2022-08-15 23:00:14,485 - INFO - joeynmt.training - Epoch   9, Step:    28200, Batch Loss:     2.195507, Batch Acc: 0.565040, Tokens per Sec:     5392, Lr: 0.000300\n",
      "2022-08-15 23:00:36,072 - INFO - joeynmt.training - Epoch   9, Step:    28300, Batch Loss:     2.133644, Batch Acc: 0.562790, Tokens per Sec:     5509, Lr: 0.000300\n",
      "2022-08-15 23:00:58,533 - INFO - joeynmt.training - Epoch   9, Step:    28400, Batch Loss:     2.324694, Batch Acc: 0.559484, Tokens per Sec:     5350, Lr: 0.000300\n",
      "2022-08-15 23:01:20,980 - INFO - joeynmt.training - Epoch   9, Step:    28500, Batch Loss:     2.144012, Batch Acc: 0.561203, Tokens per Sec:     5548, Lr: 0.000300\n",
      "2022-08-15 23:01:42,760 - INFO - joeynmt.training - Epoch   9, Step:    28600, Batch Loss:     2.166931, Batch Acc: 0.565511, Tokens per Sec:     5678, Lr: 0.000300\n",
      "2022-08-15 23:02:05,266 - INFO - joeynmt.training - Epoch   9, Step:    28700, Batch Loss:     2.164349, Batch Acc: 0.564787, Tokens per Sec:     5551, Lr: 0.000300\n",
      "2022-08-15 23:02:27,739 - INFO - joeynmt.training - Epoch   9, Step:    28800, Batch Loss:     2.111749, Batch Acc: 0.563721, Tokens per Sec:     5469, Lr: 0.000300\n",
      "2022-08-15 23:02:50,047 - INFO - joeynmt.training - Epoch   9, Step:    28900, Batch Loss:     2.013736, Batch Acc: 0.567784, Tokens per Sec:     5371, Lr: 0.000300\n",
      "2022-08-15 23:03:11,979 - INFO - joeynmt.training - Epoch   9, Step:    29000, Batch Loss:     1.977399, Batch Acc: 0.563058, Tokens per Sec:     5540, Lr: 0.000300\n",
      "2022-08-15 23:03:11,980 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:04:00,666 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:04:00,667 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.25, loss:   2.46, ppl:  11.65, acc:   0.55, generation: 47.3825[sec], evaluation: 0.6359[sec]\n",
      "2022-08-15 23:04:00,668 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 23:04:01,782 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/22000.ckpt\n",
      "2022-08-15 23:04:01,786 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:04:01,791 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:04:01,791 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:04:01,791 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of travel, and one of the freudian research is in the ethicography of life, with people who remember the old days, and the past are still feeling in the wind, they touch with the rain, they're touching the bitter blocks in the tracks.\n",
      "2022-08-15 23:04:01,791 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:04:01,795 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:04:01,795 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:04:01,796 - INFO - joeynmt.training - \tHypothesis: just the idea that jill schnor nor nor is the importance of the korean street or the importance of the myths of the oldest inuit are still more important, or that in the himalayas, the buddhist still explores the atess of the harma, which means the central epiphany of the memory of the vision, the idea that we have a very important idea that we have a very strong sense of life, but a very important sense of a very important way of life, but\n",
      "2022-08-15 23:04:01,796 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:04:01,799 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:04:01,800 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:04:01,800 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 23:04:01,800 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:04:01,804 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:04:01,807 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:04:01,808 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring kids to the world.\n",
      "2022-08-15 23:04:01,808 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:04:01,811 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:04:01,811 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:04:01,811 - INFO - joeynmt.training - \tHypothesis: we're through initiating rituals.\n",
      "2022-08-15 23:04:15,916 - INFO - joeynmt.training - Epoch   9: total training loss 6845.36\n",
      "2022-08-15 23:04:15,916 - INFO - joeynmt.training - EPOCH 10\n",
      "2022-08-15 23:04:25,300 - INFO - joeynmt.training - Epoch  10, Step:    29100, Batch Loss:     2.149641, Batch Acc: 0.574760, Tokens per Sec:     5265, Lr: 0.000300\n",
      "2022-08-15 23:04:47,188 - INFO - joeynmt.training - Epoch  10, Step:    29200, Batch Loss:     2.260235, Batch Acc: 0.574338, Tokens per Sec:     5578, Lr: 0.000300\n",
      "2022-08-15 23:05:08,885 - INFO - joeynmt.training - Epoch  10, Step:    29300, Batch Loss:     2.081426, Batch Acc: 0.577412, Tokens per Sec:     5576, Lr: 0.000300\n",
      "2022-08-15 23:05:30,651 - INFO - joeynmt.training - Epoch  10, Step:    29400, Batch Loss:     2.022209, Batch Acc: 0.576576, Tokens per Sec:     5513, Lr: 0.000300\n",
      "2022-08-15 23:05:52,593 - INFO - joeynmt.training - Epoch  10, Step:    29500, Batch Loss:     2.096400, Batch Acc: 0.574662, Tokens per Sec:     5402, Lr: 0.000300\n",
      "2022-08-15 23:06:15,054 - INFO - joeynmt.training - Epoch  10, Step:    29600, Batch Loss:     2.123060, Batch Acc: 0.570408, Tokens per Sec:     5374, Lr: 0.000300\n",
      "2022-08-15 23:06:37,264 - INFO - joeynmt.training - Epoch  10, Step:    29700, Batch Loss:     2.064199, Batch Acc: 0.576212, Tokens per Sec:     5652, Lr: 0.000300\n",
      "2022-08-15 23:06:59,137 - INFO - joeynmt.training - Epoch  10, Step:    29800, Batch Loss:     2.026910, Batch Acc: 0.570268, Tokens per Sec:     5455, Lr: 0.000300\n",
      "2022-08-15 23:07:21,279 - INFO - joeynmt.training - Epoch  10, Step:    29900, Batch Loss:     2.221744, Batch Acc: 0.573170, Tokens per Sec:     5496, Lr: 0.000300\n",
      "2022-08-15 23:07:43,890 - INFO - joeynmt.training - Epoch  10, Step:    30000, Batch Loss:     2.314511, Batch Acc: 0.575914, Tokens per Sec:     5277, Lr: 0.000300\n",
      "2022-08-15 23:07:43,891 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:08:31,435 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:08:31,437 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.35, loss:   2.46, ppl:  11.70, acc:   0.55, generation: 46.3107[sec], evaluation: 0.4679[sec]\n",
      "2022-08-15 23:08:31,438 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 23:08:32,587 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/25000.ckpt\n",
      "2022-08-15 23:08:32,591 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:08:32,597 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:08:32,598 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:08:32,598 - INFO - joeynmt.training - \tHypothesis: you know, one of the big pleasures of traveling and one of the fun of the ethics research is to live with people who can remember the old days. the past is still in the wind, they touch on the rain, they touch rocks in the bitches of the bitter plants.\n",
      "2022-08-15 23:08:32,598 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:08:32,602 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:08:32,602 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:08:32,602 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguar schnor nor nor nor nor nor is the importance of mynelson's oldest inuit is still more significant, or in the himalayas still tracking the breath of the ddna, which means the central revelation of the anthropology of memory, the idea that the idea is that we don't have a certain sense of life in a certain sense of reality, but a certain sense of purpose of the world.\n",
      "2022-08-15 23:08:32,602 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:08:32,605 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:08:32,605 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:08:32,606 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptions.\n",
      "2022-08-15 23:08:32,606 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:08:32,609 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:08:32,609 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:08:32,609 - INFO - joeynmt.training - \tHypothesis: we're all born. we're bringing kids to the world.\n",
      "2022-08-15 23:08:32,609 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:08:32,612 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:08:32,612 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:08:32,613 - INFO - joeynmt.training - \tHypothesis: we're through the reformers.\n",
      "2022-08-15 23:08:54,908 - INFO - joeynmt.training - Epoch  10, Step:    30100, Batch Loss:     2.064961, Batch Acc: 0.571881, Tokens per Sec:     5231, Lr: 0.000300\n",
      "2022-08-15 23:09:17,681 - INFO - joeynmt.training - Epoch  10, Step:    30200, Batch Loss:     2.002099, Batch Acc: 0.571288, Tokens per Sec:     5438, Lr: 0.000300\n",
      "2022-08-15 23:09:39,906 - INFO - joeynmt.training - Epoch  10, Step:    30300, Batch Loss:     2.072848, Batch Acc: 0.575023, Tokens per Sec:     5585, Lr: 0.000300\n",
      "2022-08-15 23:10:01,846 - INFO - joeynmt.training - Epoch  10, Step:    30400, Batch Loss:     2.203326, Batch Acc: 0.570916, Tokens per Sec:     5440, Lr: 0.000300\n",
      "2022-08-15 23:10:23,494 - INFO - joeynmt.training - Epoch  10, Step:    30500, Batch Loss:     2.052670, Batch Acc: 0.569823, Tokens per Sec:     5499, Lr: 0.000300\n",
      "2022-08-15 23:10:47,022 - INFO - joeynmt.training - Epoch  10, Step:    30600, Batch Loss:     1.921551, Batch Acc: 0.569496, Tokens per Sec:     5152, Lr: 0.000300\n",
      "2022-08-15 23:11:08,579 - INFO - joeynmt.training - Epoch  10, Step:    30700, Batch Loss:     1.903948, Batch Acc: 0.572594, Tokens per Sec:     5560, Lr: 0.000300\n",
      "2022-08-15 23:11:30,555 - INFO - joeynmt.training - Epoch  10, Step:    30800, Batch Loss:     2.048463, Batch Acc: 0.572401, Tokens per Sec:     5507, Lr: 0.000300\n",
      "2022-08-15 23:11:52,367 - INFO - joeynmt.training - Epoch  10, Step:    30900, Batch Loss:     2.114964, Batch Acc: 0.573988, Tokens per Sec:     5501, Lr: 0.000300\n",
      "2022-08-15 23:12:14,600 - INFO - joeynmt.training - Epoch  10, Step:    31000, Batch Loss:     2.059981, Batch Acc: 0.573093, Tokens per Sec:     5461, Lr: 0.000300\n",
      "2022-08-15 23:12:14,601 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:12:59,433 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:12:59,433 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.40, loss:   2.45, ppl:  11.54, acc:   0.55, generation: 43.7659[sec], evaluation: 0.3942[sec]\n",
      "2022-08-15 23:12:59,434 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 23:13:00,955 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/26000.ckpt\n",
      "2022-08-15 23:13:00,959 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:13:00,964 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:13:00,964 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:13:00,964 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasures of traveling and one of the freudian research is in the world of the nographic research, with people who can remember the old days, and they still feel their past, they touch on the rain, they touch the shells of the bitches.\n",
      "2022-08-15 23:13:00,964 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:13:00,968 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:13:00,968 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:13:00,969 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguar schnor nor nor travel beyond the milky road or the meaning of the oldest inuit are still full of meaning, or in the himalayas, the buddhist still describes the ataids of the dharma, the central capitalism of memory, the idea that the idea is that we have a sense of the world, but a sense of the truth of the truth, but a certain sense of purpose of the truth of the world, but a group\n",
      "2022-08-15 23:13:00,969 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:13:00,972 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:13:00,972 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:13:00,972 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 23:13:00,972 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:13:00,975 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:13:00,975 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:13:00,976 - INFO - joeynmt.training - \tHypothesis: we're all born. we're bringing kids to the world.\n",
      "2022-08-15 23:13:00,976 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:13:00,979 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:13:00,979 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:13:00,979 - INFO - joeynmt.training - \tHypothesis: we're going to go through an investigative ritual.\n",
      "2022-08-15 23:13:22,883 - INFO - joeynmt.training - Epoch  10, Step:    31100, Batch Loss:     2.198376, Batch Acc: 0.573760, Tokens per Sec:     5137, Lr: 0.000300\n",
      "2022-08-15 23:13:44,970 - INFO - joeynmt.training - Epoch  10, Step:    31200, Batch Loss:     1.913425, Batch Acc: 0.571433, Tokens per Sec:     5495, Lr: 0.000300\n",
      "2022-08-15 23:14:07,079 - INFO - joeynmt.training - Epoch  10, Step:    31300, Batch Loss:     2.083862, Batch Acc: 0.573718, Tokens per Sec:     5547, Lr: 0.000300\n",
      "2022-08-15 23:14:29,568 - INFO - joeynmt.training - Epoch  10, Step:    31400, Batch Loss:     2.119061, Batch Acc: 0.574306, Tokens per Sec:     5314, Lr: 0.000300\n",
      "2022-08-15 23:14:51,419 - INFO - joeynmt.training - Epoch  10, Step:    31500, Batch Loss:     1.982435, Batch Acc: 0.572043, Tokens per Sec:     5616, Lr: 0.000300\n",
      "2022-08-15 23:15:14,417 - INFO - joeynmt.training - Epoch  10, Step:    31600, Batch Loss:     2.133480, Batch Acc: 0.576207, Tokens per Sec:     5301, Lr: 0.000300\n",
      "2022-08-15 23:15:36,494 - INFO - joeynmt.training - Epoch  10, Step:    31700, Batch Loss:     2.065499, Batch Acc: 0.569505, Tokens per Sec:     5393, Lr: 0.000300\n",
      "2022-08-15 23:15:58,385 - INFO - joeynmt.training - Epoch  10, Step:    31800, Batch Loss:     2.223206, Batch Acc: 0.572916, Tokens per Sec:     5595, Lr: 0.000300\n",
      "2022-08-15 23:16:20,443 - INFO - joeynmt.training - Epoch  10, Step:    31900, Batch Loss:     2.071720, Batch Acc: 0.574440, Tokens per Sec:     5437, Lr: 0.000300\n",
      "2022-08-15 23:16:42,027 - INFO - joeynmt.training - Epoch  10, Step:    32000, Batch Loss:     2.062979, Batch Acc: 0.574014, Tokens per Sec:     5612, Lr: 0.000300\n",
      "2022-08-15 23:16:42,027 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:17:30,220 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:17:30,220 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.74, loss:   2.43, ppl:  11.32, acc:   0.55, generation: 47.1201[sec], evaluation: 0.4066[sec]\n",
      "2022-08-15 23:17:30,222 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 23:17:31,270 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/28000.ckpt\n",
      "2022-08-15 23:17:31,274 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:17:31,279 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:17:31,279 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:17:31,279 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of traveling, and one of the freuden is the freugraphic research that people can remember the old days, and the past still feel in the wind, they're touching on the rain, they're touching the more difficult blocks in the more sides of the plants.\n",
      "2022-08-15 23:17:31,279 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:17:31,284 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:17:31,284 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:17:31,284 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguar schans still go beyond the milky street or the importance of myths are the most inuit of the inuit, or that in himalaya still follow the studio, which means to call the central revelation of the ecology of memory, the idea that we have, in fact, in fact, in fact, that we have a certain sense of life, but that's a certain set of insight in the world, that's a very powerful way that exists in\n",
      "2022-08-15 23:17:31,285 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:17:31,288 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:17:31,288 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:17:31,288 - INFO - joeynmt.training - \tHypothesis: and of course, we all share the same adaptability.\n",
      "2022-08-15 23:17:31,288 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:17:31,291 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:17:31,291 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:17:31,291 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring kids to the world.\n",
      "2022-08-15 23:17:31,292 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:17:31,294 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:17:31,295 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:17:31,295 - INFO - joeynmt.training - \tHypothesis: we're going to run through the ritual.\n",
      "2022-08-15 23:17:53,324 - INFO - joeynmt.training - Epoch  10, Step:    32100, Batch Loss:     1.915973, Batch Acc: 0.573492, Tokens per Sec:     5357, Lr: 0.000300\n",
      "2022-08-15 23:18:15,182 - INFO - joeynmt.training - Epoch  10, Step:    32200, Batch Loss:     2.234781, Batch Acc: 0.573889, Tokens per Sec:     5521, Lr: 0.000300\n",
      "2022-08-15 23:18:35,735 - INFO - joeynmt.training - Epoch  10: total training loss 6661.57\n",
      "2022-08-15 23:18:35,736 - INFO - joeynmt.training - EPOCH 11\n",
      "2022-08-15 23:18:37,333 - INFO - joeynmt.training - Epoch  11, Step:    32300, Batch Loss:     1.951370, Batch Acc: 0.591692, Tokens per Sec:     5567, Lr: 0.000300\n",
      "2022-08-15 23:18:59,021 - INFO - joeynmt.training - Epoch  11, Step:    32400, Batch Loss:     1.803650, Batch Acc: 0.587917, Tokens per Sec:     5577, Lr: 0.000300\n",
      "2022-08-15 23:19:21,274 - INFO - joeynmt.training - Epoch  11, Step:    32500, Batch Loss:     1.979909, Batch Acc: 0.586632, Tokens per Sec:     5613, Lr: 0.000300\n",
      "2022-08-15 23:19:43,975 - INFO - joeynmt.training - Epoch  11, Step:    32600, Batch Loss:     1.963857, Batch Acc: 0.583575, Tokens per Sec:     5253, Lr: 0.000300\n",
      "2022-08-15 23:20:06,741 - INFO - joeynmt.training - Epoch  11, Step:    32700, Batch Loss:     1.785623, Batch Acc: 0.582514, Tokens per Sec:     5372, Lr: 0.000300\n",
      "2022-08-15 23:20:28,828 - INFO - joeynmt.training - Epoch  11, Step:    32800, Batch Loss:     2.027601, Batch Acc: 0.580553, Tokens per Sec:     5591, Lr: 0.000300\n",
      "2022-08-15 23:20:50,676 - INFO - joeynmt.training - Epoch  11, Step:    32900, Batch Loss:     2.080430, Batch Acc: 0.583058, Tokens per Sec:     5660, Lr: 0.000300\n",
      "2022-08-15 23:21:12,890 - INFO - joeynmt.training - Epoch  11, Step:    33000, Batch Loss:     1.868400, Batch Acc: 0.585951, Tokens per Sec:     5592, Lr: 0.000300\n",
      "2022-08-15 23:21:12,890 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:21:57,702 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:21:57,702 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.76, loss:   2.43, ppl:  11.39, acc:   0.55, generation: 43.7407[sec], evaluation: 0.4014[sec]\n",
      "2022-08-15 23:21:57,704 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 23:21:58,765 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/27000.ckpt\n",
      "2022-08-15 23:21:58,770 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:21:58,775 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:21:58,775 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:21:58,775 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of traveling, and one of the freudian science is involved with people who can remember the old days, and the past they still feel in the wind, they touch on the rain, they touch the bitter leaves of the plants.\n",
      "2022-08-15 23:21:58,775 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:21:58,779 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:21:58,779 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:21:58,780 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguines still continue beyond the milky road or the meaning of the oldest inuit are also full of meaning, or that in the himalayas, the buddhist of the harma, is the central revelation of the anthropology of the memory, which is that the idea that we can't even get in a certain sense of the world, but that we've only accepted a certain extent of the reality as a result of the most important part of the most important part of\n",
      "2022-08-15 23:21:58,780 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:21:58,784 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:21:58,785 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:21:58,785 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptions.\n",
      "2022-08-15 23:21:58,785 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:21:58,788 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:21:58,788 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:21:58,789 - INFO - joeynmt.training - \tHypothesis: we're all born. we're bringing kids to the world.\n",
      "2022-08-15 23:21:58,789 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:21:58,792 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:21:58,792 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:21:58,792 - INFO - joeynmt.training - \tHypothesis: we're going through initiating rituals.\n",
      "2022-08-15 23:22:20,421 - INFO - joeynmt.training - Epoch  11, Step:    33100, Batch Loss:     1.954581, Batch Acc: 0.580813, Tokens per Sec:     5329, Lr: 0.000300\n",
      "2022-08-15 23:22:42,349 - INFO - joeynmt.training - Epoch  11, Step:    33200, Batch Loss:     2.087253, Batch Acc: 0.584770, Tokens per Sec:     5561, Lr: 0.000300\n",
      "2022-08-15 23:23:04,285 - INFO - joeynmt.training - Epoch  11, Step:    33300, Batch Loss:     1.954621, Batch Acc: 0.584595, Tokens per Sec:     5584, Lr: 0.000300\n",
      "2022-08-15 23:23:26,524 - INFO - joeynmt.training - Epoch  11, Step:    33400, Batch Loss:     1.941315, Batch Acc: 0.584337, Tokens per Sec:     5419, Lr: 0.000300\n",
      "2022-08-15 23:23:49,750 - INFO - joeynmt.training - Epoch  11, Step:    33500, Batch Loss:     1.908169, Batch Acc: 0.582575, Tokens per Sec:     5204, Lr: 0.000300\n",
      "2022-08-15 23:24:11,549 - INFO - joeynmt.training - Epoch  11, Step:    33600, Batch Loss:     2.051881, Batch Acc: 0.582211, Tokens per Sec:     5632, Lr: 0.000300\n",
      "2022-08-15 23:24:33,341 - INFO - joeynmt.training - Epoch  11, Step:    33700, Batch Loss:     2.171783, Batch Acc: 0.585446, Tokens per Sec:     5558, Lr: 0.000300\n",
      "2022-08-15 23:24:55,594 - INFO - joeynmt.training - Epoch  11, Step:    33800, Batch Loss:     2.190862, Batch Acc: 0.581277, Tokens per Sec:     5260, Lr: 0.000300\n",
      "2022-08-15 23:25:17,526 - INFO - joeynmt.training - Epoch  11, Step:    33900, Batch Loss:     2.074083, Batch Acc: 0.580492, Tokens per Sec:     5596, Lr: 0.000300\n",
      "2022-08-15 23:25:39,369 - INFO - joeynmt.training - Epoch  11, Step:    34000, Batch Loss:     2.161979, Batch Acc: 0.585221, Tokens per Sec:     5521, Lr: 0.000300\n",
      "2022-08-15 23:25:39,370 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:26:29,624 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:26:29,624 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.10, loss:   2.41, ppl:  11.15, acc:   0.56, generation: 49.1854[sec], evaluation: 0.4005[sec]\n",
      "2022-08-15 23:26:29,626 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 23:26:30,958 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/29000.ckpt\n",
      "2022-08-15 23:26:30,962 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:26:30,967 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:26:30,967 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:26:30,967 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasures in the travel and one of the freudian research is to live together with people who can remember the old days. the past still feel in the wind, they touch on the rain, they touch them in the bitter leaves.\n",
      "2022-08-15 23:26:30,968 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:26:30,971 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:26:30,972 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:26:30,972 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguines still go beyond the milky street or the meaning of myths are the oldest of the inuit, or that in the himalayas, the buddhist still follow the breath of the dharma, the central revelation of the anthropology is to call the vision that we think is that in the world, not only a certain way that exists in a certain sense of reality, but that the world is a very important idea of the same phenomenon.\n",
      "2022-08-15 23:26:30,972 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:26:30,975 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:26:30,975 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:26:30,975 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptions.\n",
      "2022-08-15 23:26:30,976 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:26:30,978 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:26:30,979 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:26:30,979 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring kids to the world.\n",
      "2022-08-15 23:26:30,979 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:26:30,982 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:26:30,982 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:26:30,982 - INFO - joeynmt.training - \tHypothesis: we're going to go through initiation rituals.\n",
      "2022-08-15 23:26:53,652 - INFO - joeynmt.training - Epoch  11, Step:    34100, Batch Loss:     1.885993, Batch Acc: 0.581101, Tokens per Sec:     5030, Lr: 0.000300\n",
      "2022-08-15 23:27:15,614 - INFO - joeynmt.training - Epoch  11, Step:    34200, Batch Loss:     1.933984, Batch Acc: 0.580990, Tokens per Sec:     5591, Lr: 0.000300\n",
      "2022-08-15 23:27:37,760 - INFO - joeynmt.training - Epoch  11, Step:    34300, Batch Loss:     1.918357, Batch Acc: 0.582399, Tokens per Sec:     5335, Lr: 0.000300\n",
      "2022-08-15 23:28:00,231 - INFO - joeynmt.training - Epoch  11, Step:    34400, Batch Loss:     1.803294, Batch Acc: 0.579174, Tokens per Sec:     5476, Lr: 0.000300\n",
      "2022-08-15 23:28:22,986 - INFO - joeynmt.training - Epoch  11, Step:    34500, Batch Loss:     1.938386, Batch Acc: 0.582262, Tokens per Sec:     5288, Lr: 0.000300\n",
      "2022-08-15 23:28:44,933 - INFO - joeynmt.training - Epoch  11, Step:    34600, Batch Loss:     2.055214, Batch Acc: 0.581197, Tokens per Sec:     5535, Lr: 0.000300\n",
      "2022-08-15 23:29:07,471 - INFO - joeynmt.training - Epoch  11, Step:    34700, Batch Loss:     2.031729, Batch Acc: 0.581569, Tokens per Sec:     5542, Lr: 0.000300\n",
      "2022-08-15 23:29:29,911 - INFO - joeynmt.training - Epoch  11, Step:    34800, Batch Loss:     2.005698, Batch Acc: 0.584838, Tokens per Sec:     5330, Lr: 0.000300\n",
      "2022-08-15 23:29:51,923 - INFO - joeynmt.training - Epoch  11, Step:    34900, Batch Loss:     2.009844, Batch Acc: 0.578907, Tokens per Sec:     5391, Lr: 0.000300\n",
      "2022-08-15 23:30:14,009 - INFO - joeynmt.training - Epoch  11, Step:    35000, Batch Loss:     2.022517, Batch Acc: 0.580078, Tokens per Sec:     5423, Lr: 0.000300\n",
      "2022-08-15 23:30:14,010 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:31:02,134 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:31:02,134 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.66, loss:   2.41, ppl:  11.09, acc:   0.56, generation: 46.8144[sec], evaluation: 0.3954[sec]\n",
      "2022-08-15 23:31:03,162 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/30000.ckpt\n",
      "2022-08-15 23:31:03,165 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:31:03,170 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:31:03,170 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:31:03,170 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasures in travel, and one of the pleasures in the ethicical research is to live with people who can remember the old days, and the past still feel in the wind, they touch on the rainlight blocks, they touch the bitch of the plants.\n",
      "2022-08-15 23:31:03,171 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:31:03,174 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:31:03,174 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:31:03,175 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguines still beyond the milky road or the meaning of the myth of the inuit are still very much more important, or that in the himalaya, the buddhist still follows the breath of the harma, which means to call the central anthropology of the memory, the idea that the world is not a sense of a fundamentally meaning, but a human reality, but a certain way of the world, a sense of the way that we have a human reality,\n",
      "2022-08-15 23:31:03,175 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:31:03,178 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:31:03,178 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:31:03,178 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 23:31:03,178 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:31:03,181 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:31:03,181 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:31:03,182 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring kids to the world.\n",
      "2022-08-15 23:31:03,182 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:31:03,185 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:31:03,185 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:31:03,185 - INFO - joeynmt.training - \tHypothesis: we're going through initiating ritual.\n",
      "2022-08-15 23:31:25,289 - INFO - joeynmt.training - Epoch  11, Step:    35100, Batch Loss:     2.075437, Batch Acc: 0.578226, Tokens per Sec:     5204, Lr: 0.000300\n",
      "2022-08-15 23:31:47,565 - INFO - joeynmt.training - Epoch  11, Step:    35200, Batch Loss:     2.189552, Batch Acc: 0.579343, Tokens per Sec:     5513, Lr: 0.000300\n",
      "2022-08-15 23:32:09,312 - INFO - joeynmt.training - Epoch  11, Step:    35300, Batch Loss:     1.918573, Batch Acc: 0.582628, Tokens per Sec:     5583, Lr: 0.000300\n",
      "2022-08-15 23:32:31,851 - INFO - joeynmt.training - Epoch  11, Step:    35400, Batch Loss:     2.110257, Batch Acc: 0.585635, Tokens per Sec:     5347, Lr: 0.000300\n",
      "2022-08-15 23:32:54,634 - INFO - joeynmt.training - Epoch  11, Step:    35500, Batch Loss:     1.840623, Batch Acc: 0.582346, Tokens per Sec:     5363, Lr: 0.000300\n",
      "2022-08-15 23:32:58,647 - INFO - joeynmt.training - Epoch  11: total training loss 6471.40\n",
      "2022-08-15 23:32:58,648 - INFO - joeynmt.training - EPOCH 12\n",
      "2022-08-15 23:33:16,496 - INFO - joeynmt.training - Epoch  12, Step:    35600, Batch Loss:     2.078846, Batch Acc: 0.593957, Tokens per Sec:     5402, Lr: 0.000300\n",
      "2022-08-15 23:33:38,301 - INFO - joeynmt.training - Epoch  12, Step:    35700, Batch Loss:     1.936660, Batch Acc: 0.598083, Tokens per Sec:     5527, Lr: 0.000300\n",
      "2022-08-15 23:34:00,019 - INFO - joeynmt.training - Epoch  12, Step:    35800, Batch Loss:     2.096502, Batch Acc: 0.595218, Tokens per Sec:     5568, Lr: 0.000300\n",
      "2022-08-15 23:34:22,398 - INFO - joeynmt.training - Epoch  12, Step:    35900, Batch Loss:     2.019913, Batch Acc: 0.588148, Tokens per Sec:     5469, Lr: 0.000300\n",
      "2022-08-15 23:34:44,959 - INFO - joeynmt.training - Epoch  12, Step:    36000, Batch Loss:     1.840776, Batch Acc: 0.592080, Tokens per Sec:     5494, Lr: 0.000300\n",
      "2022-08-15 23:34:44,960 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:35:32,894 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:35:32,894 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.54, loss:   2.39, ppl:  10.91, acc:   0.56, generation: 46.6171[sec], evaluation: 0.6409[sec]\n",
      "2022-08-15 23:35:32,895 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 23:35:34,378 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/31000.ckpt\n",
      "2022-08-15 23:35:34,382 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:35:34,387 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:35:34,387 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:35:34,387 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of travel and a freuthegraphic research is to live with people who still remember the old days. the past still feel in the wind, they touch on the cold rocks, they love in the bitbits of the plants.\n",
      "2022-08-15 23:35:34,387 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:35:34,391 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:35:34,392 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:35:34,392 - INFO - joeynmt.training - \tHypothesis: just the idea that jaguers still go beyond the milky way or the importance of the myths of the inuit are still very important, or that in the himalayas, the buddhist of the dharma, is to call the central revelation of the anthropology to mind, the idea that we know in the world, not only in a sense of life, but the fact that we have a certain way of the reality of the world as a function of the human being, but the\n",
      "2022-08-15 23:35:34,392 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:35:34,395 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:35:34,395 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:35:34,395 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 23:35:34,395 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:35:34,398 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:35:34,398 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:35:34,398 - INFO - joeynmt.training - \tHypothesis: we're all born. we're bringing kids to the world.\n",
      "2022-08-15 23:35:34,399 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:35:34,401 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:35:34,402 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:35:34,402 - INFO - joeynmt.training - \tHypothesis: we're analyzing ritual.\n",
      "2022-08-15 23:35:56,877 - INFO - joeynmt.training - Epoch  12, Step:    36100, Batch Loss:     1.971089, Batch Acc: 0.589352, Tokens per Sec:     5211, Lr: 0.000300\n",
      "2022-08-15 23:36:18,678 - INFO - joeynmt.training - Epoch  12, Step:    36200, Batch Loss:     1.930388, Batch Acc: 0.591635, Tokens per Sec:     5587, Lr: 0.000300\n",
      "2022-08-15 23:36:40,907 - INFO - joeynmt.training - Epoch  12, Step:    36300, Batch Loss:     1.953921, Batch Acc: 0.590572, Tokens per Sec:     5407, Lr: 0.000300\n",
      "2022-08-15 23:37:03,733 - INFO - joeynmt.training - Epoch  12, Step:    36400, Batch Loss:     1.905201, Batch Acc: 0.591046, Tokens per Sec:     5347, Lr: 0.000300\n",
      "2022-08-15 23:37:26,288 - INFO - joeynmt.training - Epoch  12, Step:    36500, Batch Loss:     1.933681, Batch Acc: 0.588107, Tokens per Sec:     5359, Lr: 0.000300\n",
      "2022-08-15 23:37:48,386 - INFO - joeynmt.training - Epoch  12, Step:    36600, Batch Loss:     2.006906, Batch Acc: 0.590414, Tokens per Sec:     5505, Lr: 0.000300\n",
      "2022-08-15 23:38:10,422 - INFO - joeynmt.training - Epoch  12, Step:    36700, Batch Loss:     1.920579, Batch Acc: 0.587842, Tokens per Sec:     5431, Lr: 0.000300\n",
      "2022-08-15 23:38:32,227 - INFO - joeynmt.training - Epoch  12, Step:    36800, Batch Loss:     1.981639, Batch Acc: 0.588611, Tokens per Sec:     5443, Lr: 0.000300\n",
      "2022-08-15 23:38:54,705 - INFO - joeynmt.training - Epoch  12, Step:    36900, Batch Loss:     1.974408, Batch Acc: 0.590586, Tokens per Sec:     5518, Lr: 0.000300\n",
      "2022-08-15 23:39:17,284 - INFO - joeynmt.training - Epoch  12, Step:    37000, Batch Loss:     2.086728, Batch Acc: 0.584122, Tokens per Sec:     5420, Lr: 0.000300\n",
      "2022-08-15 23:39:17,285 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:40:00,983 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:40:00,983 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.91, loss:   2.39, ppl:  10.95, acc:   0.56, generation: 42.6212[sec], evaluation: 0.3960[sec]\n",
      "2022-08-15 23:40:02,029 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/35000.ckpt\n",
      "2022-08-15 23:40:02,034 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:40:02,039 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:40:02,039 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:40:02,039 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of travel, and one of the freuden research is to live with people who can remember the old days, and the past still feel in the wind, they touch on the rain, they're touching the more difficult bits of plants.\n",
      "2022-08-15 23:40:02,039 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:40:02,043 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:40:02,044 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:40:02,044 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguar schanananab schans still go beyond the milky way or the importance of myths are the oldest of the inuit, or that in the himalayas, the buddhist is still tracking the breath of the harma, which is the central revelation of the vision of the idea that we don't think that in a sense of life, but that we have a sense of the reality of the way that exists in a certain way of the evolution of\n",
      "2022-08-15 23:40:02,044 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:40:02,047 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:40:02,047 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:40:02,048 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 23:40:02,048 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:40:02,051 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:40:02,051 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:40:02,051 - INFO - joeynmt.training - \tHypothesis: we're all born. we're bringing kids to the world.\n",
      "2022-08-15 23:40:02,051 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:40:02,054 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:40:02,054 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:40:02,055 - INFO - joeynmt.training - \tHypothesis: we're running through initiation.\n",
      "2022-08-15 23:40:24,320 - INFO - joeynmt.training - Epoch  12, Step:    37100, Batch Loss:     1.923276, Batch Acc: 0.588874, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2022-08-15 23:40:46,508 - INFO - joeynmt.training - Epoch  12, Step:    37200, Batch Loss:     1.813742, Batch Acc: 0.592509, Tokens per Sec:     5498, Lr: 0.000300\n",
      "2022-08-15 23:41:09,277 - INFO - joeynmt.training - Epoch  12, Step:    37300, Batch Loss:     2.031778, Batch Acc: 0.591606, Tokens per Sec:     5366, Lr: 0.000300\n",
      "2022-08-15 23:41:31,256 - INFO - joeynmt.training - Epoch  12, Step:    37400, Batch Loss:     1.954256, Batch Acc: 0.589595, Tokens per Sec:     5571, Lr: 0.000300\n",
      "2022-08-15 23:41:53,609 - INFO - joeynmt.training - Epoch  12, Step:    37500, Batch Loss:     1.920890, Batch Acc: 0.591425, Tokens per Sec:     5465, Lr: 0.000300\n",
      "2022-08-15 23:42:15,820 - INFO - joeynmt.training - Epoch  12, Step:    37600, Batch Loss:     1.800635, Batch Acc: 0.585939, Tokens per Sec:     5381, Lr: 0.000300\n",
      "2022-08-15 23:42:37,687 - INFO - joeynmt.training - Epoch  12, Step:    37700, Batch Loss:     1.879681, Batch Acc: 0.588910, Tokens per Sec:     5632, Lr: 0.000300\n",
      "2022-08-15 23:42:59,746 - INFO - joeynmt.training - Epoch  12, Step:    37800, Batch Loss:     1.791359, Batch Acc: 0.589680, Tokens per Sec:     5499, Lr: 0.000300\n",
      "2022-08-15 23:43:22,280 - INFO - joeynmt.training - Epoch  12, Step:    37900, Batch Loss:     2.000903, Batch Acc: 0.585863, Tokens per Sec:     5449, Lr: 0.000300\n",
      "2022-08-15 23:43:44,685 - INFO - joeynmt.training - Epoch  12, Step:    38000, Batch Loss:     2.087182, Batch Acc: 0.588491, Tokens per Sec:     5413, Lr: 0.000300\n",
      "2022-08-15 23:43:44,685 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:44:29,128 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:44:29,129 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.65, loss:   2.38, ppl:  10.79, acc:   0.56, generation: 43.3750[sec], evaluation: 0.3935[sec]\n",
      "2022-08-15 23:44:29,129 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-15 23:44:30,204 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/32000.ckpt\n",
      "2022-08-15 23:44:30,209 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:44:30,214 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:44:30,214 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:44:30,214 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of traveling and one of the freuden in the ethnographic research is to live with people who can remember the old days, and the past still feel in the wind, they touch on the rain, they taste them in the bitter leaves.\n",
      "2022-08-15 23:44:30,214 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:44:30,218 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:44:30,218 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:44:30,218 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguar schemes still beyond the milky way or the meaning of myths are the oldest of the inuit of the inuit, or that in himalayas, the buddhist still follow the breath of dharma, which means to restore the central agreement of anthropology to memory, which is that in the world, we don't think that in a perfect model, but only have a certain sense of insight, but that exists in the event of the number of insight\n",
      "2022-08-15 23:44:30,218 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:44:30,221 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:44:30,222 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:44:30,222 - INFO - joeynmt.training - \tHypothesis: and of course, we all share the same adaptability.\n",
      "2022-08-15 23:44:30,222 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:44:30,225 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:44:30,225 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:44:30,226 - INFO - joeynmt.training - \tHypothesis: we're all born. we're bringing kids to the world.\n",
      "2022-08-15 23:44:30,226 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:44:30,229 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:44:30,229 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:44:30,229 - INFO - joeynmt.training - \tHypothesis: we're running through initiation rituals.\n",
      "2022-08-15 23:44:51,927 - INFO - joeynmt.training - Epoch  12, Step:    38100, Batch Loss:     1.778052, Batch Acc: 0.586130, Tokens per Sec:     5428, Lr: 0.000300\n",
      "2022-08-15 23:45:14,957 - INFO - joeynmt.training - Epoch  12, Step:    38200, Batch Loss:     1.857261, Batch Acc: 0.591571, Tokens per Sec:     5463, Lr: 0.000300\n",
      "2022-08-15 23:45:38,306 - INFO - joeynmt.training - Epoch  12, Step:    38300, Batch Loss:     1.856925, Batch Acc: 0.584397, Tokens per Sec:     5260, Lr: 0.000300\n",
      "2022-08-15 23:46:00,372 - INFO - joeynmt.training - Epoch  12, Step:    38400, Batch Loss:     1.968612, Batch Acc: 0.588483, Tokens per Sec:     5365, Lr: 0.000300\n",
      "2022-08-15 23:46:22,797 - INFO - joeynmt.training - Epoch  12, Step:    38500, Batch Loss:     1.928142, Batch Acc: 0.587270, Tokens per Sec:     5441, Lr: 0.000300\n",
      "2022-08-15 23:46:44,832 - INFO - joeynmt.training - Epoch  12, Step:    38600, Batch Loss:     1.988547, Batch Acc: 0.591638, Tokens per Sec:     5401, Lr: 0.000300\n",
      "2022-08-15 23:47:06,814 - INFO - joeynmt.training - Epoch  12, Step:    38700, Batch Loss:     1.928357, Batch Acc: 0.587294, Tokens per Sec:     5536, Lr: 0.000300\n",
      "2022-08-15 23:47:15,530 - INFO - joeynmt.training - Epoch  12: total training loss 6331.72\n",
      "2022-08-15 23:47:15,530 - INFO - joeynmt.training - EPOCH 13\n",
      "2022-08-15 23:47:28,608 - INFO - joeynmt.training - Epoch  13, Step:    38800, Batch Loss:     1.924394, Batch Acc: 0.599615, Tokens per Sec:     5559, Lr: 0.000300\n",
      "2022-08-15 23:47:51,184 - INFO - joeynmt.training - Epoch  13, Step:    38900, Batch Loss:     1.828414, Batch Acc: 0.601910, Tokens per Sec:     5368, Lr: 0.000300\n",
      "2022-08-15 23:48:13,309 - INFO - joeynmt.training - Epoch  13, Step:    39000, Batch Loss:     1.818045, Batch Acc: 0.597976, Tokens per Sec:     5409, Lr: 0.000300\n",
      "2022-08-15 23:48:13,309 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:49:00,648 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:49:00,648 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.37, loss:   2.37, ppl:  10.73, acc:   0.56, generation: 46.2672[sec], evaluation: 0.3952[sec]\n",
      "2022-08-15 23:49:01,625 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/33000.ckpt\n",
      "2022-08-15 23:49:01,629 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:49:01,634 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:49:01,634 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:49:01,634 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasure of traveling, and one of the freugraphic research is to live with people who can remember the old days, and the past still feel in the wind, they touch on the rain, they're able to taste them in the more bitch of the plants.\n",
      "2022-08-15 23:49:01,634 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:49:01,638 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:49:01,639 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:49:01,639 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguar schelelers still beyond the milky way or the importance of myths of the inuit of the inuit are still profoundly important, or that in himalayas, the buddhist still tracks the breath of the dharma, which means to reviewer the central anthropology to memory, which is that the world that we live in a perfectly meaning, but only a certain model of the world, the principle of the world, the most important, the phenomenon\n",
      "2022-08-15 23:49:01,639 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:49:01,642 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:49:01,642 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:49:01,642 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 23:49:01,642 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:49:01,645 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:49:01,645 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:49:01,646 - INFO - joeynmt.training - \tHypothesis: we're all born. we're bringing children to the world.\n",
      "2022-08-15 23:49:01,646 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:49:01,649 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:49:01,649 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:49:01,649 - INFO - joeynmt.training - \tHypothesis: we're going through initiation ritual.\n",
      "2022-08-15 23:49:23,573 - INFO - joeynmt.training - Epoch  13, Step:    39100, Batch Loss:     1.808291, Batch Acc: 0.599701, Tokens per Sec:     5310, Lr: 0.000300\n",
      "2022-08-15 23:49:46,318 - INFO - joeynmt.training - Epoch  13, Step:    39200, Batch Loss:     1.939602, Batch Acc: 0.599649, Tokens per Sec:     5329, Lr: 0.000300\n",
      "2022-08-15 23:50:09,129 - INFO - joeynmt.training - Epoch  13, Step:    39300, Batch Loss:     1.949161, Batch Acc: 0.599607, Tokens per Sec:     5552, Lr: 0.000300\n",
      "2022-08-15 23:50:31,222 - INFO - joeynmt.training - Epoch  13, Step:    39400, Batch Loss:     1.903693, Batch Acc: 0.598934, Tokens per Sec:     5300, Lr: 0.000300\n",
      "2022-08-15 23:50:53,038 - INFO - joeynmt.training - Epoch  13, Step:    39500, Batch Loss:     2.032820, Batch Acc: 0.594919, Tokens per Sec:     5495, Lr: 0.000300\n",
      "2022-08-15 23:51:14,926 - INFO - joeynmt.training - Epoch  13, Step:    39600, Batch Loss:     1.962984, Batch Acc: 0.600298, Tokens per Sec:     5460, Lr: 0.000300\n",
      "2022-08-15 23:51:37,555 - INFO - joeynmt.training - Epoch  13, Step:    39700, Batch Loss:     1.918133, Batch Acc: 0.596293, Tokens per Sec:     5439, Lr: 0.000300\n",
      "2022-08-15 23:51:59,876 - INFO - joeynmt.training - Epoch  13, Step:    39800, Batch Loss:     1.999196, Batch Acc: 0.594259, Tokens per Sec:     5315, Lr: 0.000300\n",
      "2022-08-15 23:52:22,072 - INFO - joeynmt.training - Epoch  13, Step:    39900, Batch Loss:     1.969874, Batch Acc: 0.596177, Tokens per Sec:     5489, Lr: 0.000300\n",
      "2022-08-15 23:52:44,351 - INFO - joeynmt.training - Epoch  13, Step:    40000, Batch Loss:     1.882149, Batch Acc: 0.592825, Tokens per Sec:     5486, Lr: 0.000300\n",
      "2022-08-15 23:52:44,351 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:53:31,949 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:53:31,950 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.63, loss:   2.37, ppl:  10.70, acc:   0.56, generation: 46.5112[sec], evaluation: 0.3978[sec]\n",
      "2022-08-15 23:53:33,122 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/37000.ckpt\n",
      "2022-08-15 23:53:33,126 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:53:33,131 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:53:33,131 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:53:33,131 - INFO - joeynmt.training - \tHypothesis: you know, one of the great lessons to travel, and one of the joder is in the ethicical research, in the world with people who can remember the old days, and the past still feel in the wind, they touch on the rain of the tides, they taste them into the more difficult bits of plants.\n",
      "2022-08-15 23:53:33,131 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:53:33,135 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:53:33,135 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:53:33,135 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguar schans are still beyond the milky street, or the importance of the mynelines are still full meaning, or that in the himalayan world, the buddhists still follow the breath of the dharma, means to draw the center of anthropology to the memory, which is that the world is not only important in a sense, but a very important model of the world, but only exists as a result of the combination of the most important,\n",
      "2022-08-15 23:53:33,136 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:53:33,139 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:53:33,139 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:53:33,139 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 23:53:33,139 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:53:33,142 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:53:33,142 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:53:33,142 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring kids to the world.\n",
      "2022-08-15 23:53:33,142 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:53:33,145 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:53:33,145 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:53:33,146 - INFO - joeynmt.training - \tHypothesis: we're running through initiation rituals.\n",
      "2022-08-15 23:53:55,385 - INFO - joeynmt.training - Epoch  13, Step:    40100, Batch Loss:     2.040154, Batch Acc: 0.597495, Tokens per Sec:     5087, Lr: 0.000300\n",
      "2022-08-15 23:54:18,382 - INFO - joeynmt.training - Epoch  13, Step:    40200, Batch Loss:     1.830693, Batch Acc: 0.596238, Tokens per Sec:     5310, Lr: 0.000300\n",
      "2022-08-15 23:54:40,704 - INFO - joeynmt.training - Epoch  13, Step:    40300, Batch Loss:     1.938488, Batch Acc: 0.595815, Tokens per Sec:     5463, Lr: 0.000300\n",
      "2022-08-15 23:55:02,603 - INFO - joeynmt.training - Epoch  13, Step:    40400, Batch Loss:     1.864218, Batch Acc: 0.595737, Tokens per Sec:     5498, Lr: 0.000300\n",
      "2022-08-15 23:55:24,513 - INFO - joeynmt.training - Epoch  13, Step:    40500, Batch Loss:     2.053099, Batch Acc: 0.598770, Tokens per Sec:     5611, Lr: 0.000300\n",
      "2022-08-15 23:55:47,015 - INFO - joeynmt.training - Epoch  13, Step:    40600, Batch Loss:     1.926989, Batch Acc: 0.597159, Tokens per Sec:     5479, Lr: 0.000300\n",
      "2022-08-15 23:56:08,782 - INFO - joeynmt.training - Epoch  13, Step:    40700, Batch Loss:     2.010710, Batch Acc: 0.594892, Tokens per Sec:     5661, Lr: 0.000300\n",
      "2022-08-15 23:56:32,049 - INFO - joeynmt.training - Epoch  13, Step:    40800, Batch Loss:     1.788724, Batch Acc: 0.595041, Tokens per Sec:     5225, Lr: 0.000300\n",
      "2022-08-15 23:56:53,959 - INFO - joeynmt.training - Epoch  13, Step:    40900, Batch Loss:     1.839309, Batch Acc: 0.595369, Tokens per Sec:     5614, Lr: 0.000300\n",
      "2022-08-15 23:57:15,544 - INFO - joeynmt.training - Epoch  13, Step:    41000, Batch Loss:     1.877752, Batch Acc: 0.593668, Tokens per Sec:     5549, Lr: 0.000300\n",
      "2022-08-15 23:57:15,544 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-15 23:58:00,063 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-15 23:58:00,064 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.13, loss:   2.35, ppl:  10.50, acc:   0.56, generation: 43.4527[sec], evaluation: 0.4050[sec]\n",
      "2022-08-15 23:58:01,097 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/34000.ckpt\n",
      "2022-08-15 23:58:01,101 - INFO - joeynmt.training - Example #0\n",
      "2022-08-15 23:58:01,115 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-15 23:58:01,116 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-15 23:58:01,116 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasures of travel, and one of the fun that's in ethically research, with people who can remember the old days, and the past still feel in the wind, they touch on the rain, they're touching the crackers of the bitous leaves of the plants.\n",
      "2022-08-15 23:58:01,116 - INFO - joeynmt.training - Example #1\n",
      "2022-08-15 23:58:01,120 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-15 23:58:01,120 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-15 23:58:01,121 - INFO - joeynmt.training - \tHypothesis: just knowing that jaguers still have the way beyond the milky street or the meaning of the myths of the inuit are deeply significant, or that in the himalayas, the buddhist, still tracks the breath of dharma, which means to draw the central revelation of anthropology to the memory, which is the idea that in the world, in a sense, we don't only have a unique model of the reality, but the number of the number of ways that we've been\n",
      "2022-08-15 23:58:01,121 - INFO - joeynmt.training - Example #2\n",
      "2022-08-15 23:58:01,124 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-15 23:58:01,124 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-15 23:58:01,124 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-15 23:58:01,124 - INFO - joeynmt.training - Example #3\n",
      "2022-08-15 23:58:01,127 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-15 23:58:01,127 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-15 23:58:01,127 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring kids to the world.\n",
      "2022-08-15 23:58:01,127 - INFO - joeynmt.training - Example #4\n",
      "2022-08-15 23:58:01,130 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-15 23:58:01,131 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-15 23:58:01,131 - INFO - joeynmt.training - \tHypothesis: we're starting to start colonial.\n",
      "2022-08-15 23:58:23,748 - INFO - joeynmt.training - Epoch  13, Step:    41100, Batch Loss:     2.043017, Batch Acc: 0.589722, Tokens per Sec:     5102, Lr: 0.000300\n",
      "2022-08-15 23:58:45,773 - INFO - joeynmt.training - Epoch  13, Step:    41200, Batch Loss:     1.714437, Batch Acc: 0.592425, Tokens per Sec:     5421, Lr: 0.000300\n",
      "2022-08-15 23:59:07,747 - INFO - joeynmt.training - Epoch  13, Step:    41300, Batch Loss:     2.013532, Batch Acc: 0.595095, Tokens per Sec:     5574, Lr: 0.000300\n",
      "2022-08-15 23:59:29,955 - INFO - joeynmt.training - Epoch  13, Step:    41400, Batch Loss:     2.003557, Batch Acc: 0.592939, Tokens per Sec:     5551, Lr: 0.000300\n",
      "2022-08-15 23:59:51,947 - INFO - joeynmt.training - Epoch  13, Step:    41500, Batch Loss:     1.949353, Batch Acc: 0.595977, Tokens per Sec:     5461, Lr: 0.000300\n",
      "2022-08-16 00:00:13,890 - INFO - joeynmt.training - Epoch  13, Step:    41600, Batch Loss:     1.969093, Batch Acc: 0.594623, Tokens per Sec:     5506, Lr: 0.000300\n",
      "2022-08-16 00:00:36,337 - INFO - joeynmt.training - Epoch  13, Step:    41700, Batch Loss:     2.070494, Batch Acc: 0.593195, Tokens per Sec:     5451, Lr: 0.000300\n",
      "2022-08-16 00:00:58,387 - INFO - joeynmt.training - Epoch  13, Step:    41800, Batch Loss:     1.828833, Batch Acc: 0.589129, Tokens per Sec:     5312, Lr: 0.000300\n",
      "2022-08-16 00:01:20,726 - INFO - joeynmt.training - Epoch  13, Step:    41900, Batch Loss:     2.012246, Batch Acc: 0.593136, Tokens per Sec:     5437, Lr: 0.000300\n",
      "2022-08-16 00:01:36,353 - INFO - joeynmt.training - Epoch  13: total training loss 6243.89\n",
      "2022-08-16 00:01:36,353 - INFO - joeynmt.training - EPOCH 14\n",
      "2022-08-16 00:01:42,775 - INFO - joeynmt.training - Epoch  14, Step:    42000, Batch Loss:     1.827300, Batch Acc: 0.606160, Tokens per Sec:     5339, Lr: 0.000300\n",
      "2022-08-16 00:01:42,776 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 00:02:28,575 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 00:02:28,575 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.90, loss:   2.33, ppl:  10.30, acc:   0.57, generation: 44.7159[sec], evaluation: 0.3953[sec]\n",
      "2022-08-16 00:02:28,576 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 00:02:29,650 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/41000.ckpt\n",
      "2022-08-16 00:02:29,654 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 00:02:29,660 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-16 00:02:29,660 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-16 00:02:29,660 - INFO - joeynmt.training - \tHypothesis: you know, one of the great pleasures of traveling and one of the freuden in the ethic research, is to live with people who can remember the old days, and the past still feel in the wind, they touch on the rain of the rain, they taste them in the more bitous leaves of the plants.\n",
      "2022-08-16 00:02:29,660 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 00:02:29,664 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-16 00:02:29,664 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-16 00:02:29,664 - INFO - joeynmt.training - \tHypothesis: just the idea that jaguar schans still are beyond the milky way, or the meaning of the myths of the inuit are still full of meaning, or that in himalayas, the buddhists are still tracking the breath of the dharma, says to draw the central revelation of anthropology to memory, that's the idea that the world that we don't have a real model, but only define the number of ways that exists in the world, but the number of implications of\n",
      "2022-08-16 00:02:29,664 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 00:02:29,668 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-16 00:02:29,668 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-16 00:02:29,668 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptions.\n",
      "2022-08-16 00:02:29,668 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 00:02:29,671 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-16 00:02:29,671 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-16 00:02:29,672 - INFO - joeynmt.training - \tHypothesis: we're all born. we bring kids to the world.\n",
      "2022-08-16 00:02:29,672 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 00:02:29,675 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-16 00:02:29,675 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-16 00:02:29,675 - INFO - joeynmt.training - \tHypothesis: we're going to run through the privatization.\n",
      "2022-08-16 00:02:52,862 - INFO - joeynmt.training - Epoch  14, Step:    42100, Batch Loss:     1.640154, Batch Acc: 0.606786, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2022-08-16 00:03:14,428 - INFO - joeynmt.training - Epoch  14, Step:    42200, Batch Loss:     1.826662, Batch Acc: 0.604612, Tokens per Sec:     5624, Lr: 0.000300\n",
      "2022-08-16 00:03:36,030 - INFO - joeynmt.training - Epoch  14, Step:    42300, Batch Loss:     1.759245, Batch Acc: 0.603112, Tokens per Sec:     5471, Lr: 0.000300\n",
      "2022-08-16 00:03:58,058 - INFO - joeynmt.training - Epoch  14, Step:    42400, Batch Loss:     1.916583, Batch Acc: 0.601039, Tokens per Sec:     5612, Lr: 0.000300\n",
      "2022-08-16 00:04:20,781 - INFO - joeynmt.training - Epoch  14, Step:    42500, Batch Loss:     1.878928, Batch Acc: 0.605172, Tokens per Sec:     5405, Lr: 0.000300\n",
      "2022-08-16 00:04:42,992 - INFO - joeynmt.training - Epoch  14, Step:    42600, Batch Loss:     1.812599, Batch Acc: 0.601792, Tokens per Sec:     5589, Lr: 0.000300\n",
      "2022-08-16 00:05:05,319 - INFO - joeynmt.training - Epoch  14, Step:    42700, Batch Loss:     1.804100, Batch Acc: 0.601118, Tokens per Sec:     5471, Lr: 0.000300\n",
      "2022-08-16 00:05:27,145 - INFO - joeynmt.training - Epoch  14, Step:    42800, Batch Loss:     1.823404, Batch Acc: 0.599215, Tokens per Sec:     5396, Lr: 0.000300\n",
      "2022-08-16 00:05:49,530 - INFO - joeynmt.training - Epoch  14, Step:    42900, Batch Loss:     2.007395, Batch Acc: 0.598119, Tokens per Sec:     5514, Lr: 0.000300\n",
      "2022-08-16 00:06:11,942 - INFO - joeynmt.training - Epoch  14, Step:    43000, Batch Loss:     1.841224, Batch Acc: 0.601524, Tokens per Sec:     5380, Lr: 0.000300\n",
      "2022-08-16 00:06:11,942 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 00:06:59,210 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 00:06:59,210 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.11, loss:   2.33, ppl:  10.31, acc:   0.57, generation: 46.0326[sec], evaluation: 0.5036[sec]\n",
      "2022-08-16 00:06:59,211 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 00:07:00,637 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/39000.ckpt\n",
      "2022-08-16 00:07:00,641 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 00:07:00,646 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-16 00:07:00,646 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-16 00:07:00,646 - INFO - joeynmt.training - \tHypothesis: you know, one of the great examples of travel, and one of the freudian talks about the ethical research, in common with the people who can remember the old days, and the past still feel in the wind, they touch on the rain, they touch them in the bitter leaves of the bitsom.\n",
      "2022-08-16 00:07:00,647 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 00:07:00,650 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-16 00:07:00,650 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-16 00:07:00,651 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguar schans still have to travel beyond the milky way, or the meaning of myths, the oldest of the inuit are still profoundly important, or in himalayas, the buddhists still follow the breath of dharma, meaning to restore the central revelation of anthropology to memory, which is that the world is that we don't have a real meaning, but a certain way of negotiating reality, but a group of reality is a certain way of reality.\n",
      "2022-08-16 00:07:00,651 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 00:07:00,654 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-16 00:07:00,654 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-16 00:07:00,654 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptability.\n",
      "2022-08-16 00:07:00,654 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 00:07:00,657 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-16 00:07:00,657 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-16 00:07:00,657 - INFO - joeynmt.training - \tHypothesis: we're all born. we're bringing kids to the world.\n",
      "2022-08-16 00:07:00,658 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 00:07:00,662 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-16 00:07:00,662 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-16 00:07:00,662 - INFO - joeynmt.training - \tHypothesis: we're going to go through ritual.\n",
      "2022-08-16 00:07:23,343 - INFO - joeynmt.training - Epoch  14, Step:    43100, Batch Loss:     1.799344, Batch Acc: 0.602724, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2022-08-16 00:07:45,052 - INFO - joeynmt.training - Epoch  14, Step:    43200, Batch Loss:     1.995772, Batch Acc: 0.598174, Tokens per Sec:     5378, Lr: 0.000300\n",
      "2022-08-16 00:08:06,760 - INFO - joeynmt.training - Epoch  14, Step:    43300, Batch Loss:     1.899611, Batch Acc: 0.595423, Tokens per Sec:     5489, Lr: 0.000300\n",
      "2022-08-16 00:08:28,764 - INFO - joeynmt.training - Epoch  14, Step:    43400, Batch Loss:     1.800556, Batch Acc: 0.594586, Tokens per Sec:     5390, Lr: 0.000300\n",
      "2022-08-16 00:08:50,826 - INFO - joeynmt.training - Epoch  14, Step:    43500, Batch Loss:     1.856773, Batch Acc: 0.597352, Tokens per Sec:     5454, Lr: 0.000300\n",
      "2022-08-16 00:09:14,177 - INFO - joeynmt.training - Epoch  14, Step:    43600, Batch Loss:     1.762520, Batch Acc: 0.599849, Tokens per Sec:     5095, Lr: 0.000300\n",
      "2022-08-16 00:09:36,360 - INFO - joeynmt.training - Epoch  14, Step:    43700, Batch Loss:     1.778504, Batch Acc: 0.599586, Tokens per Sec:     5471, Lr: 0.000300\n",
      "2022-08-16 00:09:58,211 - INFO - joeynmt.training - Epoch  14, Step:    43800, Batch Loss:     1.954127, Batch Acc: 0.595595, Tokens per Sec:     5494, Lr: 0.000300\n",
      "2022-08-16 00:10:20,271 - INFO - joeynmt.training - Epoch  14, Step:    43900, Batch Loss:     1.959280, Batch Acc: 0.593568, Tokens per Sec:     5448, Lr: 0.000300\n",
      "2022-08-16 00:10:42,320 - INFO - joeynmt.training - Epoch  14, Step:    44000, Batch Loss:     1.888745, Batch Acc: 0.600250, Tokens per Sec:     5486, Lr: 0.000300\n",
      "2022-08-16 00:10:42,321 - INFO - joeynmt.prediction - Predicting 2052 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 00:11:30,897 - INFO - joeynmt.metrics - nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 00:11:30,898 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.78, loss:   2.32, ppl:  10.20, acc:   0.57, generation: 47.4943[sec], evaluation: 0.3989[sec]\n",
      "2022-08-16 00:11:31,960 - INFO - joeynmt.helpers - delete iwslt14_deen_fastbpe/36000.ckpt\n",
      "2022-08-16 00:11:31,963 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 00:11:31,969 - INFO - joeynmt.training - \tSource:     wissen sie , eines der großen vernügen beim reisen und eine der freuden bei der ethnographischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen geglätteten steinen berühren , sie in den bitteren blättern der pflanzen schmecken .\n",
      "2022-08-16 00:11:31,969 - INFO - joeynmt.training - \tReference:  you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .\n",
      "2022-08-16 00:11:31,969 - INFO - joeynmt.training - \tHypothesis: you know, one of the great joy of travel, and one of the fun ones in the ethicic research is to live with the people who can remember the old days, and the past still feel in the wind, they touch on the rain, touch rocks, they taste them in the bitous leaves of the plants.\n",
      "2022-08-16 00:11:31,970 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 00:11:31,976 - INFO - joeynmt.training - \tSource:     einfach das wissen , dass jaguar-schamanen noch immer jenseits der milchstraße reisen oder die bedeutung der mythen der ältesten der inuit noch voller bedeutung sind , oder dass im himalaya die buddhisten noch immer den atem des dharma verfolgen , bedeutet , sich die zentrale offenbarung der anthropologie ins gedächtnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absoluten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpassung die unsere ahnen , wenngleich erfolgreich , vor vielen generationen wählten .\n",
      "2022-08-16 00:11:31,976 - INFO - joeynmt.training - \tReference:  just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .\n",
      "2022-08-16 00:11:31,976 - INFO - joeynmt.training - \tHypothesis: just the knowledge that jaguers still follow the milky way beyond the milky street or the meaning of the myelelinuit is much more important, or that in himalayas, the buddhist still follow the breath of dharma, is to draw the central anthropology of the eye, which is the idea that we don't only think in a perfect sense of life, but only as a result of a group of the possibility, the implications of the number of possibilities that we\n",
      "2022-08-16 00:11:31,977 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 00:11:31,979 - INFO - joeynmt.training - \tSource:     und natürlich teilen wir alle dieselben anpassungsnotwendigkeiten .\n",
      "2022-08-16 00:11:31,980 - INFO - joeynmt.training - \tReference:  and of course , we all share the same adaptive imperatives .\n",
      "2022-08-16 00:11:31,980 - INFO - joeynmt.training - \tHypothesis: and of course, we share all the same adaptions.\n",
      "2022-08-16 00:11:31,980 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 00:11:31,983 - INFO - joeynmt.training - \tSource:     wir werden alle geboren . wir bringen kinder zur welt .\n",
      "2022-08-16 00:11:31,983 - INFO - joeynmt.training - \tReference:  we &apos;re all born . we all bring our children into the world .\n",
      "2022-08-16 00:11:31,983 - INFO - joeynmt.training - \tHypothesis: we're all born. we're going to bring kids to the world.\n",
      "2022-08-16 00:11:31,984 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 00:11:31,987 - INFO - joeynmt.training - \tSource:     wir durchlaufen initiationsrituale .\n",
      "2022-08-16 00:11:31,987 - INFO - joeynmt.training - \tReference:  we go through initiation rites .\n",
      "2022-08-16 00:11:31,987 - INFO - joeynmt.training - \tHypothesis: we're going to run through ritual.\n"
     ]
    }
   ],
   "source": [
    "!cd {root_dir} && python -m joeynmt train data/iwslt14/fastbpe_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hQ3YqxZCe4dw",
   "metadata": {
    "id": "hQ3YqxZCe4dw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i8xdpHS3e4rC",
   "metadata": {
    "id": "i8xdpHS3e4rC"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HlhiQk4mfMIO",
   "metadata": {
    "id": "HlhiQk4mfMIO"
   },
   "source": [
    "## Split on Whitespaces\n",
    "\n",
    "\n",
    "JoeyNMT will split texts on whitespaces when you specify `level=\"word\"` in the config.\n",
    "It is useful if your input texts are alreay tokenized by someone else and you have to retain it.\n",
    "\n",
    "Here, we show a sample usecase with iwslt15 en-vi dataset preprocessed by Stanford NLP group.\n",
    "https://nlp.stanford.edu/projects/nmt/\n",
    "\n",
    "Download the dataset builder script and vocab files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aXn4f7vxiSgk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXn4f7vxiSgk",
    "outputId": "b89ec1ce-d490-4fd4-828c-93c8ca9520af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-16 07:13:22--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 139741 (136K) [text/plain]\n",
      "Saving to: ‘/content/drive/MyDrive/data/iwslt15/vocab.en’\n",
      "\n",
      "/content/drive/MyDr 100%[===================>] 136.47K   561KB/s    in 0.2s    \n",
      "\n",
      "2022-08-16 07:13:23 (561 KB/s) - ‘/content/drive/MyDrive/data/iwslt15/vocab.en’ saved [139741/139741]\n",
      "\n",
      "--2022-08-16 07:13:23--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 46767 (46K) [text/plain]\n",
      "Saving to: ‘/content/drive/MyDrive/data/iwslt15/vocab.vi’\n",
      "\n",
      "/content/drive/MyDr 100%[===================>]  45.67K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-08-16 07:13:23 (469 KB/s) - ‘/content/drive/MyDrive/data/iwslt15/vocab.vi’ saved [46767/46767]\n",
      "\n",
      "--2022-08-16 07:13:23--  https://raw.githubusercontent.com/may-/datasets/master/datasets/iwslt15/iwslt15.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2022-08-16 07:13:23 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir {root_dir}/data/iwslt15\n",
    "!wget -O {root_dir}/data/iwslt15/vocab.en https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en\n",
    "!wget -O {root_dir}/data/iwslt15/vocab.vi https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi\n",
    "!wget -O {root_dir}/data/iwslt15/iwslt15.py https://raw.githubusercontent.com/may-/datasets/master/datasets/iwslt15/iwslt15.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffe99e-3fab-4e5b-a0a9-7b8b4f30537b",
   "metadata": {},
   "source": [
    "We have separated vocab files per language, not a single joint vocab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nSBGU5f6jr6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSBGU5f6jr6b",
    "outputId": "be4e8da3-2653-41c5-db63-df4ca1c61266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<s>\n",
      "</s>\n",
      "Rachel\n",
      ":\n",
      "The\n",
      "science\n",
      "behind\n",
      "a\n",
      "climate\n"
     ]
    }
   ],
   "source": [
    "!head -10 {root_dir}/data/iwslt15/vocab.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ioVaJ2SWmB7S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioVaJ2SWmB7S",
    "outputId": "a3f57197-43bc-49cf-bd0e-f1a58c8363a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<s>\n",
      "</s>\n",
      "Khoa\n",
      "học\n",
      "đằng\n",
      "sau\n",
      "một\n",
      "tiêu\n",
      "đề\n"
     ]
    }
   ],
   "source": [
    "!head -10 {root_dir}/data/iwslt15/vocab.vi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_2LK841kkbuT",
   "metadata": {
    "id": "_2LK841kkbuT"
   },
   "source": [
    "Download the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "LNiKDmOljsFD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547,
     "referenced_widgets": [
      "d8bea9ed290e4e9093280ecf30e40e76",
      "11208e71ca224360bb3a19428d1666e0",
      "e811ab05b4004f8c9ca5db7d5653d365",
      "8b28cd538fdc4de8bd1715d7da4f9660",
      "2abcc7810447406c8a0fa08dc0a4fb3e",
      "b170f87f3eed49539b296cb04bb1327c",
      "ec3867f8184a447bb304ef26a53095fd",
      "462ccf11cf0246f28046ff219bf0db8e",
      "5521232d155e463e8c6d79503423895b",
      "45b03a72f012428a97da972d86f9229f",
      "8b548b1f5fd34e5fb2a2591c848ae41d",
      "62d71c6432f44d2d928063ece3de65a3",
      "2019cd63192647b392ae48e85ebfa1b8",
      "b53672c8f0c5421b927ac73110175af1",
      "859974f5e60a4cd39803273ec0af372b",
      "ab9d39d5aec34b34aa5dc135385ab51f",
      "d155c2fc2e05415aa4a86ca814998457",
      "559fb3b3c0a64e5fbfbe78d54efbb90f",
      "3800772f0bc74a308dbc1939d06f8d4b",
      "e88cf4d0dd0f4decb820965372afa113",
      "4e7a6483739a4ce38f8d6ebe900a89f6",
      "19777cefd32a47408b5c768570775f6d",
      "0f0b7b9be81643ea83c065c56f588824",
      "5e96c517c01c4329a46241e33e89d219",
      "bb40cf7eebaf4628a2d6d5d01dbef583",
      "d5ee389e7e344fc48ed328810fe91f51",
      "c238e479b18c44839d46cc557c1be27d",
      "ad52b9c845874da69b591ac6b4df1e22",
      "9cfa0525d6344e0c9a4b2e07f637df6b",
      "97a8756ce6224f379223ef76052d73ce",
      "86cc4d44365a4e5aa9b6fa27d3c0932e",
      "d089b50fd8624d0e916a8052786ca6d5",
      "ae23eecee81348999c8862c4c929e5a8",
      "55e99e911d9842fda2aa7122cdd4fc09",
      "0c718d2705fd4372b570943312aad077",
      "bf55263c138c45daabba0498c79056aa",
      "7a2510b2452f4303a98d010b4660486e",
      "bd80be291e7546c0886f7938283d22bd",
      "2cfa7e4f49154f45be0d0c0ca91f4c8e",
      "65da89eebd6342cd908e0d725ab2ab49",
      "204178f1f7bd4ab892fa5bdf47909dcb",
      "f6309d69d64d4ac9998481790cbca063",
      "864cf880c1d64da6a51a7e9a46ccba2a",
      "d4cd39762dbe4ae5b2ad9d943c7f4151",
      "46452d95277d4e0a9c7662be89ee48f4",
      "30c51062df1e44dda5dfc3ab1ea5fec5",
      "73f568b15238421ebc8af92e00f5e5d4",
      "9bd7fdb25052491e877a47dad9f2827c",
      "5ea8a4c33e434fc1847e17328dc69f4c",
      "1f819700db504313a5fdf566a07261b1",
      "0d867e7ef95e4217b82254f7fe5ce6ec",
      "aaabdb5cdca3447d880d5d60576e0664",
      "e5fbd6b53bb94051b4235a57853591f1",
      "769c25ababe6478587c5f0e117c49103",
      "dd7047324fc1435098c471496b2285e5",
      "bd227b6e98a348f8886aed0bfc4ba94d",
      "f1a4244327be4edfab6c20a940b40579",
      "19c1677a88a6439e8ee983512672b5f0",
      "f1be400a1273449ea6179e6a3229b136",
      "0a61fbf753d644acbcbab8cf37bfb227",
      "bc864feb25be4f798b18ad5af700e260",
      "96ea1ea254e64e618df32643a7c923dd",
      "71aef2d1db4f4054bb3e67df794141aa",
      "ac0ec675aa4f4de7b7a07c217476e1ba",
      "f796fe15ff6a4127a6288fad1f0c5331",
      "3d562f2f6cdf4dfdaffac3e0eb9bf213",
      "959a1822eb3643029185daffd0e6cb63",
      "d3f7d48f24ee4efa91018c2ad43c619b",
      "0fa6558cf20a4598b11db115fcc6e2fc",
      "46f5ff45c5d542c3a1dc7595d6dc65c9",
      "e8d9d6b9ad2e42819beaa2dfdd52a63f",
      "a1930c66806644318d3893a34286939a",
      "9528cd36b35a41fbb8eb21910f172b4a",
      "869fb4a9a5e54cefb9adac572807286d",
      "357925fee60e498a9873d41352be0d89",
      "63e63430d1b2485d8d9988d7c09bed0d",
      "8fc70a6a14144814a9ca3e825abc019f",
      "da8728572b1d401abe579368f65155ee",
      "60ce6bd2a23e416fa16bfa98376a31bd",
      "1664592b61e14e4b9c87150580e573b6",
      "6727165b8edc4733a4f7ce08fc7b9da5",
      "2a2636cc0c4647bbbbf1d0d261376f35",
      "fd00a0a4c0724f3080a5de179c3ca880",
      "937659bc1dec4e248e786ad3a8bc686f",
      "af901ab329c0414ebd2c243768826c82",
      "6257b78b5e014e40a8cbae49dd0fcd42",
      "cea2bf9c6dd54056b4cc854edd9273ff",
      "a062972fdd514593bcdb9dff844da9fb",
      "0088e966ec24479e93c0708943d7c50a",
      "42ac07d93bd04fcdaf84e4816bd148f8",
      "2d1580ec31fe4095ac7cf88910ba8786",
      "bca867bc93f8414b92045a5edb6d804a",
      "0f91969769b740bda999dedda5749d11",
      "4f15fe947b0b44c69d2b6b6f7883a734",
      "9fae32990eb14b48a9b3252f14456336",
      "7d7179d4fb2a4aa29d9358074503cb9a",
      "858702e842174e47a39919997e5dcdfe",
      "adb3cbd075684381b4f02b69fd762b3e",
      "fa7d7885b28446f58bbc264b8a305a4e",
      "bf93c428bd484902935a22cad8c8e3dd",
      "0d5d96469e5d490aa205e73477a0b424",
      "d23ef3166bec4aeba9bf4ff6a51d0f4c",
      "08b737b565784db299378bad41c3453e",
      "9c4ddccfde3c41d79c700d89ed863ce7",
      "e2fcba7e77094c9ab93783addebbca12",
      "44df3a9d1b4047c0bfc87c023d6c14bf",
      "a6ddfdd4787a467381cf8401d1264fa8",
      "8894c6cd80ec4a9990aabba67edfda5d",
      "d0b0ecf86aa14e068d4b963f1505cca6",
      "3eb076444f644f3493987876b974da34"
     ]
    },
    "id": "LNiKDmOljsFD",
    "outputId": "9da9d239-f93c-4960-8782-d5a3eb7855a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset iwslt15/en-vi to /content/drive/MyDrive/.cache/iwslt15/en-vi/1.0.0/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bea9ed290e4e9093280ecf30e40e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d71c6432f44d2d928063ece3de65a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/18.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0b7b9be81643ea83c065c56f588824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/140k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e99e911d9842fda2aa7122cdd4fc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/188k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46452d95277d4e0a9c7662be89ee48f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/132k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd227b6e98a348f8886aed0bfc4ba94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/184k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959a1822eb3643029185daffd0e6cb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8728572b1d401abe579368f65155ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0088e966ec24479e93c0708943d7c50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset iwslt15 downloaded and prepared to /content/drive/MyDrive/.cache/iwslt15/en-vi/1.0.0/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf93c428bd484902935a22cad8c8e3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 133317\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1553\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "iwslt15_envi = load_dataset(f\"{root_dir}/data/iwslt15\", name=\"en-vi\")\n",
    "iwslt15_envi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_gc01Y0xkrRn",
   "metadata": {
    "id": "_gc01Y0xkrRn"
   },
   "source": [
    "Inspect the data. We can see the punctuations are already separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "UgjJw4PKkobb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgjJw4PKkobb",
    "outputId": "8d1b6aeb-bd94-48e5-c6c1-64e36bb042fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Over 15,000 scientists go to San Francisco every year for that .',\n",
       " 'vi': 'Mỗi năm , hơn 15,000 nhà khoa học đến San Francisco để tham dự hội nghị này .'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwslt15_envi['train'][10]['translation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PDBWPeDXi-sQ",
   "metadata": {
    "id": "PDBWPeDXi-sQ"
   },
   "source": [
    "Create a config file specifying `level: \"word\"` in both src and trg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "XucerPGce4u6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XucerPGce4u6",
    "outputId": "c0f399f9-ead8-4497-a9fd-c6849887990c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2421"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwslt15_envi_config = \"\"\"\n",
    "name: \"transformer_iwslt15_envi\"\n",
    "joeynmt_version: \"2.0.0\"\n",
    "\n",
    "data:\n",
    "    train: \"iwslt15\"\n",
    "    dev: \"iwslt15\"\n",
    "    test: \"iwslt15\"\n",
    "    dataset_type: \"huggingface\"\n",
    "    dataset_cfg:\n",
    "        name: \"en-vi\"\n",
    "    src:\n",
    "        lang: \"en\"\n",
    "        lowercase: False\n",
    "        normalize: False\n",
    "        level: \"word\"\n",
    "        voc_file: \"data/iwslt15/vocab.en\"\n",
    "        tokenizer_cfg:\n",
    "            pretokenizer: \"none\"\n",
    "    trg:\n",
    "        lang: \"vi\"\n",
    "        lowercase: False\n",
    "        normalize: False\n",
    "        level: \"word\"\n",
    "        voc_file: \"data/iwslt15/vocab.vi\"\n",
    "        tokenizer_cfg:\n",
    "            pretokenizer: \"none\"\n",
    "\n",
    "testing:\n",
    "    n_best: 1\n",
    "    beam_size: 5\n",
    "    beam_alpha: 1.0\n",
    "    batch_size: 1024\n",
    "    batch_type: \"token\"\n",
    "    max_output_length: 150\n",
    "    eval_metrics: [\"bleu\"]\n",
    "    return_prob: \"none\"\n",
    "    return_attention: False\n",
    "    sacrebleu_cfg:\n",
    "        tokenize: \"13a\"\n",
    "        lowercase: False\n",
    "\n",
    "training:\n",
    "    #load_model: \"iwslt15_envi/best.ckpt\"\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.99]\n",
    "    scheduling: \"warmupinversesquareroot\"\n",
    "    learning_rate_warmup: 4000\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0001\n",
    "    learning_rate_min: 0.000005\n",
    "    weight_decay: 0.0\n",
    "    clip_grad_norm: 1.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_multiplier: 4\n",
    "    batch_size: 1024\n",
    "    batch_type: \"token\"\n",
    "    early_stopping_metric: \"bleu\"\n",
    "    epochs: 40\n",
    "    validation_freq: 1000\n",
    "    logging_freq: 100\n",
    "    model_dir: \"iwslt15_envi\"\n",
    "    overwrite: False\n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    print_valid_sents: [0, 1, 2, 3, 4]\n",
    "    keep_best_ckpts: 5\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier_uniform\"\n",
    "    embed_initializer: \"xavier_uniform\"\n",
    "    embed_init_gain: 1.0\n",
    "    init_gain: 1.0\n",
    "    bias_initializer: \"zeros\"\n",
    "    tied_embeddings: False\n",
    "    tied_softmax: False\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.1\n",
    "        layer_norm: \"pre\"\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.1\n",
    "        layer_norm: \"pre\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "(Path(root_dir) / 'data/iwslt15/config.yaml').write_text(iwslt15_envi_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf6b81-1a5e-4b51-be46-f49c2dfdd290",
   "metadata": {
    "id": "U21eWcwsjdtE"
   },
   "source": [
    "Start training with preprocessed input texts. You will see \"BasicTokenizer\" in the lines with \"INFO - joeynmt.tokenizers\".  \n",
    "\n",
    "Please pay attention not only to the console log, but also to the output of \"train.log\" file. There, you can observe the model generation before detokenizing, besides the post-processed string outputs, in the validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wwIusvYJjdpt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwIusvYJjdpt",
    "outputId": "0efd4d45-2f19-45f0-b18b-dd00a8863e37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-16 07:22:52,896 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-08-16 07:22:52,897 - INFO - joeynmt.helpers -                           cfg.name : transformer_iwslt15_envi\n",
      "2022-08-16 07:22:52,898 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0\n",
      "2022-08-16 07:22:52,898 - INFO - joeynmt.helpers -                     cfg.data.train : iwslt15\n",
      "2022-08-16 07:22:52,898 - INFO - joeynmt.helpers -                       cfg.data.dev : iwslt15\n",
      "2022-08-16 07:22:52,898 - INFO - joeynmt.helpers -                      cfg.data.test : iwslt15\n",
      "2022-08-16 07:22:52,898 - INFO - joeynmt.helpers -              cfg.data.dataset_type : huggingface\n",
      "2022-08-16 07:22:52,898 - INFO - joeynmt.helpers -          cfg.data.dataset_cfg.name : en-vi\n",
      "2022-08-16 07:22:52,899 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en\n",
      "2022-08-16 07:22:52,899 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False\n",
      "2022-08-16 07:22:52,899 - INFO - joeynmt.helpers -             cfg.data.src.normalize : False\n",
      "2022-08-16 07:22:52,899 - INFO - joeynmt.helpers -                 cfg.data.src.level : word\n",
      "2022-08-16 07:22:52,899 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : data/iwslt15/vocab.en\n",
      "2022-08-16 07:22:52,899 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none\n",
      "2022-08-16 07:22:52,899 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : vi\n",
      "2022-08-16 07:22:52,900 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False\n",
      "2022-08-16 07:22:52,900 - INFO - joeynmt.helpers -             cfg.data.trg.normalize : False\n",
      "2022-08-16 07:22:52,900 - INFO - joeynmt.helpers -                 cfg.data.trg.level : word\n",
      "2022-08-16 07:22:52,900 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : data/iwslt15/vocab.vi\n",
      "2022-08-16 07:22:52,900 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none\n",
      "2022-08-16 07:22:52,900 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1\n",
      "2022-08-16 07:22:52,900 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\n",
      "2022-08-16 07:22:52,901 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0\n",
      "2022-08-16 07:22:52,901 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 1024\n",
      "2022-08-16 07:22:52,901 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token\n",
      "2022-08-16 07:22:52,901 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 150\n",
      "2022-08-16 07:22:52,901 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']\n",
      "2022-08-16 07:22:52,901 - INFO - joeynmt.helpers -            cfg.testing.return_prob : none\n",
      "2022-08-16 07:22:52,902 - INFO - joeynmt.helpers -       cfg.testing.return_attention : False\n",
      "2022-08-16 07:22:52,902 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a\n",
      "2022-08-16 07:22:52,902 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.lowercase : False\n",
      "2022-08-16 07:22:52,902 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\n",
      "2022-08-16 07:22:52,902 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\n",
      "2022-08-16 07:22:52,902 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\n",
      "2022-08-16 07:22:52,902 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.99]\n",
      "2022-08-16 07:22:52,903 - INFO - joeynmt.helpers -            cfg.training.scheduling : warmupinversesquareroot\n",
      "2022-08-16 07:22:52,903 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 4000\n",
      "2022-08-16 07:22:52,903 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\n",
      "2022-08-16 07:22:52,903 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0001\n",
      "2022-08-16 07:22:52,903 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 5e-06\n",
      "2022-08-16 07:22:52,903 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\n",
      "2022-08-16 07:22:52,903 - INFO - joeynmt.helpers -        cfg.training.clip_grad_norm : 1.0\n",
      "2022-08-16 07:22:52,903 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1\n",
      "2022-08-16 07:22:52,904 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 4\n",
      "2022-08-16 07:22:52,904 - INFO - joeynmt.helpers -            cfg.training.batch_size : 1024\n",
      "2022-08-16 07:22:52,904 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\n",
      "2022-08-16 07:22:52,904 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : bleu\n",
      "2022-08-16 07:22:52,904 - INFO - joeynmt.helpers -                cfg.training.epochs : 40\n",
      "2022-08-16 07:22:52,904 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000\n",
      "2022-08-16 07:22:52,904 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100\n",
      "2022-08-16 07:22:52,905 - INFO - joeynmt.helpers -             cfg.training.model_dir : iwslt15_envi\n",
      "2022-08-16 07:22:52,905 - INFO - joeynmt.helpers -             cfg.training.overwrite : False\n",
      "2022-08-16 07:22:52,905 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\n",
      "2022-08-16 07:22:52,905 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\n",
      "2022-08-16 07:22:52,905 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]\n",
      "2022-08-16 07:22:52,905 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 5\n",
      "2022-08-16 07:22:52,905 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform\n",
      "2022-08-16 07:22:52,905 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform\n",
      "2022-08-16 07:22:52,906 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\n",
      "2022-08-16 07:22:52,906 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\n",
      "2022-08-16 07:22:52,906 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\n",
      "2022-08-16 07:22:52,906 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False\n",
      "2022-08-16 07:22:52,906 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : False\n",
      "2022-08-16 07:22:52,906 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\n",
      "2022-08-16 07:22:52,906 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\n",
      "2022-08-16 07:22:52,906 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4\n",
      "2022-08-16 07:22:52,907 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2022-08-16 07:22:52,907 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2022-08-16 07:22:52,907 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0\n",
      "2022-08-16 07:22:52,907 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256\n",
      "2022-08-16 07:22:52,907 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024\n",
      "2022-08-16 07:22:52,907 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.1\n",
      "2022-08-16 07:22:52,907 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre\n",
      "2022-08-16 07:22:52,908 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\n",
      "2022-08-16 07:22:52,908 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\n",
      "2022-08-16 07:22:52,908 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4\n",
      "2022-08-16 07:22:52,908 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2022-08-16 07:22:52,908 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2022-08-16 07:22:52,908 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0\n",
      "2022-08-16 07:22:52,909 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256\n",
      "2022-08-16 07:22:52,909 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024\n",
      "2022-08-16 07:22:52,909 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.1\n",
      "2022-08-16 07:22:52,909 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre\n",
      "2022-08-16 07:22:52,916 - INFO - joeynmt.data - Building tokenizer...\n",
      "2022-08-16 07:22:52,916 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)\n",
      "2022-08-16 07:22:52,916 - INFO - joeynmt.tokenizers - vi tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)\n",
      "2022-08-16 07:22:52,916 - INFO - joeynmt.data - Loading train set...\n",
      "2022-08-16 07:22:52,976 - INFO - numexpr.utils - NumExpr defaulting to 2 threads.\n",
      "2022-08-16 07:22:53,820 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/iwslt15/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81 (last modified on Tue Aug 16 07:16:32 2022) since it couldn't be found locally at iwslt15., or remotely on the Hugging Face Hub.\n",
      "2022-08-16 07:22:53,871 - WARNING - datasets.builder - Reusing dataset iwslt15 (/content/drive/MyDrive/.cache/iwslt15/en-vi/1.0.0/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81)\n",
      "2022-08-16 07:22:53,888 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/.cache/iwslt15/en-vi/1.0.0/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81/cache-159c8769f6c27195.arrow\n",
      "2022-08-16 07:22:53,905 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/.cache/iwslt15/en-vi/1.0.0/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81/cache-7d3b0ed487d32463.arrow\n",
      "2022-08-16 07:22:53,950 - INFO - joeynmt.data - Building vocabulary...\n",
      "2022-08-16 07:22:56,698 - INFO - joeynmt.data - Loading dev set...\n",
      "2022-08-16 07:22:56,940 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/iwslt15/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81 (last modified on Tue Aug 16 07:16:32 2022) since it couldn't be found locally at iwslt15., or remotely on the Hugging Face Hub.\n",
      "2022-08-16 07:22:56,954 - WARNING - datasets.builder - Reusing dataset iwslt15 (/content/drive/MyDrive/.cache/iwslt15/en-vi/1.0.0/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81)\n",
      "2022-08-16 07:22:57,097 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/.cache/iwslt15/en-vi/1.0.0/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81/cache-0f0af0872c5eea42.arrow\n",
      "2022-08-16 07:22:57,232 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/.cache/iwslt15/en-vi/1.0.0/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81/cache-d382443956506d82.arrow\n",
      "2022-08-16 07:22:57,235 - INFO - joeynmt.data - Loading test set...\n",
      "2022-08-16 07:22:57,482 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/iwslt15/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81 (last modified on Tue Aug 16 07:16:32 2022) since it couldn't be found locally at iwslt15., or remotely on the Hugging Face Hub.\n",
      "2022-08-16 07:22:57,510 - WARNING - datasets.builder - Reusing dataset iwslt15 (/content/drive/MyDrive/.cache/iwslt15/en-vi/1.0.0/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81)\n",
      "2022-08-16 07:22:57,651 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/.cache/iwslt15/en-vi/1.0.0/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81/cache-7b843562a4cf7915.arrow\n",
      "2022-08-16 07:22:57,792 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/.cache/iwslt15/en-vi/1.0.0/c3b9d1bd246837934d62a45b3fbead26d7e863e9db5c20d8e54e6d865fad8b81/cache-093806e6aa7d730f.arrow\n",
      "2022-08-16 07:22:57,794 - INFO - joeynmt.data - Data loaded.\n",
      "2022-08-16 07:22:57,795 - INFO - joeynmt.helpers - Train dataset: HuggingfaceDataset(len=133166, src_lang=en, trg_lang=vi, has_trg=True, random_subset=-1, name=en-vi, split=train)\n",
      "2022-08-16 07:22:57,795 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1553, src_lang=en, trg_lang=vi, has_trg=True, random_subset=-1, name=en-vi, split=validation)\n",
      "2022-08-16 07:22:57,795 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1268, src_lang=en, trg_lang=vi, has_trg=True, random_subset=-1, name=en-vi, split=test)\n",
      "2022-08-16 07:22:57,796 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Rachel Pike : The science behind a climate headline\n",
      "\t[TRG] Khoa học đằng sau một tiêu đề về khí hậu\n",
      "2022-08-16 07:22:57,796 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) Rachel (5) : (6) The (7) science (8) behind (9) a\n",
      "2022-08-16 07:22:57,797 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) Khoa (5) học (6) đằng (7) sau (8) một (9) tiêu\n",
      "2022-08-16 07:22:57,797 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 17192\n",
      "2022-08-16 07:22:57,797 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 7710\n",
      "2022-08-16 07:22:57,810 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-16 07:22:58,191 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-16 07:23:00,359 - INFO - joeynmt.model - Total params: 19408896\n",
      "2022-08-16 07:23:00,361 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=17192),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=7710),\n",
      "\tloss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))\n",
      "2022-08-16 07:23:02,780 - INFO - joeynmt.builders - Adam(lr=0.0001, weight_decay=0.0, betas=[0.9, 0.99])\n",
      "2022-08-16 07:23:02,780 - INFO - joeynmt.builders - WarmupInverseSquareRootScheduler(warmup=4000, decay_rate=0.006325, peak_rate=0.0001, min_rate=5e-06)\n",
      "2022-08-16 07:23:02,781 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 4\n",
      "\tbatch size per device: 1024\n",
      "\teffective batch size (w. parallel & accumulation): 4096\n",
      "2022-08-16 07:23:02,781 - INFO - joeynmt.training - EPOCH 1\n",
      "2022-08-16 07:23:30,492 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     7.401969, Batch Acc: 0.036926, Tokens per Sec:     6104, Lr: 0.000005\n",
      "2022-08-16 07:23:58,452 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     7.250658, Batch Acc: 0.053370, Tokens per Sec:     6238, Lr: 0.000005\n",
      "2022-08-16 07:24:27,434 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     7.113125, Batch Acc: 0.057770, Tokens per Sec:     5956, Lr: 0.000008\n",
      "2022-08-16 07:24:54,154 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     6.819252, Batch Acc: 0.059348, Tokens per Sec:     6251, Lr: 0.000010\n",
      "2022-08-16 07:25:21,075 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     6.532805, Batch Acc: 0.066094, Tokens per Sec:     6344, Lr: 0.000013\n",
      "2022-08-16 07:25:48,262 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     6.220961, Batch Acc: 0.078174, Tokens per Sec:     6279, Lr: 0.000015\n",
      "2022-08-16 07:26:15,079 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     5.890118, Batch Acc: 0.087280, Tokens per Sec:     6405, Lr: 0.000018\n",
      "2022-08-16 07:26:42,996 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     5.772040, Batch Acc: 0.090939, Tokens per Sec:     6139, Lr: 0.000020\n",
      "2022-08-16 07:27:09,657 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     5.536598, Batch Acc: 0.095568, Tokens per Sec:     6330, Lr: 0.000023\n",
      "2022-08-16 07:27:36,439 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     5.559038, Batch Acc: 0.100035, Tokens per Sec:     6420, Lr: 0.000025\n",
      "2022-08-16 07:27:36,439 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 07:28:37,811 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 07:28:37,811 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 07:28:37,811 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 07:28:37,818 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 07:28:37,818 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   0.01, loss:   5.44, ppl: 231.30, acc:   0.10, generation: 61.1468[sec], evaluation: 0.2168[sec]\n",
      "2022-08-16 07:28:37,819 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 07:28:38,983 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 07:28:38,987 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 07:28:38,987 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 07:28:38,987 - INFO - joeynmt.training - \tHypothesis: Và , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , .\n",
      "2022-08-16 07:28:38,987 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 07:28:38,989 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 07:28:38,990 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 07:28:38,990 - INFO - joeynmt.training - \tHypothesis: Và , , , , .\n",
      "2022-08-16 07:28:38,990 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 07:28:38,992 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 07:28:38,992 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 07:28:38,992 - INFO - joeynmt.training - \tHypothesis: Và , , , , , , , , .\n",
      "2022-08-16 07:28:38,993 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 07:28:38,995 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 07:28:38,995 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 07:28:38,995 - INFO - joeynmt.training - \tHypothesis: Và , , , , , , .\n",
      "2022-08-16 07:28:38,995 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 07:28:38,997 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 07:28:38,998 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 07:28:38,998 - INFO - joeynmt.training - \tHypothesis: Và , , , , , , , , , , , .\n",
      "2022-08-16 07:29:07,650 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     5.390365, Batch Acc: 0.100928, Tokens per Sec:     5779, Lr: 0.000028\n",
      "2022-08-16 07:29:34,286 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     5.260734, Batch Acc: 0.109220, Tokens per Sec:     6464, Lr: 0.000030\n",
      "2022-08-16 07:30:00,623 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     5.218475, Batch Acc: 0.124585, Tokens per Sec:     6423, Lr: 0.000033\n",
      "2022-08-16 07:30:27,559 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     5.157898, Batch Acc: 0.130497, Tokens per Sec:     6380, Lr: 0.000035\n",
      "2022-08-16 07:30:54,888 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     5.063225, Batch Acc: 0.136866, Tokens per Sec:     6193, Lr: 0.000037\n",
      "2022-08-16 07:31:21,549 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     5.054890, Batch Acc: 0.144553, Tokens per Sec:     6446, Lr: 0.000040\n",
      "2022-08-16 07:31:48,607 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     4.994030, Batch Acc: 0.151604, Tokens per Sec:     6251, Lr: 0.000043\n",
      "2022-08-16 07:32:14,922 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     4.936798, Batch Acc: 0.154640, Tokens per Sec:     6381, Lr: 0.000045\n",
      "2022-08-16 07:32:41,326 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     4.852986, Batch Acc: 0.159401, Tokens per Sec:     6525, Lr: 0.000048\n",
      "2022-08-16 07:33:09,180 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     4.692124, Batch Acc: 0.162294, Tokens per Sec:     6225, Lr: 0.000050\n",
      "2022-08-16 07:33:09,181 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 07:34:12,150 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 07:34:12,151 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 07:34:12,151 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 07:34:12,158 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 07:34:12,158 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   0.82, loss:   4.73, ppl: 112.82, acc:   0.17, generation: 62.6769[sec], evaluation: 0.2841[sec]\n",
      "2022-08-16 07:34:12,159 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 07:34:13,502 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 07:34:13,506 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 07:34:13,506 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 07:34:13,507 - INFO - joeynmt.training - \tHypothesis: Và chúng ta có thể có thể có thể có thể có thể có thể có thể có thể một người , và những người , và những người , và những người , và những người có thể có thể có thể có thể có thể có thể có thể có thể có thể có thể có thể làm một người , và một người , và một người và những người và những người , và những người và những người và những người và những người , và những người , và một người có thể có thể có thể được một người và những người và những người .\n",
      "2022-08-16 07:34:13,507 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 07:34:13,509 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 07:34:13,509 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 07:34:13,509 - INFO - joeynmt.training - \tHypothesis: Và tôi là một người không có thể .\n",
      "2022-08-16 07:34:13,509 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 07:34:13,511 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 07:34:13,511 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 07:34:13,512 - INFO - joeynmt.training - \tHypothesis: Và một người là một người có thể có thể làm một người .\n",
      "2022-08-16 07:34:13,512 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 07:34:13,514 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 07:34:13,514 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 07:34:13,514 - INFO - joeynmt.training - \tHypothesis: Và tôi có thể có thể làm một người .\n",
      "2022-08-16 07:34:13,514 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 07:34:13,516 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 07:34:13,516 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 07:34:13,516 - INFO - joeynmt.training - \tHypothesis: Và một người , một người có thể có thể có thể có thể có thể có thể có thể .\n",
      "2022-08-16 07:34:17,617 - INFO - joeynmt.training - Epoch   1: total training loss 11719.12\n",
      "2022-08-16 07:34:17,618 - INFO - joeynmt.training - EPOCH 2\n",
      "2022-08-16 07:34:39,957 - INFO - joeynmt.training - Epoch   2, Step:     2100, Batch Loss:     4.670640, Batch Acc: 0.167968, Tokens per Sec:     6455, Lr: 0.000053\n",
      "2022-08-16 07:35:08,417 - INFO - joeynmt.training - Epoch   2, Step:     2200, Batch Loss:     4.715820, Batch Acc: 0.173837, Tokens per Sec:     6016, Lr: 0.000055\n",
      "2022-08-16 07:35:34,774 - INFO - joeynmt.training - Epoch   2, Step:     2300, Batch Loss:     4.483063, Batch Acc: 0.178570, Tokens per Sec:     6467, Lr: 0.000058\n",
      "2022-08-16 07:36:02,333 - INFO - joeynmt.training - Epoch   2, Step:     2400, Batch Loss:     4.584716, Batch Acc: 0.186178, Tokens per Sec:     6230, Lr: 0.000060\n",
      "2022-08-16 07:36:29,174 - INFO - joeynmt.training - Epoch   2, Step:     2500, Batch Loss:     4.589149, Batch Acc: 0.190202, Tokens per Sec:     6348, Lr: 0.000063\n",
      "2022-08-16 07:36:55,687 - INFO - joeynmt.training - Epoch   2, Step:     2600, Batch Loss:     4.479554, Batch Acc: 0.195010, Tokens per Sec:     6436, Lr: 0.000065\n",
      "2022-08-16 07:37:23,012 - INFO - joeynmt.training - Epoch   2, Step:     2700, Batch Loss:     4.384800, Batch Acc: 0.200909, Tokens per Sec:     6235, Lr: 0.000068\n",
      "2022-08-16 07:37:49,299 - INFO - joeynmt.training - Epoch   2, Step:     2800, Batch Loss:     4.303728, Batch Acc: 0.207210, Tokens per Sec:     6461, Lr: 0.000070\n",
      "2022-08-16 07:38:15,788 - INFO - joeynmt.training - Epoch   2, Step:     2900, Batch Loss:     4.213878, Batch Acc: 0.211815, Tokens per Sec:     6509, Lr: 0.000073\n",
      "2022-08-16 07:38:43,294 - INFO - joeynmt.training - Epoch   2, Step:     3000, Batch Loss:     4.350326, Batch Acc: 0.220484, Tokens per Sec:     6265, Lr: 0.000075\n",
      "2022-08-16 07:38:43,295 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 07:39:45,875 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 07:39:45,875 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 07:39:45,875 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 07:39:45,882 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 07:39:45,882 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   1.47, loss:   4.22, ppl:  68.03, acc:   0.23, generation: 62.2541[sec], evaluation: 0.3133[sec]\n",
      "2022-08-16 07:39:45,883 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 07:39:46,780 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 07:39:46,784 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 07:39:46,784 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 07:39:46,784 - INFO - joeynmt.training - \tHypothesis: Và tôi đã nói rằng <unk> <unk> <unk> <unk> , và tôi đã nói rằng <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
      "2022-08-16 07:39:46,784 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 07:39:46,786 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 07:39:46,786 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 07:39:46,788 - INFO - joeynmt.training - \tHypothesis: Và không phải là một người .\n",
      "2022-08-16 07:39:46,788 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 07:39:46,790 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 07:39:46,791 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 07:39:46,791 - INFO - joeynmt.training - \tHypothesis: Và một số trong một số trong một số trong một số .\n",
      "2022-08-16 07:39:46,791 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 07:39:46,793 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 07:39:46,793 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 07:39:46,793 - INFO - joeynmt.training - \tHypothesis: Tôi nghĩ rằng bạn thấy một cách mà tôi đã làm một cách .\n",
      "2022-08-16 07:39:46,793 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 07:39:46,796 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 07:39:46,796 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 07:39:46,796 - INFO - joeynmt.training - \tHypothesis: Và đây là một số <unk> <unk> <unk> <unk> <unk> .\n",
      "2022-08-16 07:40:13,495 - INFO - joeynmt.training - Epoch   2, Step:     3100, Batch Loss:     4.166135, Batch Acc: 0.223328, Tokens per Sec:     6199, Lr: 0.000077\n",
      "2022-08-16 07:40:39,908 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     4.251886, Batch Acc: 0.232146, Tokens per Sec:     6443, Lr: 0.000080\n",
      "2022-08-16 07:41:07,279 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     4.179643, Batch Acc: 0.237368, Tokens per Sec:     6288, Lr: 0.000082\n",
      "2022-08-16 07:41:35,798 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     4.003470, Batch Acc: 0.242872, Tokens per Sec:     5981, Lr: 0.000085\n",
      "2022-08-16 07:42:02,482 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     4.076006, Batch Acc: 0.251077, Tokens per Sec:     6411, Lr: 0.000088\n",
      "2022-08-16 07:42:29,530 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     4.141248, Batch Acc: 0.253683, Tokens per Sec:     6374, Lr: 0.000090\n",
      "2022-08-16 07:42:56,222 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     4.061621, Batch Acc: 0.262853, Tokens per Sec:     6446, Lr: 0.000092\n",
      "2022-08-16 07:43:24,561 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     3.984343, Batch Acc: 0.270032, Tokens per Sec:     5903, Lr: 0.000095\n",
      "2022-08-16 07:43:51,199 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     3.887771, Batch Acc: 0.274229, Tokens per Sec:     6431, Lr: 0.000097\n",
      "2022-08-16 07:44:18,286 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     3.827006, Batch Acc: 0.278702, Tokens per Sec:     6342, Lr: 0.000100\n",
      "2022-08-16 07:44:18,287 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 07:45:12,307 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 07:45:12,308 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 07:45:12,308 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 07:45:12,316 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 07:45:12,316 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   2.87, loss:   3.74, ppl:  42.26, acc:   0.29, generation: 53.6849[sec], evaluation: 0.3270[sec]\n",
      "2022-08-16 07:45:12,317 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 07:45:13,231 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 07:45:13,234 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 07:45:13,235 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 07:45:13,235 - INFO - joeynmt.training - \tHypothesis: Bạn có thể nói về những người trong những người trong những người trong những người này , trong những người ở đây , những người đã làm việc ở đây , những người ở đây , những người đã làm việc ở đây , và những người đã làm việc ở <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> , và những người đã làm việc ở <unk> <unk> <unk> <unk> <unk> <unk> <unk> , và những người đã được những người đã làm việc ở đây , và những người đã làm việc làm việc ở trong những người trong những người ở <unk> <unk> <unk> <unk> <unk> .\n",
      "2022-08-16 07:45:13,235 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 07:45:13,237 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 07:45:13,238 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 07:45:13,238 - INFO - joeynmt.training - \tHypothesis: Đây là một người không phải là một câu chuyện .\n",
      "2022-08-16 07:45:13,238 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 07:45:13,240 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 07:45:13,240 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 07:45:13,240 - INFO - joeynmt.training - \tHypothesis: Nó là một người có thể làm việc .\n",
      "2022-08-16 07:45:13,240 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 07:45:13,242 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 07:45:13,242 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 07:45:13,242 - INFO - joeynmt.training - \tHypothesis: Tôi muốn nói về một người khác .\n",
      "2022-08-16 07:45:13,243 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 07:45:13,245 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 07:45:13,245 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 07:45:13,245 - INFO - joeynmt.training - \tHypothesis: Một số đầu tiên của tôi là một người phụ nữ của người ta .\n",
      "2022-08-16 07:45:21,390 - INFO - joeynmt.training - Epoch   2: total training loss 8654.10\n",
      "2022-08-16 07:45:21,391 - INFO - joeynmt.training - EPOCH 3\n",
      "2022-08-16 07:45:42,754 - INFO - joeynmt.training - Epoch   3, Step:     4100, Batch Loss:     3.621330, Batch Acc: 0.288810, Tokens per Sec:     5554, Lr: 0.000099\n",
      "2022-08-16 07:46:09,452 - INFO - joeynmt.training - Epoch   3, Step:     4200, Batch Loss:     3.632933, Batch Acc: 0.291434, Tokens per Sec:     6455, Lr: 0.000098\n",
      "2022-08-16 07:46:36,078 - INFO - joeynmt.training - Epoch   3, Step:     4300, Batch Loss:     3.613806, Batch Acc: 0.297463, Tokens per Sec:     6344, Lr: 0.000096\n",
      "2022-08-16 07:47:02,524 - INFO - joeynmt.training - Epoch   3, Step:     4400, Batch Loss:     3.574900, Batch Acc: 0.299816, Tokens per Sec:     6537, Lr: 0.000095\n",
      "2022-08-16 07:47:28,860 - INFO - joeynmt.training - Epoch   3, Step:     4500, Batch Loss:     3.601477, Batch Acc: 0.306483, Tokens per Sec:     6444, Lr: 0.000094\n",
      "2022-08-16 07:47:57,212 - INFO - joeynmt.training - Epoch   3, Step:     4600, Batch Loss:     3.612250, Batch Acc: 0.308657, Tokens per Sec:     5982, Lr: 0.000093\n",
      "2022-08-16 07:48:23,776 - INFO - joeynmt.training - Epoch   3, Step:     4700, Batch Loss:     3.573543, Batch Acc: 0.313399, Tokens per Sec:     6420, Lr: 0.000092\n",
      "2022-08-16 07:48:50,042 - INFO - joeynmt.training - Epoch   3, Step:     4800, Batch Loss:     3.584611, Batch Acc: 0.316579, Tokens per Sec:     6529, Lr: 0.000091\n",
      "2022-08-16 07:49:16,495 - INFO - joeynmt.training - Epoch   3, Step:     4900, Batch Loss:     3.531519, Batch Acc: 0.317275, Tokens per Sec:     6387, Lr: 0.000090\n",
      "2022-08-16 07:49:43,733 - INFO - joeynmt.training - Epoch   3, Step:     5000, Batch Loss:     3.573827, Batch Acc: 0.324724, Tokens per Sec:     6238, Lr: 0.000089\n",
      "2022-08-16 07:49:43,733 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 07:50:37,182 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 07:50:37,182 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 07:50:37,182 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 07:50:37,190 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 07:50:37,190 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   5.64, loss:   3.42, ppl:  30.65, acc:   0.34, generation: 53.1144[sec], evaluation: 0.3239[sec]\n",
      "2022-08-16 07:50:37,191 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 07:50:38,361 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 07:50:38,364 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 07:50:38,365 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 07:50:38,365 - INFO - joeynmt.training - \tHypothesis: Làm sao tôi có thể nói về những năm trước khi các nhà khoa học về những người khác nhau , những người khác nhau trong những người khác nhau trong những người khác nhau , trong những người phụ nữ này , và tôi đã nói với những người đàn ông đã được viết về những người ở đây , và sau đó là một cuộc sống ở Mỹ , và sau đó là một ngày hôm nay , và sau đó là một người phụ nữ ở Mỹ , và sau khi tôi đã nói chuyện với những người phụ nữ trong những người Mỹ .\n",
      "2022-08-16 07:50:38,365 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 07:50:38,367 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 07:50:38,367 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 07:50:38,368 - INFO - joeynmt.training - \tHypothesis: Đây không phải là một câu chuyện về câu chuyện .\n",
      "2022-08-16 07:50:38,368 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 07:50:38,370 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 07:50:38,370 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 07:50:38,370 - INFO - joeynmt.training - \tHypothesis: Nó là một nhà khoa học đã được làm việc với nhau .\n",
      "2022-08-16 07:50:38,370 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 07:50:38,372 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 07:50:38,372 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 07:50:38,372 - INFO - joeynmt.training - \tHypothesis: Hãy nói với tôi về một vài số số .\n",
      "2022-08-16 07:50:38,372 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 07:50:38,374 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 07:50:38,375 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 07:50:38,375 - INFO - joeynmt.training - \tHypothesis: Bạn tưởng tượng đầu tiên : một người phụ nữ đã được gọi là người phụ nữ .\n",
      "2022-08-16 07:51:04,850 - INFO - joeynmt.training - Epoch   3, Step:     5100, Batch Loss:     3.504946, Batch Acc: 0.328667, Tokens per Sec:     6083, Lr: 0.000089\n",
      "2022-08-16 07:51:31,524 - INFO - joeynmt.training - Epoch   3, Step:     5200, Batch Loss:     3.578614, Batch Acc: 0.325881, Tokens per Sec:     6404, Lr: 0.000088\n",
      "2022-08-16 07:51:59,706 - INFO - joeynmt.training - Epoch   3, Step:     5300, Batch Loss:     3.522932, Batch Acc: 0.330923, Tokens per Sec:     6119, Lr: 0.000087\n",
      "2022-08-16 07:52:26,071 - INFO - joeynmt.training - Epoch   3, Step:     5400, Batch Loss:     3.438348, Batch Acc: 0.338968, Tokens per Sec:     6458, Lr: 0.000086\n",
      "2022-08-16 07:52:53,330 - INFO - joeynmt.training - Epoch   3, Step:     5500, Batch Loss:     3.391664, Batch Acc: 0.342038, Tokens per Sec:     6216, Lr: 0.000085\n",
      "2022-08-16 07:53:20,240 - INFO - joeynmt.training - Epoch   3, Step:     5600, Batch Loss:     3.220399, Batch Acc: 0.344323, Tokens per Sec:     6447, Lr: 0.000085\n",
      "2022-08-16 07:53:47,092 - INFO - joeynmt.training - Epoch   3, Step:     5700, Batch Loss:     3.296606, Batch Acc: 0.345475, Tokens per Sec:     6344, Lr: 0.000084\n",
      "2022-08-16 07:54:14,130 - INFO - joeynmt.training - Epoch   3, Step:     5800, Batch Loss:     3.318111, Batch Acc: 0.351957, Tokens per Sec:     6403, Lr: 0.000083\n",
      "2022-08-16 07:54:40,565 - INFO - joeynmt.training - Epoch   3, Step:     5900, Batch Loss:     3.301739, Batch Acc: 0.352430, Tokens per Sec:     6533, Lr: 0.000082\n",
      "2022-08-16 07:55:07,991 - INFO - joeynmt.training - Epoch   3, Step:     6000, Batch Loss:     3.316872, Batch Acc: 0.355878, Tokens per Sec:     6295, Lr: 0.000082\n",
      "2022-08-16 07:55:07,991 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 07:56:04,466 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 07:56:04,467 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 07:56:04,467 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 07:56:04,475 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 07:56:04,475 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.71, loss:   3.20, ppl:  24.50, acc:   0.37, generation: 55.9803[sec], evaluation: 0.4844[sec]\n",
      "2022-08-16 07:56:04,476 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 07:56:05,379 - INFO - joeynmt.helpers - delete iwslt15_envi/1000.ckpt\n",
      "2022-08-16 07:56:05,383 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 07:56:05,387 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 07:56:05,387 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 07:56:05,387 - INFO - joeynmt.training - \tHypothesis: Làm thế nào tôi có thể nói về khoảng 10 phút trong thế giới , trong thế giới , cách mà những thứ khác biệt của thế giới của những cuộc sống của mình trong cuộc sống của tôi đã bị bỏ qua những người đàn ông đã được viết vào năm trước khi tôi đã đi đến một ngày hôm nay , và cô bé gái tôi đã trở thành phố trong một ngày hôm nay , và cô bé gái trong những người đàn ông ấy đã bị bỏ qua một ngày hôm nay , và một ngày hôm nay , và một ngày hôm nay tôi đã nói chuyện với những người phụ nữ ở <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> , và cô ấy đã nói chuyện với những người đàn ông ấy đã đi qua những người đàn ông ấy đã bị bỏ\n",
      "2022-08-16 07:56:05,388 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 07:56:05,390 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 07:56:05,390 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 07:56:05,390 - INFO - joeynmt.training - \tHypothesis: Đây không phải là một câu chuyện .\n",
      "2022-08-16 07:56:05,390 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 07:56:05,392 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 07:56:05,393 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 07:56:05,393 - INFO - joeynmt.training - \tHypothesis: Nó là một chiếc xe đạp đã bắt đầu với nhau .\n",
      "2022-08-16 07:56:05,393 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 07:56:05,395 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 07:56:05,396 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 07:56:05,396 - INFO - joeynmt.training - \tHypothesis: Hãy cho tôi nói về một vài thứ .\n",
      "2022-08-16 07:56:05,396 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 07:56:05,398 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 07:56:05,398 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 07:56:05,398 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng đầu tiên là một người đàn ông của ông ấy .\n",
      "2022-08-16 07:56:17,717 - INFO - joeynmt.training - Epoch   3: total training loss 7085.68\n",
      "2022-08-16 07:56:17,717 - INFO - joeynmt.training - EPOCH 4\n",
      "2022-08-16 07:56:32,210 - INFO - joeynmt.training - Epoch   4, Step:     6100, Batch Loss:     3.334697, Batch Acc: 0.366113, Tokens per Sec:     6255, Lr: 0.000081\n",
      "2022-08-16 07:56:58,481 - INFO - joeynmt.training - Epoch   4, Step:     6200, Batch Loss:     3.067591, Batch Acc: 0.366191, Tokens per Sec:     6489, Lr: 0.000080\n",
      "2022-08-16 07:57:25,797 - INFO - joeynmt.training - Epoch   4, Step:     6300, Batch Loss:     3.113929, Batch Acc: 0.370324, Tokens per Sec:     6294, Lr: 0.000080\n",
      "2022-08-16 07:57:52,262 - INFO - joeynmt.training - Epoch   4, Step:     6400, Batch Loss:     3.181544, Batch Acc: 0.375035, Tokens per Sec:     6424, Lr: 0.000079\n",
      "2022-08-16 07:58:20,362 - INFO - joeynmt.training - Epoch   4, Step:     6500, Batch Loss:     3.146858, Batch Acc: 0.374047, Tokens per Sec:     5993, Lr: 0.000078\n",
      "2022-08-16 07:58:46,997 - INFO - joeynmt.training - Epoch   4, Step:     6600, Batch Loss:     3.134765, Batch Acc: 0.377733, Tokens per Sec:     6382, Lr: 0.000078\n",
      "2022-08-16 07:59:13,873 - INFO - joeynmt.training - Epoch   4, Step:     6700, Batch Loss:     3.000870, Batch Acc: 0.381495, Tokens per Sec:     6401, Lr: 0.000077\n",
      "2022-08-16 07:59:41,256 - INFO - joeynmt.training - Epoch   4, Step:     6800, Batch Loss:     3.116483, Batch Acc: 0.384715, Tokens per Sec:     6260, Lr: 0.000077\n",
      "2022-08-16 08:00:08,699 - INFO - joeynmt.training - Epoch   4, Step:     6900, Batch Loss:     2.945821, Batch Acc: 0.383644, Tokens per Sec:     6267, Lr: 0.000076\n",
      "2022-08-16 08:00:35,090 - INFO - joeynmt.training - Epoch   4, Step:     7000, Batch Loss:     3.181730, Batch Acc: 0.386382, Tokens per Sec:     6383, Lr: 0.000076\n",
      "2022-08-16 08:00:35,090 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 08:01:26,061 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 08:01:26,062 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 08:01:26,062 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 08:01:26,071 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 08:01:26,071 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.90, loss:   3.05, ppl:  21.05, acc:   0.39, generation: 50.6384[sec], evaluation: 0.3233[sec]\n",
      "2022-08-16 08:01:26,072 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 08:01:26,950 - INFO - joeynmt.helpers - delete iwslt15_envi/2000.ckpt\n",
      "2022-08-16 08:01:26,954 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 08:01:26,957 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 08:01:26,958 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 08:01:26,958 - INFO - joeynmt.training - \tHypothesis: Làm thế nào tôi có thể nói về khoảng 10 phút trong thế giới , cách mà thế giới hạn , về sự kiện lớn nhất của thế giới , những người phụ nữ lớn hơn là những người phụ nữ bị bỏ qua đời của cô bé gái của cô ấy , và trẻ em gái của cô ấy , và một ngày nay , một ngày trước khi tôi đã từng ngày nữa , và một ngày , một ngày càng nhiều người đàn ông đã từng ngày càng nhiều hơn một ngày nữa .\n",
      "2022-08-16 08:01:26,958 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 08:01:26,960 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 08:01:26,960 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 08:01:26,960 - INFO - joeynmt.training - \tHypothesis: Đây không phải là một câu chuyện .\n",
      "2022-08-16 08:01:26,961 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 08:01:26,962 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 08:01:26,963 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 08:01:26,963 - INFO - joeynmt.training - \tHypothesis: Nó là một nhà khoa học đã cố gắng làm việc với nhau .\n",
      "2022-08-16 08:01:26,963 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 08:01:26,965 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 08:01:26,965 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 08:01:26,965 - INFO - joeynmt.training - \tHypothesis: Hãy cho tôi nói về một số lượng .\n",
      "2022-08-16 08:01:26,965 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 08:01:26,967 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 08:01:26,968 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 08:01:26,968 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng đầu tiên : một người đàn ông đã bị bỏ sống của mình .\n",
      "2022-08-16 08:01:53,298 - INFO - joeynmt.training - Epoch   4, Step:     7100, Batch Loss:     2.913961, Batch Acc: 0.386464, Tokens per Sec:     6251, Lr: 0.000075\n",
      "2022-08-16 08:02:22,594 - INFO - joeynmt.training - Epoch   4, Step:     7200, Batch Loss:     3.281651, Batch Acc: 0.388586, Tokens per Sec:     5934, Lr: 0.000075\n",
      "2022-08-16 08:02:48,940 - INFO - joeynmt.training - Epoch   4, Step:     7300, Batch Loss:     2.834907, Batch Acc: 0.393912, Tokens per Sec:     6416, Lr: 0.000074\n",
      "2022-08-16 08:03:15,370 - INFO - joeynmt.training - Epoch   4, Step:     7400, Batch Loss:     3.078848, Batch Acc: 0.395848, Tokens per Sec:     6406, Lr: 0.000074\n",
      "2022-08-16 08:03:41,938 - INFO - joeynmt.training - Epoch   4, Step:     7500, Batch Loss:     2.807648, Batch Acc: 0.395488, Tokens per Sec:     6466, Lr: 0.000073\n",
      "2022-08-16 08:04:08,471 - INFO - joeynmt.training - Epoch   4, Step:     7600, Batch Loss:     2.799983, Batch Acc: 0.403682, Tokens per Sec:     6503, Lr: 0.000073\n",
      "2022-08-16 08:04:36,898 - INFO - joeynmt.training - Epoch   4, Step:     7700, Batch Loss:     2.880127, Batch Acc: 0.398380, Tokens per Sec:     6033, Lr: 0.000072\n",
      "2022-08-16 08:05:03,615 - INFO - joeynmt.training - Epoch   4, Step:     7800, Batch Loss:     2.930422, Batch Acc: 0.400204, Tokens per Sec:     6407, Lr: 0.000072\n",
      "2022-08-16 08:05:30,100 - INFO - joeynmt.training - Epoch   4, Step:     7900, Batch Loss:     3.090561, Batch Acc: 0.404458, Tokens per Sec:     6418, Lr: 0.000071\n",
      "2022-08-16 08:05:56,676 - INFO - joeynmt.training - Epoch   4, Step:     8000, Batch Loss:     3.049644, Batch Acc: 0.403549, Tokens per Sec:     6487, Lr: 0.000071\n",
      "2022-08-16 08:05:56,677 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 08:06:50,293 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 08:06:50,293 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 08:06:50,293 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 08:06:50,301 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 08:06:50,301 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  11.61, loss:   2.91, ppl:  18.37, acc:   0.42, generation: 53.2814[sec], evaluation: 0.3263[sec]\n",
      "2022-08-16 08:06:50,302 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 08:06:51,213 - INFO - joeynmt.helpers - delete iwslt15_envi/3000.ckpt\n",
      "2022-08-16 08:06:51,629 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 08:06:51,631 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 08:06:51,632 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 08:06:51,632 - INFO - joeynmt.training - \tHypothesis: Làm sao tôi có thể nói về 10 phút về những người phụ nữ trong thế giới , về cách mà thế hệ của những người khác biệt của những người đang được sử dụng được những người đàn ông của cô bé gái của cô ấy , cô gái mẹ tôi , cô ấy đã đi đến những người phụ nữ trong những đứa trẻ , và những người phụ nữ ở trong những năm trước khi họ đã được hàng ngày nay , những người đàn ông đã chết trong vòng quanh một ngày nay , những người phụ nữ , và những người đàn ông đã chết ở đây , trong những người phụ nữ ở đây , và những người phụ nữ ở đây , những người đàn ông đã chết vào năm qua những người đàn ông ấy đã từng chết ở những người phụ\n",
      "2022-08-16 08:06:51,632 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 08:06:51,634 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 08:06:51,634 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 08:06:51,634 - INFO - joeynmt.training - \tHypothesis: Đây không phải là một câu chuyện .\n",
      "2022-08-16 08:06:51,635 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 08:06:51,637 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 08:06:51,637 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 08:06:51,637 - INFO - joeynmt.training - \tHypothesis: Nó là một câu chuyện thú vị đã bắt đầu làm việc với nhau .\n",
      "2022-08-16 08:06:51,637 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 08:06:51,639 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 08:06:51,639 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 08:06:51,640 - INFO - joeynmt.training - \tHypothesis: Hãy để tôi nói về một vài hình .\n",
      "2022-08-16 08:06:51,640 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 08:06:51,642 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 08:06:51,642 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 08:06:51,642 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng đầu tiên của một người đàn ông : một người đàn ông đang làm việc .\n",
      "2022-08-16 08:07:08,461 - INFO - joeynmt.training - Epoch   4: total training loss 6202.42\n",
      "2022-08-16 08:07:08,461 - INFO - joeynmt.training - EPOCH 5\n",
      "2022-08-16 08:07:18,183 - INFO - joeynmt.training - Epoch   5, Step:     8100, Batch Loss:     2.711547, Batch Acc: 0.414825, Tokens per Sec:     6465, Lr: 0.000070\n",
      "2022-08-16 08:07:44,652 - INFO - joeynmt.training - Epoch   5, Step:     8200, Batch Loss:     2.956510, Batch Acc: 0.418748, Tokens per Sec:     6456, Lr: 0.000070\n",
      "2022-08-16 08:08:11,032 - INFO - joeynmt.training - Epoch   5, Step:     8300, Batch Loss:     2.963065, Batch Acc: 0.415251, Tokens per Sec:     6439, Lr: 0.000069\n",
      "2022-08-16 08:08:39,794 - INFO - joeynmt.training - Epoch   5, Step:     8400, Batch Loss:     2.826695, Batch Acc: 0.421070, Tokens per Sec:     5978, Lr: 0.000069\n",
      "2022-08-16 08:09:07,757 - INFO - joeynmt.training - Epoch   5, Step:     8500, Batch Loss:     3.052926, Batch Acc: 0.422487, Tokens per Sec:     6242, Lr: 0.000069\n",
      "2022-08-16 08:09:34,177 - INFO - joeynmt.training - Epoch   5, Step:     8600, Batch Loss:     2.892885, Batch Acc: 0.420881, Tokens per Sec:     6430, Lr: 0.000068\n",
      "2022-08-16 08:10:00,498 - INFO - joeynmt.training - Epoch   5, Step:     8700, Batch Loss:     2.650974, Batch Acc: 0.423393, Tokens per Sec:     6479, Lr: 0.000068\n",
      "2022-08-16 08:10:26,776 - INFO - joeynmt.training - Epoch   5, Step:     8800, Batch Loss:     2.931453, Batch Acc: 0.430714, Tokens per Sec:     6601, Lr: 0.000067\n",
      "2022-08-16 08:10:53,981 - INFO - joeynmt.training - Epoch   5, Step:     8900, Batch Loss:     2.970531, Batch Acc: 0.424667, Tokens per Sec:     6315, Lr: 0.000067\n",
      "2022-08-16 08:11:21,427 - INFO - joeynmt.training - Epoch   5, Step:     9000, Batch Loss:     2.891180, Batch Acc: 0.430398, Tokens per Sec:     6213, Lr: 0.000067\n",
      "2022-08-16 08:11:21,427 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 08:12:14,421 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 08:12:14,422 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 08:12:14,422 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 08:12:14,430 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 08:12:14,430 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  13.37, loss:   2.82, ppl:  16.72, acc:   0.43, generation: 52.6551[sec], evaluation: 0.3279[sec]\n",
      "2022-08-16 08:12:14,431 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 08:12:15,390 - INFO - joeynmt.helpers - delete iwslt15_envi/4000.ckpt\n",
      "2022-08-16 08:12:15,394 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 08:12:15,398 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 08:12:15,399 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 08:12:15,399 - INFO - joeynmt.training - \tHypothesis: Làm thế nào tôi có thể nói về 10 phút về những người phụ nữ trên thế giới , về những thế hệ kinh ngạc , về sự kinh ngạc của những người bị đánh giá lớn của cuộc sống của cô bé gái cô bé gái , mẹ tôi , cô gái cô gái cô gái của cô bé , và phụ nữ , và phụ nữ đã chết trong những người trong vòng quanh một ngày , và những người phụ nữ ở những người phụ nữ , và những người phụ nữ ở đây đã chết ở đây , và thậm chí ở <unk> đã chết ở đây , ở những người phụ nữ ở đó , và thậm chí ở những người phụ nữ , những người đàn ông ta đã chết vào năm trước đó ,\n",
      "2022-08-16 08:12:15,399 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 08:12:15,401 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 08:12:15,401 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 08:12:15,401 - INFO - joeynmt.training - \tHypothesis: Đây không phải là một câu chuyện .\n",
      "2022-08-16 08:12:15,401 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 08:12:15,404 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 08:12:15,406 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 08:12:15,406 - INFO - joeynmt.training - \tHypothesis: Đó là một câu hỏi <unk> đã vẫn đang làm việc với nhau .\n",
      "2022-08-16 08:12:15,407 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 08:12:15,409 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 08:12:15,409 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 08:12:15,409 - INFO - joeynmt.training - \tHypothesis: Hãy để tôi nói với các bạn về một số hình ảnh .\n",
      "2022-08-16 08:12:15,409 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 08:12:15,412 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 08:12:15,412 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 08:12:15,412 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng đầu tiên : một người đàn ông đang bị bỏ sống của ông .\n",
      "2022-08-16 08:12:43,829 - INFO - joeynmt.training - Epoch   5, Step:     9100, Batch Loss:     2.845354, Batch Acc: 0.429428, Tokens per Sec:     5716, Lr: 0.000066\n",
      "2022-08-16 08:13:10,154 - INFO - joeynmt.training - Epoch   5, Step:     9200, Batch Loss:     2.893974, Batch Acc: 0.434385, Tokens per Sec:     6511, Lr: 0.000066\n",
      "2022-08-16 08:13:36,527 - INFO - joeynmt.training - Epoch   5, Step:     9300, Batch Loss:     2.885087, Batch Acc: 0.432872, Tokens per Sec:     6550, Lr: 0.000066\n",
      "2022-08-16 08:14:04,452 - INFO - joeynmt.training - Epoch   5, Step:     9400, Batch Loss:     2.913812, Batch Acc: 0.435925, Tokens per Sec:     6120, Lr: 0.000065\n",
      "2022-08-16 08:14:31,283 - INFO - joeynmt.training - Epoch   5, Step:     9500, Batch Loss:     2.840907, Batch Acc: 0.434392, Tokens per Sec:     6365, Lr: 0.000065\n",
      "2022-08-16 08:14:58,676 - INFO - joeynmt.training - Epoch   5, Step:     9600, Batch Loss:     2.663704, Batch Acc: 0.439295, Tokens per Sec:     6297, Lr: 0.000065\n",
      "2022-08-16 08:15:25,092 - INFO - joeynmt.training - Epoch   5, Step:     9700, Batch Loss:     2.828726, Batch Acc: 0.438016, Tokens per Sec:     6501, Lr: 0.000064\n",
      "2022-08-16 08:15:51,647 - INFO - joeynmt.training - Epoch   5, Step:     9800, Batch Loss:     2.818538, Batch Acc: 0.438060, Tokens per Sec:     6472, Lr: 0.000064\n",
      "2022-08-16 08:16:19,114 - INFO - joeynmt.training - Epoch   5, Step:     9900, Batch Loss:     2.501620, Batch Acc: 0.439230, Tokens per Sec:     6213, Lr: 0.000064\n",
      "2022-08-16 08:16:46,549 - INFO - joeynmt.training - Epoch   5, Step:    10000, Batch Loss:     2.608002, Batch Acc: 0.442275, Tokens per Sec:     6189, Lr: 0.000063\n",
      "2022-08-16 08:16:46,549 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 08:17:35,508 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 08:17:35,508 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 08:17:35,508 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 08:17:35,516 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 08:17:35,516 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  14.29, loss:   2.74, ppl:  15.44, acc:   0.45, generation: 48.6299[sec], evaluation: 0.3199[sec]\n",
      "2022-08-16 08:17:35,517 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 08:17:36,477 - INFO - joeynmt.helpers - delete iwslt15_envi/5000.ckpt\n",
      "2022-08-16 08:17:36,480 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 08:17:36,483 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 08:17:36,484 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 08:17:36,484 - INFO - joeynmt.training - \tHypothesis: Làm thế nào tôi có thể nói về 10 phút về những người phụ nữ trên thế giới , về sự kinh ngạc của những người bị tổn thương hiệu của cuộc sống với con gái trẻ em gái trẻ em , mẹ tôi , mẹ cô gái trẻ em gái của cô bé và ngày hôm qua một ngày sau đó , cô gái tôi đã chết ở một ngày trước khi còn lại ở đây , những năm qua đời sống ở đây , và thậm chí chưa bao giờ đã chết người phụ nữ ở đây đã từng ngày nay đã chết ở trong suốt cả những người ở đây .\n",
      "2022-08-16 08:17:36,484 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 08:17:36,486 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 08:17:36,486 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 08:17:36,486 - INFO - joeynmt.training - \tHypothesis: Đây không phải là câu chuyện .\n",
      "2022-08-16 08:17:36,486 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 08:17:36,489 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 08:17:36,489 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 08:17:36,489 - INFO - joeynmt.training - \tHypothesis: Đó là một câu hỏi đơn giản là đã cố gắng kết nối nhau .\n",
      "2022-08-16 08:17:36,489 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 08:17:36,491 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 08:17:36,491 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 08:17:36,491 - INFO - joeynmt.training - \tHypothesis: Hãy để tôi nói với các bạn về một số loại .\n",
      "2022-08-16 08:17:36,491 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 08:17:36,493 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 08:17:36,494 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 08:17:36,494 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng đầu tiên : một người đàn ông đang bị bỏ qua cuộc sống của ông .\n",
      "2022-08-16 08:17:56,462 - INFO - joeynmt.training - Epoch   5: total training loss 5635.98\n",
      "2022-08-16 08:17:56,463 - INFO - joeynmt.training - EPOCH 6\n",
      "2022-08-16 08:18:03,345 - INFO - joeynmt.training - Epoch   6, Step:    10100, Batch Loss:     2.637223, Batch Acc: 0.455054, Tokens per Sec:     6387, Lr: 0.000063\n",
      "2022-08-16 08:18:30,893 - INFO - joeynmt.training - Epoch   6, Step:    10200, Batch Loss:     2.597104, Batch Acc: 0.449681, Tokens per Sec:     6263, Lr: 0.000063\n",
      "2022-08-16 08:18:59,539 - INFO - joeynmt.training - Epoch   6, Step:    10300, Batch Loss:     2.894526, Batch Acc: 0.452855, Tokens per Sec:     6014, Lr: 0.000062\n",
      "2022-08-16 08:19:26,229 - INFO - joeynmt.training - Epoch   6, Step:    10400, Batch Loss:     2.695140, Batch Acc: 0.453557, Tokens per Sec:     6402, Lr: 0.000062\n",
      "2022-08-16 08:19:52,432 - INFO - joeynmt.training - Epoch   6, Step:    10500, Batch Loss:     2.719758, Batch Acc: 0.456265, Tokens per Sec:     6513, Lr: 0.000062\n",
      "2022-08-16 08:20:18,729 - INFO - joeynmt.training - Epoch   6, Step:    10600, Batch Loss:     2.446258, Batch Acc: 0.456682, Tokens per Sec:     6525, Lr: 0.000061\n",
      "2022-08-16 08:20:45,284 - INFO - joeynmt.training - Epoch   6, Step:    10700, Batch Loss:     2.615066, Batch Acc: 0.458122, Tokens per Sec:     6590, Lr: 0.000061\n",
      "2022-08-16 08:21:13,496 - INFO - joeynmt.training - Epoch   6, Step:    10800, Batch Loss:     2.752223, Batch Acc: 0.455899, Tokens per Sec:     6062, Lr: 0.000061\n",
      "2022-08-16 08:21:39,813 - INFO - joeynmt.training - Epoch   6, Step:    10900, Batch Loss:     2.505851, Batch Acc: 0.461445, Tokens per Sec:     6437, Lr: 0.000061\n",
      "2022-08-16 08:22:06,385 - INFO - joeynmt.training - Epoch   6, Step:    11000, Batch Loss:     2.694567, Batch Acc: 0.457190, Tokens per Sec:     6376, Lr: 0.000060\n",
      "2022-08-16 08:22:06,385 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 08:22:56,370 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 08:22:56,370 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 08:22:56,371 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 08:22:56,379 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 08:22:56,379 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  15.70, loss:   2.67, ppl:  14.45, acc:   0.46, generation: 49.6440[sec], evaluation: 0.3306[sec]\n",
      "2022-08-16 08:22:56,380 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 08:22:57,341 - INFO - joeynmt.helpers - delete iwslt15_envi/6000.ckpt\n",
      "2022-08-16 08:22:57,345 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 08:22:57,350 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 08:22:57,350 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 08:22:57,350 - INFO - joeynmt.training - \tHypothesis: Làm thế nào tôi có thể nói về 10 phút về những người phụ nữ trên 3 thế hệ , về sự kiện kinh ngạc của những người bị kết nối với sự hạnh phúc của cuộc sống của con gái trẻ em với con gái , cô bé gái cô ấy , cô bé gái của cô ấy , và cô bé gái trong vòng quanh một ngày dài năm qua một ngày nào đó , và thậm chí còn lại bao giờ nhìn thấy ở đây , và những người phụ nữ lớn hơn bao giờ qua những người phụ nữ ở đây đã chết ở đây , và ở đây , và thậm chí chưa bao giờ đến trước khi còn lại ở đây , và ở trong vòng quanh tôi đã đi đến trong vòng quanh một con gái còn lại ở trong vòng quanh nó\n",
      "2022-08-16 08:22:57,350 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 08:22:57,353 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 08:22:57,353 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 08:22:57,353 - INFO - joeynmt.training - \tHypothesis: Đây không phải là một câu chuyện hoàn toàn .\n",
      "2022-08-16 08:22:57,353 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 08:22:57,355 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 08:22:57,356 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 08:22:57,356 - INFO - joeynmt.training - \tHypothesis: Nó là một câu hỏi đơn giản mà vẫn đang đặt lại .\n",
      "2022-08-16 08:22:57,356 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 08:22:57,359 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 08:22:57,360 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 08:22:57,360 - INFO - joeynmt.training - \tHypothesis: Hãy để tôi nói với các bạn về một số thứ .\n",
      "2022-08-16 08:22:57,360 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 08:22:57,362 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 08:22:57,362 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 08:22:57,362 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng đầu tiên của anh ta : một người đàn ông đang chạy đua cuộc sống của ông .\n",
      "2022-08-16 08:23:26,070 - INFO - joeynmt.training - Epoch   6, Step:    11100, Batch Loss:     2.550264, Batch Acc: 0.460075, Tokens per Sec:     5789, Lr: 0.000060\n",
      "2022-08-16 08:23:52,424 - INFO - joeynmt.training - Epoch   6, Step:    11200, Batch Loss:     2.478442, Batch Acc: 0.465022, Tokens per Sec:     6530, Lr: 0.000060\n",
      "2022-08-16 08:24:19,030 - INFO - joeynmt.training - Epoch   6, Step:    11300, Batch Loss:     2.654824, Batch Acc: 0.461562, Tokens per Sec:     6396, Lr: 0.000059\n",
      "2022-08-16 08:24:45,488 - INFO - joeynmt.training - Epoch   6, Step:    11400, Batch Loss:     2.703890, Batch Acc: 0.464527, Tokens per Sec:     6550, Lr: 0.000059\n",
      "2022-08-16 08:25:13,738 - INFO - joeynmt.training - Epoch   6, Step:    11500, Batch Loss:     2.499737, Batch Acc: 0.462887, Tokens per Sec:     5953, Lr: 0.000059\n",
      "2022-08-16 08:25:41,264 - INFO - joeynmt.training - Epoch   6, Step:    11600, Batch Loss:     2.574948, Batch Acc: 0.467022, Tokens per Sec:     6242, Lr: 0.000059\n",
      "2022-08-16 08:26:07,526 - INFO - joeynmt.training - Epoch   6, Step:    11700, Batch Loss:     2.464832, Batch Acc: 0.463564, Tokens per Sec:     6491, Lr: 0.000058\n",
      "2022-08-16 08:26:34,046 - INFO - joeynmt.training - Epoch   6, Step:    11800, Batch Loss:     2.441717, Batch Acc: 0.465251, Tokens per Sec:     6440, Lr: 0.000058\n",
      "2022-08-16 08:27:00,863 - INFO - joeynmt.training - Epoch   6, Step:    11900, Batch Loss:     2.901241, Batch Acc: 0.470488, Tokens per Sec:     6374, Lr: 0.000058\n",
      "2022-08-16 08:27:28,514 - INFO - joeynmt.training - Epoch   6, Step:    12000, Batch Loss:     2.436042, Batch Acc: 0.465836, Tokens per Sec:     6307, Lr: 0.000058\n",
      "2022-08-16 08:27:28,515 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 08:28:22,415 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 08:28:22,415 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 08:28:22,416 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 08:28:22,424 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 08:28:22,424 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  16.56, loss:   2.61, ppl:  13.54, acc:   0.47, generation: 53.5649[sec], evaluation: 0.3272[sec]\n",
      "2022-08-16 08:28:22,425 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 08:28:23,416 - INFO - joeynmt.helpers - delete iwslt15_envi/7000.ckpt\n",
      "2022-08-16 08:28:23,435 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 08:28:23,439 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 08:28:23,440 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 08:28:23,440 - INFO - joeynmt.training - \tHypothesis: Làm thế nào tôi có thể nói về 10 phút về những người phụ nữ trên 3 thế hệ , về sự kinh ngạc của những người bị tổn thương mạnh mẽ của cuộc sống trong cuộc đời của một cô gái trẻ em gái trẻ em gái trẻ em gái , cô gái cô gái của cô gái , cô gái cô gái và mẹ cô gái , và mẹ cô gái của cô gái trẻ em gái trong vòng quanh một ngày càng nhỏ hơn , và những ngày càng nhiều năm qua đời sống ở đây , và không bao giờ hết sức khoẻ mạnh hơn nữa , và tôi đã đi vào năm trước khi nào đó , và thậm chí còn lại ở đây , và không gian dài hơn nữa .\n",
      "2022-08-16 08:28:23,440 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 08:28:23,442 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 08:28:23,442 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 08:28:23,442 - INFO - joeynmt.training - \tHypothesis: Đây không phải là một câu chuyện hoàn toàn .\n",
      "2022-08-16 08:28:23,443 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 08:28:23,444 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 08:28:23,445 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 08:28:23,445 - INFO - joeynmt.training - \tHypothesis: Đó là một câu đố hoàn toàn bị kết thúc .\n",
      "2022-08-16 08:28:23,445 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 08:28:23,447 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 08:28:23,447 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 08:28:23,448 - INFO - joeynmt.training - \tHypothesis: Để tôi kể cho các bạn về một số mẫu .\n",
      "2022-08-16 08:28:23,448 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 08:28:23,450 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 08:28:23,450 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 08:28:23,450 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng đầu tiên : một người đàn ông đang chạy đua cuộc đời của ông .\n",
      "2022-08-16 08:28:45,482 - INFO - joeynmt.training - Epoch   6: total training loss 5250.99\n",
      "2022-08-16 08:28:45,482 - INFO - joeynmt.training - EPOCH 7\n",
      "2022-08-16 08:28:49,960 - INFO - joeynmt.training - Epoch   7, Step:    12100, Batch Loss:     2.516472, Batch Acc: 0.477766, Tokens per Sec:     6454, Lr: 0.000057\n",
      "2022-08-16 08:29:18,148 - INFO - joeynmt.training - Epoch   7, Step:    12200, Batch Loss:     2.753255, Batch Acc: 0.477331, Tokens per Sec:     6084, Lr: 0.000057\n",
      "2022-08-16 08:29:45,198 - INFO - joeynmt.training - Epoch   7, Step:    12300, Batch Loss:     2.433648, Batch Acc: 0.478504, Tokens per Sec:     6345, Lr: 0.000057\n",
      "2022-08-16 08:30:11,378 - INFO - joeynmt.training - Epoch   7, Step:    12400, Batch Loss:     2.378775, Batch Acc: 0.482500, Tokens per Sec:     6532, Lr: 0.000057\n",
      "2022-08-16 08:30:38,750 - INFO - joeynmt.training - Epoch   7, Step:    12500, Batch Loss:     2.448792, Batch Acc: 0.479565, Tokens per Sec:     6336, Lr: 0.000057\n",
      "2022-08-16 08:31:05,832 - INFO - joeynmt.training - Epoch   7, Step:    12600, Batch Loss:     2.399942, Batch Acc: 0.478646, Tokens per Sec:     6381, Lr: 0.000056\n",
      "2022-08-16 08:31:33,362 - INFO - joeynmt.training - Epoch   7, Step:    12700, Batch Loss:     2.553570, Batch Acc: 0.482112, Tokens per Sec:     6169, Lr: 0.000056\n",
      "2022-08-16 08:32:00,378 - INFO - joeynmt.training - Epoch   7, Step:    12800, Batch Loss:     2.443489, Batch Acc: 0.480987, Tokens per Sec:     6369, Lr: 0.000056\n",
      "2022-08-16 08:32:26,769 - INFO - joeynmt.training - Epoch   7, Step:    12900, Batch Loss:     2.534639, Batch Acc: 0.476831, Tokens per Sec:     6418, Lr: 0.000056\n",
      "2022-08-16 08:32:54,069 - INFO - joeynmt.training - Epoch   7, Step:    13000, Batch Loss:     2.449757, Batch Acc: 0.483881, Tokens per Sec:     6243, Lr: 0.000055\n",
      "2022-08-16 08:32:54,070 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 08:33:45,986 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 08:33:45,986 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 08:33:45,987 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 08:33:45,995 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 08:33:45,995 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  16.90, loss:   2.58, ppl:  13.19, acc:   0.48, generation: 51.3864[sec], evaluation: 0.5186[sec]\n",
      "2022-08-16 08:33:45,995 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 08:33:46,912 - INFO - joeynmt.helpers - delete iwslt15_envi/8000.ckpt\n",
      "2022-08-16 08:33:46,915 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 08:33:46,919 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 08:33:46,919 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 08:33:46,919 - INFO - joeynmt.training - \tHypothesis: Làm thế nào tôi nói về 10 phút về những người phụ nữ trên 3 thế hệ kinh ngạc , về sự kinh ngạc của những người bị đánh giá của cuộc sống trong cuộc sống của cô bé gái trẻ em với con gái trẻ em , mẹ cô gái của cô ấy , cô gái cô ấy , và mẹ tôi đã đi vào một đêm nhỏ hơn 9 năm trước khi những con gái nhỏ bé đã được giới thiệu lại một con gái ở đây , và một ngày nào đó sẽ không bao giờ được giới thiệu lại không bao giờ để làm cho đến những con gái ở đây .\n",
      "2022-08-16 08:33:46,919 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 08:33:46,921 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 08:33:46,922 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 08:33:46,922 - INFO - joeynmt.training - \tHypothesis: Đây không phải là câu chuyện hoàn toàn .\n",
      "2022-08-16 08:33:46,922 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 08:33:46,924 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 08:33:46,924 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 08:33:46,924 - INFO - joeynmt.training - \tHypothesis: Đó là một câu chuyện ngắn mà vẫn đang được đặt lại với nhau .\n",
      "2022-08-16 08:33:46,924 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 08:33:46,926 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 08:33:46,927 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 08:33:46,927 - INFO - joeynmt.training - \tHypothesis: Để tôi nói với các bạn về một số mẫu .\n",
      "2022-08-16 08:33:46,927 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 08:33:46,929 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 08:33:46,930 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 08:33:46,930 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng đầu tiên : một người đàn ông đang bị đánh giá cuộc sống của ông .\n",
      "2022-08-16 08:34:13,327 - INFO - joeynmt.training - Epoch   7, Step:    13100, Batch Loss:     2.409903, Batch Acc: 0.482619, Tokens per Sec:     6172, Lr: 0.000055\n",
      "2022-08-16 08:34:39,844 - INFO - joeynmt.training - Epoch   7, Step:    13200, Batch Loss:     2.510992, Batch Acc: 0.484316, Tokens per Sec:     6406, Lr: 0.000055\n",
      "2022-08-16 08:35:07,519 - INFO - joeynmt.training - Epoch   7, Step:    13300, Batch Loss:     2.471586, Batch Acc: 0.481555, Tokens per Sec:     6202, Lr: 0.000055\n",
      "2022-08-16 08:35:35,594 - INFO - joeynmt.training - Epoch   7, Step:    13400, Batch Loss:     2.262790, Batch Acc: 0.481954, Tokens per Sec:     6086, Lr: 0.000055\n",
      "2022-08-16 08:36:01,853 - INFO - joeynmt.training - Epoch   7, Step:    13500, Batch Loss:     2.418172, Batch Acc: 0.483020, Tokens per Sec:     6468, Lr: 0.000054\n",
      "2022-08-16 08:36:28,179 - INFO - joeynmt.training - Epoch   7, Step:    13600, Batch Loss:     2.318658, Batch Acc: 0.486176, Tokens per Sec:     6589, Lr: 0.000054\n",
      "2022-08-16 08:36:54,406 - INFO - joeynmt.training - Epoch   7, Step:    13700, Batch Loss:     2.394173, Batch Acc: 0.486699, Tokens per Sec:     6521, Lr: 0.000054\n",
      "2022-08-16 08:37:21,796 - INFO - joeynmt.training - Epoch   7, Step:    13800, Batch Loss:     2.459407, Batch Acc: 0.485868, Tokens per Sec:     6211, Lr: 0.000054\n",
      "2022-08-16 08:37:49,195 - INFO - joeynmt.training - Epoch   7, Step:    13900, Batch Loss:     2.512361, Batch Acc: 0.484572, Tokens per Sec:     6262, Lr: 0.000054\n",
      "2022-08-16 08:38:15,524 - INFO - joeynmt.training - Epoch   7, Step:    14000, Batch Loss:     2.398164, Batch Acc: 0.485366, Tokens per Sec:     6482, Lr: 0.000053\n",
      "2022-08-16 08:38:15,524 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 08:39:04,766 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 08:39:04,767 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 08:39:04,767 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 08:39:04,775 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 08:39:04,775 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.01, loss:   2.53, ppl:  12.54, acc:   0.48, generation: 48.9088[sec], evaluation: 0.3238[sec]\n",
      "2022-08-16 08:39:04,776 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 08:39:05,785 - INFO - joeynmt.helpers - delete iwslt15_envi/9000.ckpt\n",
      "2022-08-16 08:39:05,789 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 08:39:05,792 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 08:39:05,793 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 08:39:05,793 - INFO - joeynmt.training - \tHypothesis: Làm thế nào tôi có thể nói trong 10 phút về những người phụ nữ trong 3 thế hệ , về sự kinh ngạc của những người phụ nữ , về sự kết nối của cuộc sống của một đứa trẻ gái trẻ em gái trẻ em gái trẻ em với những đứa trẻ gái trẻ em gái trẻ em gái , mẹ và mẹ tôi , mẹ cô gái trẻ em gái trong vòng 5 ngày và những người đàn ông đã từng đến gần như thế giới , bao giờ nhìn thấy một ngày càng nhiều hơn bao giờ , và không bao giờ nhìn thấy những người phụ nữ nào đó trong cuộc sống ở đó , và không bao giờ nhìn thấy người phụ nữ nào đó trong cuộc sống sót lại ở trong cuộc sống ở đây ?\n",
      "2022-08-16 08:39:05,793 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 08:39:05,795 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 08:39:05,795 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 08:39:05,795 - INFO - joeynmt.training - \tHypothesis: Đây không phải là một câu chuyện hoàn toàn .\n",
      "2022-08-16 08:39:05,795 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 08:39:05,797 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 08:39:05,798 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 08:39:05,798 - INFO - joeynmt.training - \tHypothesis: Nó là một câu đố <unk> vẫn đang kết nối với nhau .\n",
      "2022-08-16 08:39:05,798 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 08:39:05,800 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 08:39:05,801 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 08:39:05,801 - INFO - joeynmt.training - \tHypothesis: Để tôi kể cho các bạn nghe một số mảnh .\n",
      "2022-08-16 08:39:05,801 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 08:39:05,803 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 08:39:05,803 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 08:39:05,803 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng hình ảnh đầu tiên : một người đàn ông đang đốt đời .\n",
      "2022-08-16 08:39:31,374 - INFO - joeynmt.training - Epoch   7: total training loss 4990.00\n",
      "2022-08-16 08:39:31,374 - INFO - joeynmt.training - EPOCH 8\n",
      "2022-08-16 08:39:32,427 - INFO - joeynmt.training - Epoch   8, Step:    14100, Batch Loss:     2.376823, Batch Acc: 0.506352, Tokens per Sec:     6440, Lr: 0.000053\n",
      "2022-08-16 08:40:01,621 - INFO - joeynmt.training - Epoch   8, Step:    14200, Batch Loss:     2.358160, Batch Acc: 0.497063, Tokens per Sec:     5965, Lr: 0.000053\n",
      "2022-08-16 08:40:27,766 - INFO - joeynmt.training - Epoch   8, Step:    14300, Batch Loss:     2.388275, Batch Acc: 0.493036, Tokens per Sec:     6498, Lr: 0.000053\n",
      "2022-08-16 08:40:53,733 - INFO - joeynmt.training - Epoch   8, Step:    14400, Batch Loss:     2.216228, Batch Acc: 0.499171, Tokens per Sec:     6623, Lr: 0.000053\n",
      "2022-08-16 08:41:19,625 - INFO - joeynmt.training - Epoch   8, Step:    14500, Batch Loss:     2.381488, Batch Acc: 0.497348, Tokens per Sec:     6626, Lr: 0.000053\n",
      "2022-08-16 08:41:46,899 - INFO - joeynmt.training - Epoch   8, Step:    14600, Batch Loss:     2.400499, Batch Acc: 0.495167, Tokens per Sec:     6232, Lr: 0.000052\n",
      "2022-08-16 08:42:13,884 - INFO - joeynmt.training - Epoch   8, Step:    14700, Batch Loss:     2.313862, Batch Acc: 0.500752, Tokens per Sec:     6357, Lr: 0.000052\n",
      "2022-08-16 08:42:40,638 - INFO - joeynmt.training - Epoch   8, Step:    14800, Batch Loss:     2.310854, Batch Acc: 0.495306, Tokens per Sec:     6445, Lr: 0.000052\n",
      "2022-08-16 08:43:06,641 - INFO - joeynmt.training - Epoch   8, Step:    14900, Batch Loss:     2.352278, Batch Acc: 0.495544, Tokens per Sec:     6572, Lr: 0.000052\n",
      "2022-08-16 08:43:32,657 - INFO - joeynmt.training - Epoch   8, Step:    15000, Batch Loss:     2.338731, Batch Acc: 0.500242, Tokens per Sec:     6520, Lr: 0.000052\n",
      "2022-08-16 08:43:32,658 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 08:44:24,360 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 08:44:24,361 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 08:44:24,361 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 08:44:24,369 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 08:44:24,370 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.48, loss:   2.50, ppl:  12.17, acc:   0.49, generation: 51.3533[sec], evaluation: 0.3393[sec]\n",
      "2022-08-16 08:44:24,370 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 08:44:25,340 - INFO - joeynmt.helpers - delete iwslt15_envi/10000.ckpt\n",
      "2022-08-16 08:44:25,343 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 08:44:25,347 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 08:44:25,347 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 08:44:25,347 - INFO - joeynmt.training - \tHypothesis: Làm thế nào tôi có thể nói 10 phút về những kết nối của phụ nữ trên 3 thế hệ , về sự kinh ngạc của những người bị kết nối với những con gái trẻ em gái trẻ em với con gái , mẹ cô ấy , mẹ cô ấy , và mẹ cô ấy cho đến 5 ngày và một đêm nhỏ hơn cả những đêm gần đây , trong vòng quanh một ngày trước đó , không bao giờ để đi đến một ngày nào đó , và thậm chí chưa bao giờ nhìn thấy một ngày nào đó , và không bao giờ có thể đi xa xôi , trong cuộc sống trong cuộc sống sót lại ở đây .\n",
      "2022-08-16 08:44:25,347 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 08:44:25,350 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 08:44:25,350 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 08:44:25,350 - INFO - joeynmt.training - \tHypothesis: Đây không phải là câu chuyện hoàn toàn .\n",
      "2022-08-16 08:44:25,350 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 08:44:25,352 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 08:44:25,352 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 08:44:25,352 - INFO - joeynmt.training - \tHypothesis: Đó là một câu đố hoàn toàn bị đặt vào nhau .\n",
      "2022-08-16 08:44:25,353 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 08:44:25,355 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 08:44:25,355 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 08:44:25,355 - INFO - joeynmt.training - \tHypothesis: Để tôi kể cho các bạn nghe về một số mảnh .\n",
      "2022-08-16 08:44:25,356 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 08:44:25,358 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 08:44:25,358 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 08:44:25,358 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng đầu tiên : một người đàn ông đang đốt cháy cuộc sống của ông .\n",
      "2022-08-16 08:44:52,315 - INFO - joeynmt.training - Epoch   8, Step:    15100, Batch Loss:     2.077572, Batch Acc: 0.500364, Tokens per Sec:     6093, Lr: 0.000051\n",
      "2022-08-16 08:45:18,564 - INFO - joeynmt.training - Epoch   8, Step:    15200, Batch Loss:     2.492379, Batch Acc: 0.499463, Tokens per Sec:     6450, Lr: 0.000051\n",
      "2022-08-16 08:45:44,607 - INFO - joeynmt.training - Epoch   8, Step:    15300, Batch Loss:     2.297393, Batch Acc: 0.497394, Tokens per Sec:     6676, Lr: 0.000051\n",
      "2022-08-16 08:46:12,345 - INFO - joeynmt.training - Epoch   8, Step:    15400, Batch Loss:     2.218722, Batch Acc: 0.501671, Tokens per Sec:     6126, Lr: 0.000051\n",
      "2022-08-16 08:46:38,454 - INFO - joeynmt.training - Epoch   8, Step:    15500, Batch Loss:     2.458813, Batch Acc: 0.500577, Tokens per Sec:     6643, Lr: 0.000051\n",
      "2022-08-16 08:47:05,457 - INFO - joeynmt.training - Epoch   8, Step:    15600, Batch Loss:     2.319690, Batch Acc: 0.498710, Tokens per Sec:     6389, Lr: 0.000051\n",
      "2022-08-16 08:47:31,391 - INFO - joeynmt.training - Epoch   8, Step:    15700, Batch Loss:     2.382803, Batch Acc: 0.501970, Tokens per Sec:     6586, Lr: 0.000050\n",
      "2022-08-16 08:47:57,349 - INFO - joeynmt.training - Epoch   8, Step:    15800, Batch Loss:     2.208094, Batch Acc: 0.501675, Tokens per Sec:     6625, Lr: 0.000050\n",
      "2022-08-16 08:48:24,345 - INFO - joeynmt.training - Epoch   8, Step:    15900, Batch Loss:     2.194117, Batch Acc: 0.496608, Tokens per Sec:     6313, Lr: 0.000050\n",
      "2022-08-16 08:48:50,449 - INFO - joeynmt.training - Epoch   8, Step:    16000, Batch Loss:     2.402405, Batch Acc: 0.500757, Tokens per Sec:     6551, Lr: 0.000050\n",
      "2022-08-16 08:48:50,449 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 08:49:39,471 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 08:49:39,471 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 08:49:39,471 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 08:49:39,479 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 08:49:39,480 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.98, loss:   2.47, ppl:  11.80, acc:   0.49, generation: 48.6842[sec], evaluation: 0.3222[sec]\n",
      "2022-08-16 08:49:39,480 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 08:49:40,798 - INFO - joeynmt.helpers - delete iwslt15_envi/11000.ckpt\n",
      "2022-08-16 08:49:40,801 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 08:49:40,805 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 08:49:40,805 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 08:49:40,806 - INFO - joeynmt.training - \tHypothesis: Làm thế nào tôi có thể nói trong 10 phút về những kết nối kết nối của phụ nữ trên 3 thế hệ , về sự kinh ngạc của những người đàn ông trẻ em gái trẻ em với cô gái , mẹ em gái trẻ em và mẹ tôi trong vòng 5 ngày trong một đêm nhỏ hơn bao giờ được nuôi dưỡng trong những đứa trẻ , và những con gái lớn hơn bao giờ hết sức mạnh hơn bao giờ đến nỗi đau đớn và không bao giờ hết sức khoẻ của cô ấy .\n",
      "2022-08-16 08:49:40,806 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 08:49:40,808 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 08:49:40,808 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 08:49:40,808 - INFO - joeynmt.training - \tHypothesis: Đây không phải là câu chuyện hoàn thành .\n",
      "2022-08-16 08:49:40,809 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 08:49:40,810 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 08:49:40,811 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 08:49:40,811 - INFO - joeynmt.training - \tHypothesis: Đó là một câu đố <unk> vẫn đang đặt nhau .\n",
      "2022-08-16 08:49:40,811 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 08:49:40,813 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 08:49:40,813 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 08:49:40,813 - INFO - joeynmt.training - \tHypothesis: Để tôi nói với các bạn về một số mảnh .\n",
      "2022-08-16 08:49:40,814 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 08:49:40,816 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 08:49:40,816 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 08:49:40,816 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng hình ảnh đầu tiên : một người đàn ông đang đốt cháy đời .\n",
      "2022-08-16 08:50:08,116 - INFO - joeynmt.training - Epoch   8, Step:    16100, Batch Loss:     2.461181, Batch Acc: 0.500396, Tokens per Sec:     5952, Lr: 0.000050\n",
      "2022-08-16 08:50:10,711 - INFO - joeynmt.training - Epoch   8: total training loss 4779.43\n",
      "2022-08-16 08:50:10,712 - INFO - joeynmt.training - EPOCH 9\n",
      "2022-08-16 08:50:35,432 - INFO - joeynmt.training - Epoch   9, Step:    16200, Batch Loss:     2.171238, Batch Acc: 0.507660, Tokens per Sec:     6496, Lr: 0.000050\n",
      "2022-08-16 08:51:01,738 - INFO - joeynmt.training - Epoch   9, Step:    16300, Batch Loss:     2.172394, Batch Acc: 0.507418, Tokens per Sec:     6472, Lr: 0.000050\n",
      "2022-08-16 08:51:27,945 - INFO - joeynmt.training - Epoch   9, Step:    16400, Batch Loss:     2.586539, Batch Acc: 0.513394, Tokens per Sec:     6393, Lr: 0.000049\n",
      "2022-08-16 08:51:54,983 - INFO - joeynmt.training - Epoch   9, Step:    16500, Batch Loss:     2.221967, Batch Acc: 0.510185, Tokens per Sec:     6331, Lr: 0.000049\n",
      "2022-08-16 08:52:21,965 - INFO - joeynmt.training - Epoch   9, Step:    16600, Batch Loss:     2.280917, Batch Acc: 0.509882, Tokens per Sec:     6335, Lr: 0.000049\n",
      "2022-08-16 08:52:47,909 - INFO - joeynmt.training - Epoch   9, Step:    16700, Batch Loss:     2.275926, Batch Acc: 0.508734, Tokens per Sec:     6553, Lr: 0.000049\n",
      "2022-08-16 08:53:13,830 - INFO - joeynmt.training - Epoch   9, Step:    16800, Batch Loss:     2.259735, Batch Acc: 0.511896, Tokens per Sec:     6543, Lr: 0.000049\n",
      "2022-08-16 08:53:40,060 - INFO - joeynmt.training - Epoch   9, Step:    16900, Batch Loss:     2.289679, Batch Acc: 0.511410, Tokens per Sec:     6539, Lr: 0.000049\n",
      "2022-08-16 08:54:07,040 - INFO - joeynmt.training - Epoch   9, Step:    17000, Batch Loss:     2.199394, Batch Acc: 0.512469, Tokens per Sec:     6343, Lr: 0.000049\n",
      "2022-08-16 08:54:07,041 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 08:54:56,536 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 08:54:56,536 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 08:54:56,536 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 08:54:56,546 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 08:54:56,546 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.28, loss:   2.45, ppl:  11.58, acc:   0.50, generation: 49.1521[sec], evaluation: 0.3337[sec]\n",
      "2022-08-16 08:54:56,547 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 08:54:57,508 - INFO - joeynmt.helpers - delete iwslt15_envi/12000.ckpt\n",
      "2022-08-16 08:54:57,512 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 08:54:57,516 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 08:54:57,516 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 08:54:57,516 - INFO - joeynmt.training - \tHypothesis: Làm thế nào tôi có thể nói trong 10 phút về những kết nối kết nối của phụ nữ trên 3 thế hệ , về sự kinh ngạc của những người bị kết nối đầy sức mạnh của cuộc sống của một đứa trẻ trẻ với những đứa trẻ trẻ tuổi của cô bé gái , mẹ cô ấy , mẹ cô ấy , và mẹ cô ấy trong vòng 5 ngày trong một đêm dài hơn <unk> , khoảng năm qua , những con gái đã bỏ qua những con gái , và không bao giờ hết những con gái chưa bao giờ để đi xa xôi này , và không bao giờ để nhìn thấy được nhìn thấy rằng , và không bao giờ hết .\n",
      "2022-08-16 08:54:57,517 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 08:54:57,519 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 08:54:57,519 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 08:54:57,519 - INFO - joeynmt.training - \tHypothesis: Đây không phải là câu chuyện hoàn toàn .\n",
      "2022-08-16 08:54:57,520 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 08:54:57,521 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 08:54:57,522 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 08:54:57,522 - INFO - joeynmt.training - \tHypothesis: Đó là một câu đố về <unk> vẫn đang được đặt lại với nhau .\n",
      "2022-08-16 08:54:57,522 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 08:54:57,524 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 08:54:57,524 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 08:54:57,524 - INFO - joeynmt.training - \tHypothesis: Để tôi nói với các bạn về một số mảnh .\n",
      "2022-08-16 08:54:57,524 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 08:54:57,527 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 08:54:57,527 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 08:54:57,527 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng một bức ảnh đầu tiên : một người đàn ông đang đốt cháy cuộc đời của ông .\n",
      "2022-08-16 08:55:23,647 - INFO - joeynmt.training - Epoch   9, Step:    17100, Batch Loss:     2.562418, Batch Acc: 0.512191, Tokens per Sec:     6311, Lr: 0.000048\n",
      "2022-08-16 08:55:49,832 - INFO - joeynmt.training - Epoch   9, Step:    17200, Batch Loss:     2.240025, Batch Acc: 0.512199, Tokens per Sec:     6600, Lr: 0.000048\n",
      "2022-08-16 08:56:15,676 - INFO - joeynmt.training - Epoch   9, Step:    17300, Batch Loss:     2.104115, Batch Acc: 0.515415, Tokens per Sec:     6718, Lr: 0.000048\n",
      "2022-08-16 08:56:44,403 - INFO - joeynmt.training - Epoch   9, Step:    17400, Batch Loss:     2.686036, Batch Acc: 0.509117, Tokens per Sec:     6071, Lr: 0.000048\n",
      "2022-08-16 08:57:10,481 - INFO - joeynmt.training - Epoch   9, Step:    17500, Batch Loss:     2.273933, Batch Acc: 0.511656, Tokens per Sec:     6636, Lr: 0.000048\n",
      "2022-08-16 08:57:36,380 - INFO - joeynmt.training - Epoch   9, Step:    17600, Batch Loss:     2.243421, Batch Acc: 0.512854, Tokens per Sec:     6658, Lr: 0.000048\n",
      "2022-08-16 08:58:02,174 - INFO - joeynmt.training - Epoch   9, Step:    17700, Batch Loss:     2.263133, Batch Acc: 0.512029, Tokens per Sec:     6607, Lr: 0.000048\n",
      "2022-08-16 08:58:28,681 - INFO - joeynmt.training - Epoch   9, Step:    17800, Batch Loss:     2.309373, Batch Acc: 0.515311, Tokens per Sec:     6352, Lr: 0.000047\n",
      "2022-08-16 08:58:55,167 - INFO - joeynmt.training - Epoch   9, Step:    17900, Batch Loss:     2.260663, Batch Acc: 0.508657, Tokens per Sec:     6370, Lr: 0.000047\n",
      "2022-08-16 08:59:21,259 - INFO - joeynmt.training - Epoch   9, Step:    18000, Batch Loss:     2.264556, Batch Acc: 0.514984, Tokens per Sec:     6520, Lr: 0.000047\n",
      "2022-08-16 08:59:21,259 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 09:00:08,210 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 09:00:08,210 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 09:00:08,211 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 09:00:08,219 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 09:00:08,220 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.26, loss:   2.43, ppl:  11.35, acc:   0.50, generation: 46.6200[sec], evaluation: 0.3224[sec]\n",
      "2022-08-16 09:00:09,108 - INFO - joeynmt.helpers - delete iwslt15_envi/13000.ckpt\n",
      "2022-08-16 09:00:09,125 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 09:00:09,129 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 09:00:09,129 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 09:00:09,130 - INFO - joeynmt.training - \tHypothesis: Làm sao tôi có thể nói trong 10 phút về những kết nối kết nối của phụ nữ trên 3 thế hệ , về sự kinh ngạc của những người đàn ông trẻ em gái trẻ em với cô gái trẻ em , mẹ cô ấy , mẹ cô ấy trong vòng 5 ngày và một đêm nhỏ hơn bao nhiêu năm trước khi con tàu ở Trung Quốc , và những con gái nhỏ bé đã giữ lại cho đến nỗi đau đớn và không bao nhiêu người phụ nữ , và không bao giờ nhìn thấy một cô bé gái ở đây .\n",
      "2022-08-16 09:00:09,130 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 09:00:09,132 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 09:00:09,132 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 09:00:09,132 - INFO - joeynmt.training - \tHypothesis: Đây không phải là câu chuyện hoàn toàn .\n",
      "2022-08-16 09:00:09,132 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 09:00:09,134 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 09:00:09,135 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 09:00:09,135 - INFO - joeynmt.training - \tHypothesis: Đó là một câu đố <unk> vẫn đang cùng nhau .\n",
      "2022-08-16 09:00:09,135 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 09:00:09,137 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 09:00:09,137 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 09:00:09,137 - INFO - joeynmt.training - \tHypothesis: Để tôi kể cho các bạn nghe về một số mảnh .\n",
      "2022-08-16 09:00:09,137 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 09:00:09,139 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 09:00:09,140 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 09:00:09,140 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng một bức ảnh đầu tiên : một người đàn ông đang đốt cháy cuộc đời của ông .\n",
      "2022-08-16 09:00:37,243 - INFO - joeynmt.training - Epoch   9, Step:    18100, Batch Loss:     2.244928, Batch Acc: 0.512680, Tokens per Sec:     5954, Lr: 0.000047\n",
      "2022-08-16 09:00:42,595 - INFO - joeynmt.training - Epoch   9: total training loss 4622.80\n",
      "2022-08-16 09:00:42,596 - INFO - joeynmt.training - EPOCH 10\n",
      "2022-08-16 09:01:03,318 - INFO - joeynmt.training - Epoch  10, Step:    18200, Batch Loss:     2.288924, Batch Acc: 0.523377, Tokens per Sec:     6684, Lr: 0.000047\n",
      "2022-08-16 09:01:30,113 - INFO - joeynmt.training - Epoch  10, Step:    18300, Batch Loss:     2.284136, Batch Acc: 0.525455, Tokens per Sec:     6396, Lr: 0.000047\n",
      "2022-08-16 09:01:56,772 - INFO - joeynmt.training - Epoch  10, Step:    18400, Batch Loss:     2.134892, Batch Acc: 0.516784, Tokens per Sec:     6378, Lr: 0.000047\n",
      "2022-08-16 09:02:22,466 - INFO - joeynmt.training - Epoch  10, Step:    18500, Batch Loss:     2.235380, Batch Acc: 0.517884, Tokens per Sec:     6626, Lr: 0.000046\n",
      "2022-08-16 09:02:48,897 - INFO - joeynmt.training - Epoch  10, Step:    18600, Batch Loss:     2.369292, Batch Acc: 0.522584, Tokens per Sec:     6422, Lr: 0.000046\n",
      "2022-08-16 09:03:14,629 - INFO - joeynmt.training - Epoch  10, Step:    18700, Batch Loss:     2.134843, Batch Acc: 0.515918, Tokens per Sec:     6714, Lr: 0.000046\n",
      "2022-08-16 09:03:41,578 - INFO - joeynmt.training - Epoch  10, Step:    18800, Batch Loss:     2.248907, Batch Acc: 0.521890, Tokens per Sec:     6350, Lr: 0.000046\n",
      "2022-08-16 09:04:07,137 - INFO - joeynmt.training - Epoch  10, Step:    18900, Batch Loss:     2.142916, Batch Acc: 0.523519, Tokens per Sec:     6697, Lr: 0.000046\n",
      "2022-08-16 09:04:32,728 - INFO - joeynmt.training - Epoch  10, Step:    19000, Batch Loss:     2.268675, Batch Acc: 0.524857, Tokens per Sec:     6624, Lr: 0.000046\n",
      "2022-08-16 09:04:32,729 - INFO - joeynmt.prediction - Predicting 1553 example(s)... (Greedy decoding with min_output_length=1, max_output_length=150, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-16 09:05:25,272 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-16 09:05:25,273 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-16 09:05:25,273 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "2022-08-16 09:05:25,282 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-16 09:05:25,282 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.11, loss:   2.41, ppl:  11.18, acc:   0.50, generation: 52.2099[sec], evaluation: 0.3243[sec]\n",
      "2022-08-16 09:05:25,283 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-16 09:05:26,218 - INFO - joeynmt.helpers - delete iwslt15_envi/14000.ckpt\n",
      "2022-08-16 09:05:26,223 - INFO - joeynmt.training - Example #0\n",
      "2022-08-16 09:05:26,233 - INFO - joeynmt.training - \tSource:     How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "2022-08-16 09:05:26,233 - INFO - joeynmt.training - \tReference:  Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "2022-08-16 09:05:26,233 - INFO - joeynmt.training - \tHypothesis: Làm thế nào tôi có thể nói trong 10 phút về những kết nối của phụ nữ trên 3 thế hệ , về sự kinh ngạc của những mối quan hệ kinh ngạc của những con gái trẻ em gái trẻ em với những con gái trẻ em gái , mẹ cô ấy , mẹ em gái trong 5 ngày và đêm trong vòng 5 ngày trong một đêm dài hơn 30 năm trước , để giữ lại những mối liên kết của người đàn ông đã được đưa ra đời trong cuộc sống của một cô bé gái , và không bao giờ đây không bao giờ để đi tới những người phụ nữ lớn hơn bao giờ đến nơi nào đó ?\n",
      "2022-08-16 09:05:26,233 - INFO - joeynmt.training - Example #1\n",
      "2022-08-16 09:05:26,236 - INFO - joeynmt.training - \tSource:     This is not a finished story .\n",
      "2022-08-16 09:05:26,236 - INFO - joeynmt.training - \tReference:  Câu chuyện này chưa kết thúc .\n",
      "2022-08-16 09:05:26,236 - INFO - joeynmt.training - \tHypothesis: Đây không phải là câu chuyện hoàn toàn .\n",
      "2022-08-16 09:05:26,236 - INFO - joeynmt.training - Example #2\n",
      "2022-08-16 09:05:26,238 - INFO - joeynmt.training - \tSource:     It is a jigsaw puzzle still being put together .\n",
      "2022-08-16 09:05:26,239 - INFO - joeynmt.training - \tReference:  Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "2022-08-16 09:05:26,239 - INFO - joeynmt.training - \tHypothesis: Đó là một câu đố <unk> vẫn đang được đặt vào cùng nhau .\n",
      "2022-08-16 09:05:26,239 - INFO - joeynmt.training - Example #3\n",
      "2022-08-16 09:05:26,241 - INFO - joeynmt.training - \tSource:     Let me tell you about some of the pieces .\n",
      "2022-08-16 09:05:26,241 - INFO - joeynmt.training - \tReference:  Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "2022-08-16 09:05:26,241 - INFO - joeynmt.training - \tHypothesis: Để tôi kể cho các bạn về một số mảnh .\n",
      "2022-08-16 09:05:26,241 - INFO - joeynmt.training - Example #4\n",
      "2022-08-16 09:05:26,244 - INFO - joeynmt.training - \tSource:     Imagine the first piece : a man burning his life &apos;s work .\n",
      "2022-08-16 09:05:26,244 - INFO - joeynmt.training - \tReference:  Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "2022-08-16 09:05:26,244 - INFO - joeynmt.training - \tHypothesis: Hãy tưởng tượng một bức ảnh đầu tiên : một người đàn ông đang đốt cháy cuộc đời của anh ta .\n",
      "2022-08-16 09:05:52,831 - INFO - joeynmt.training - Epoch  10, Step:    19100, Batch Loss:     2.152181, Batch Acc: 0.522395, Tokens per Sec:     6249, Lr: 0.000046\n"
     ]
    }
   ],
   "source": [
    "!cd {root_dir} && python -m joeynmt train data/iwslt15/config.yaml"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tokenizer_tutorial_ja.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0058e56d30cb40598b62e2798d57dd4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13da6ffd78964675b699f544510947bb",
      "placeholder": "​",
      "style": "IPY_MODEL_0743dbb4232a4143bf3d22f10a606006",
      "value": " 1280/0 [00:00&lt;00:00, 6863.20 examples/s]"
     }
    },
    "0088e966ec24479e93c0708943d7c50a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_42ac07d93bd04fcdaf84e4816bd148f8",
       "IPY_MODEL_2d1580ec31fe4095ac7cf88910ba8786",
       "IPY_MODEL_bca867bc93f8414b92045a5edb6d804a"
      ],
      "layout": "IPY_MODEL_0f91969769b740bda999dedda5749d11"
     }
    },
    "0427037fa8cc4f87830440f1ca0c49e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0743dbb4232a4143bf3d22f10a606006": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "085a74d291524e66a2ba6af109bc1f29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08b737b565784db299378bad41c3453e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0b0ecf86aa14e068d4b963f1505cca6",
      "placeholder": "​",
      "style": "IPY_MODEL_3eb076444f644f3493987876b974da34",
      "value": " 3/3 [00:00&lt;00:00, 54.53it/s]"
     }
    },
    "0a61fbf753d644acbcbab8cf37bfb227": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c718d2705fd4372b570943312aad077": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2cfa7e4f49154f45be0d0c0ca91f4c8e",
      "placeholder": "​",
      "style": "IPY_MODEL_65da89eebd6342cd908e0d725ab2ab49",
      "value": "Downloading data: 100%"
     }
    },
    "0d5d96469e5d490aa205e73477a0b424": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2fcba7e77094c9ab93783addebbca12",
      "placeholder": "​",
      "style": "IPY_MODEL_44df3a9d1b4047c0bfc87c023d6c14bf",
      "value": "100%"
     }
    },
    "0d867e7ef95e4217b82254f7fe5ce6ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f0b7b9be81643ea83c065c56f588824": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e96c517c01c4329a46241e33e89d219",
       "IPY_MODEL_bb40cf7eebaf4628a2d6d5d01dbef583",
       "IPY_MODEL_d5ee389e7e344fc48ed328810fe91f51"
      ],
      "layout": "IPY_MODEL_c238e479b18c44839d46cc557c1be27d"
     }
    },
    "0f7a938d874a42cb917a486bdb096315": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3080c8063942424b8374f593c971a2df",
       "IPY_MODEL_5c5bc58dcb794bbbac54defd763f179d",
       "IPY_MODEL_c33ff826c05b4ec6aa87edfb75228364"
      ],
      "layout": "IPY_MODEL_a7758b23eabf41c5a4fa948f90405b73"
     }
    },
    "0f91969769b740bda999dedda5749d11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fa6558cf20a4598b11db115fcc6e2fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_869fb4a9a5e54cefb9adac572807286d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_357925fee60e498a9873d41352be0d89",
      "value": 1
     }
    },
    "11208e71ca224360bb3a19428d1666e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b170f87f3eed49539b296cb04bb1327c",
      "placeholder": "​",
      "style": "IPY_MODEL_ec3867f8184a447bb304ef26a53095fd",
      "value": "Downloading data: 100%"
     }
    },
    "13da6ffd78964675b699f544510947bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1664592b61e14e4b9c87150580e573b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af901ab329c0414ebd2c243768826c82",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6257b78b5e014e40a8cbae49dd0fcd42",
      "value": 1
     }
    },
    "16bec58461cd4974ab503fc844c407b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "19777cefd32a47408b5c768570775f6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "19c1677a88a6439e8ee983512672b5f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71aef2d1db4f4054bb3e67df794141aa",
      "max": 183855,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac0ec675aa4f4de7b7a07c217476e1ba",
      "value": 183855
     }
    },
    "1f6be19d0d4a48118220d1a880d0745d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6c2958faa4e45c784eb60165f674a71",
      "placeholder": "​",
      "style": "IPY_MODEL_417b45ffccbf4fb5874c056d7ba30b5c",
      "value": " 1.09G/1.09G [00:26&lt;00:00, 25.6MB/s]"
     }
    },
    "1f819700db504313a5fdf566a07261b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2019cd63192647b392ae48e85ebfa1b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d155c2fc2e05415aa4a86ca814998457",
      "placeholder": "​",
      "style": "IPY_MODEL_559fb3b3c0a64e5fbfbe78d54efbb90f",
      "value": "Downloading data: 100%"
     }
    },
    "204178f1f7bd4ab892fa5bdf47909dcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2128acc04eab49a6a99b5486a317d5e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6d5e11a804e4fd7a01fc7c1e7700a5b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2359486a3c9041e2a7e2e03238aa817f",
      "value": 1
     }
    },
    "2359486a3c9041e2a7e2e03238aa817f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23f4f10e6a4a400cafecba96e89384a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24a96bde324642369009617fcb6c9e16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "296f06e92f824c10b2bd9b772c65a3b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "298eaf75350e4a918dc7e859ada452b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fa81f2f29ef462bb7114509546e3fc4",
      "placeholder": "​",
      "style": "IPY_MODEL_16bec58461cd4974ab503fc844c407b7",
      "value": "Downloading data: 100%"
     }
    },
    "2a2636cc0c4647bbbbf1d0d261376f35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2abcc7810447406c8a0fa08dc0a4fb3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cfa7e4f49154f45be0d0c0ca91f4c8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d1580ec31fe4095ac7cf88910ba8786": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d7179d4fb2a4aa29d9358074503cb9a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_858702e842174e47a39919997e5dcdfe",
      "value": 1
     }
    },
    "2f5a3572fbc7412190a0be7158be3988": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5841ded11a864b6e87863f6e545ffa23",
       "IPY_MODEL_2128acc04eab49a6a99b5486a317d5e3",
       "IPY_MODEL_4c7c78770d2041e2a66c99e49bad7f01"
      ],
      "layout": "IPY_MODEL_0427037fa8cc4f87830440f1ca0c49e7"
     }
    },
    "3080c8063942424b8374f593c971a2df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47d839270a46420b9b18893ed0805344",
      "placeholder": "​",
      "style": "IPY_MODEL_24a96bde324642369009617fcb6c9e16",
      "value": "100%"
     }
    },
    "30c51062df1e44dda5dfc3ab1ea5fec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f819700db504313a5fdf566a07261b1",
      "placeholder": "​",
      "style": "IPY_MODEL_0d867e7ef95e4217b82254f7fe5ce6ec",
      "value": "Downloading data: 100%"
     }
    },
    "3197e91e6e234c18b5b9822de3d1f231": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "357925fee60e498a9873d41352be0d89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3800772f0bc74a308dbc1939d06f8d4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39cc1007bf714de6abe9505ef637175c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2343705bce74db382439479af490d7c",
      "placeholder": "​",
      "style": "IPY_MODEL_50651d8566c14893a12ec28b3f7c44df",
      "value": " 3728/0 [00:00&lt;00:00, 12588.00 examples/s]"
     }
    },
    "3d562f2f6cdf4dfdaffac3e0eb9bf213": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3eb076444f644f3493987876b974da34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f7095b4a86e40ecb6740c12fd53a99f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "417b45ffccbf4fb5874c056d7ba30b5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42ac07d93bd04fcdaf84e4816bd148f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f15fe947b0b44c69d2b6b6f7883a734",
      "placeholder": "​",
      "style": "IPY_MODEL_9fae32990eb14b48a9b3252f14456336",
      "value": "Generating test split: "
     }
    },
    "42c431d43f414396b17b0fa740557e02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "44df3a9d1b4047c0bfc87c023d6c14bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45b03a72f012428a97da972d86f9229f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "462ccf11cf0246f28046ff219bf0db8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46452d95277d4e0a9c7662be89ee48f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_30c51062df1e44dda5dfc3ab1ea5fec5",
       "IPY_MODEL_73f568b15238421ebc8af92e00f5e5d4",
       "IPY_MODEL_9bd7fdb25052491e877a47dad9f2827c"
      ],
      "layout": "IPY_MODEL_5ea8a4c33e434fc1847e17328dc69f4c"
     }
    },
    "46f5ff45c5d542c3a1dc7595d6dc65c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63e63430d1b2485d8d9988d7c09bed0d",
      "placeholder": "​",
      "style": "IPY_MODEL_8fc70a6a14144814a9ca3e825abc019f",
      "value": " 133162/0 [00:04&lt;00:00, 30239.08 examples/s]"
     }
    },
    "47d839270a46420b9b18893ed0805344": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49918e2f66224c75b4cd1b154d1d48f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4c7c78770d2041e2a66c99e49bad7f01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5ed3f6d70ee4608ab38ac2fbe78e943",
      "placeholder": "​",
      "style": "IPY_MODEL_9fbf5bbe8bb646239b2196faf41283af",
      "value": " 172374/0 [00:07&lt;00:00, 24671.28 examples/s]"
     }
    },
    "4e7a6483739a4ce38f8d6ebe900a89f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f15fe947b0b44c69d2b6b6f7883a734": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50651d8566c14893a12ec28b3f7c44df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "53a1d3e2d5754413b77dd2f5c5e1ae90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62821e35851043569ba3fcc822c235ed",
       "IPY_MODEL_79c9b2b64c4e48f1905b425db44e2db9",
       "IPY_MODEL_39cc1007bf714de6abe9505ef637175c"
      ],
      "layout": "IPY_MODEL_85542cc8c06944f9b5e8090ef35a9407"
     }
    },
    "5521232d155e463e8c6d79503423895b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55333d675b42426bb1f5bc1ccf82827f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aacf253901d14d58b5eabf27a80a131e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8967c4fc0eca4fbb9892afd07f5d2619",
      "value": 1
     }
    },
    "559fb3b3c0a64e5fbfbe78d54efbb90f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55e99e911d9842fda2aa7122cdd4fc09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0c718d2705fd4372b570943312aad077",
       "IPY_MODEL_bf55263c138c45daabba0498c79056aa",
       "IPY_MODEL_7a2510b2452f4303a98d010b4660486e"
      ],
      "layout": "IPY_MODEL_bd80be291e7546c0886f7938283d22bd"
     }
    },
    "5841ded11a864b6e87863f6e545ffa23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec5bac39bb254f7d8690d7aef2984b86",
      "placeholder": "​",
      "style": "IPY_MODEL_42c431d43f414396b17b0fa740557e02",
      "value": "Generating train split: "
     }
    },
    "587548fc703142a78f10a394ceb37597": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db082aaeff6e4f9db14c580993e859f4",
      "max": 1093557157,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e99eafed62824d1aa6cc17c5c3f6a8c6",
      "value": 1093557157
     }
    },
    "5c5bc58dcb794bbbac54defd763f179d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff5a3824f6e94689be3bf83eed88302e",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_636c2c57cb3b44c0a8fe8033459f3bb5",
      "value": 3
     }
    },
    "5e96c517c01c4329a46241e33e89d219": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad52b9c845874da69b591ac6b4df1e22",
      "placeholder": "​",
      "style": "IPY_MODEL_9cfa0525d6344e0c9a4b2e07f637df6b",
      "value": "Downloading data: 100%"
     }
    },
    "5ea8a4c33e434fc1847e17328dc69f4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60ce6bd2a23e416fa16bfa98376a31bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd00a0a4c0724f3080a5de179c3ca880",
      "placeholder": "​",
      "style": "IPY_MODEL_937659bc1dec4e248e786ad3a8bc686f",
      "value": "Generating validation split: "
     }
    },
    "6257b78b5e014e40a8cbae49dd0fcd42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "62821e35851043569ba3fcc822c235ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_296f06e92f824c10b2bd9b772c65a3b2",
      "placeholder": "​",
      "style": "IPY_MODEL_085a74d291524e66a2ba6af109bc1f29",
      "value": "Generating test split: "
     }
    },
    "62d71c6432f44d2d928063ece3de65a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2019cd63192647b392ae48e85ebfa1b8",
       "IPY_MODEL_b53672c8f0c5421b927ac73110175af1",
       "IPY_MODEL_859974f5e60a4cd39803273ec0af372b"
      ],
      "layout": "IPY_MODEL_ab9d39d5aec34b34aa5dc135385ab51f"
     }
    },
    "636c2c57cb3b44c0a8fe8033459f3bb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "63e63430d1b2485d8d9988d7c09bed0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65da89eebd6342cd908e0d725ab2ab49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6727165b8edc4733a4f7ce08fc7b9da5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cea2bf9c6dd54056b4cc854edd9273ff",
      "placeholder": "​",
      "style": "IPY_MODEL_a062972fdd514593bcdb9dff844da9fb",
      "value": " 1313/0 [00:00&lt;00:00, 13128.51 examples/s]"
     }
    },
    "71aef2d1db4f4054bb3e67df794141aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72ad75a111624935ae72eb30d8e2243a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73f568b15238421ebc8af92e00f5e5d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aaabdb5cdca3447d880d5d60576e0664",
      "max": 132264,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e5fbd6b53bb94051b4235a57853591f1",
      "value": 132264
     }
    },
    "76686bd0636a4401ad2b8debedd39e8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "769c25ababe6478587c5f0e117c49103": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79c9b2b64c4e48f1905b425db44e2db9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f696be6a65924089b85589ecc2cddf61",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8d4f1a442df5476da052df622c56498d",
      "value": 1
     }
    },
    "7a2510b2452f4303a98d010b4660486e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_864cf880c1d64da6a51a7e9a46ccba2a",
      "placeholder": "​",
      "style": "IPY_MODEL_d4cd39762dbe4ae5b2ad9d943c7f4151",
      "value": " 188k/188k [00:00&lt;00:00, 344kB/s]"
     }
    },
    "7c4465ec5e544906aa41a7e6cc82451e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76686bd0636a4401ad2b8debedd39e8c",
      "placeholder": "​",
      "style": "IPY_MODEL_3f7095b4a86e40ecb6740c12fd53a99f",
      "value": "Generating validation split: "
     }
    },
    "7ccc882eaf30450abd88f05a435ce670": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d7179d4fb2a4aa29d9358074503cb9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "85542cc8c06944f9b5e8090ef35a9407": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "858702e842174e47a39919997e5dcdfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "859974f5e60a4cd39803273ec0af372b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e7a6483739a4ce38f8d6ebe900a89f6",
      "placeholder": "​",
      "style": "IPY_MODEL_19777cefd32a47408b5c768570775f6d",
      "value": " 18.1M/18.1M [00:01&lt;00:00, 21.4MB/s]"
     }
    },
    "864cf880c1d64da6a51a7e9a46ccba2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "869fb4a9a5e54cefb9adac572807286d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "86cc4d44365a4e5aa9b6fa27d3c0932e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8894c6cd80ec4a9990aabba67edfda5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8967c4fc0eca4fbb9892afd07f5d2619": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8b28cd538fdc4de8bd1715d7da4f9660": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45b03a72f012428a97da972d86f9229f",
      "placeholder": "​",
      "style": "IPY_MODEL_8b548b1f5fd34e5fb2a2591c848ae41d",
      "value": " 13.6M/13.6M [00:01&lt;00:00, 17.5MB/s]"
     }
    },
    "8b548b1f5fd34e5fb2a2591c848ae41d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d4f1a442df5476da052df622c56498d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8fc70a6a14144814a9ca3e825abc019f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "937659bc1dec4e248e786ad3a8bc686f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9528cd36b35a41fbb8eb21910f172b4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "959a1822eb3643029185daffd0e6cb63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d3f7d48f24ee4efa91018c2ad43c619b",
       "IPY_MODEL_0fa6558cf20a4598b11db115fcc6e2fc",
       "IPY_MODEL_46f5ff45c5d542c3a1dc7595d6dc65c9"
      ],
      "layout": "IPY_MODEL_e8d9d6b9ad2e42819beaa2dfdd52a63f"
     }
    },
    "96ea1ea254e64e618df32643a7c923dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97a8756ce6224f379223ef76052d73ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bd7fdb25052491e877a47dad9f2827c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_769c25ababe6478587c5f0e117c49103",
      "placeholder": "​",
      "style": "IPY_MODEL_dd7047324fc1435098c471496b2285e5",
      "value": " 132k/132k [00:00&lt;00:00, 342kB/s]"
     }
    },
    "9c4ddccfde3c41d79c700d89ed863ce7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cfa0525d6344e0c9a4b2e07f637df6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fa81f2f29ef462bb7114509546e3fc4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fae32990eb14b48a9b3252f14456336": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fbf5bbe8bb646239b2196faf41283af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a062972fdd514593bcdb9dff844da9fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1930c66806644318d3893a34286939a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a20a447b39f84312a0eff1887bb9ed44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ccc882eaf30450abd88f05a435ce670",
      "max": 174443,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_49918e2f66224c75b4cd1b154d1d48f8",
      "value": 174443
     }
    },
    "a6a47180d0424330b5efb8d81129ce01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7c4465ec5e544906aa41a7e6cc82451e",
       "IPY_MODEL_55333d675b42426bb1f5bc1ccf82827f",
       "IPY_MODEL_0058e56d30cb40598b62e2798d57dd4c"
      ],
      "layout": "IPY_MODEL_b3fca54d08eb4439989d098413ac777e"
     }
    },
    "a6ddfdd4787a467381cf8401d1264fa8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7758b23eabf41c5a4fa948f90405b73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaabdb5cdca3447d880d5d60576e0664": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aacf253901d14d58b5eabf27a80a131e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "ab9d39d5aec34b34aa5dc135385ab51f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac0ec675aa4f4de7b7a07c217476e1ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "acba547bead4495ca0cb675070d18ce4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad52b9c845874da69b591ac6b4df1e22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adb3cbd075684381b4f02b69fd762b3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae23eecee81348999c8862c4c929e5a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af901ab329c0414ebd2c243768826c82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "b170f87f3eed49539b296cb04bb1327c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3fca54d08eb4439989d098413ac777e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b53672c8f0c5421b927ac73110175af1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3800772f0bc74a308dbc1939d06f8d4b",
      "max": 18074646,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e88cf4d0dd0f4decb820965372afa113",
      "value": 18074646
     }
    },
    "b5ed3f6d70ee4608ab38ac2fbe78e943": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb40cf7eebaf4628a2d6d5d01dbef583": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97a8756ce6224f379223ef76052d73ce",
      "max": 140250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_86cc4d44365a4e5aa9b6fa27d3c0932e",
      "value": 140250
     }
    },
    "bc864feb25be4f798b18ad5af700e260": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bca867bc93f8414b92045a5edb6d804a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adb3cbd075684381b4f02b69fd762b3e",
      "placeholder": "​",
      "style": "IPY_MODEL_fa7d7885b28446f58bbc264b8a305a4e",
      "value": " 488/0 [00:00&lt;00:00, 4879.43 examples/s]"
     }
    },
    "bd227b6e98a348f8886aed0bfc4ba94d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f1a4244327be4edfab6c20a940b40579",
       "IPY_MODEL_19c1677a88a6439e8ee983512672b5f0",
       "IPY_MODEL_f1be400a1273449ea6179e6a3229b136"
      ],
      "layout": "IPY_MODEL_0a61fbf753d644acbcbab8cf37bfb227"
     }
    },
    "bd80be291e7546c0886f7938283d22bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf55263c138c45daabba0498c79056aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_204178f1f7bd4ab892fa5bdf47909dcb",
      "max": 188396,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6309d69d64d4ac9998481790cbca063",
      "value": 188396
     }
    },
    "bf93c428bd484902935a22cad8c8e3dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0d5d96469e5d490aa205e73477a0b424",
       "IPY_MODEL_d23ef3166bec4aeba9bf4ff6a51d0f4c",
       "IPY_MODEL_08b737b565784db299378bad41c3453e"
      ],
      "layout": "IPY_MODEL_9c4ddccfde3c41d79c700d89ed863ce7"
     }
    },
    "c14ec55597d9413a93670c0523baf8d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1f9fa74a4284eb388375f2479fa64c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_298eaf75350e4a918dc7e859ada452b3",
       "IPY_MODEL_587548fc703142a78f10a394ceb37597",
       "IPY_MODEL_1f6be19d0d4a48118220d1a880d0745d"
      ],
      "layout": "IPY_MODEL_d8cfbf6182774960a9b1024519afc926"
     }
    },
    "c238e479b18c44839d46cc557c1be27d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c33ff826c05b4ec6aa87edfb75228364": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acba547bead4495ca0cb675070d18ce4",
      "placeholder": "​",
      "style": "IPY_MODEL_23f4f10e6a4a400cafecba96e89384a7",
      "value": " 3/3 [00:00&lt;00:00, 49.30it/s]"
     }
    },
    "cea2bf9c6dd54056b4cc854edd9273ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d082f074220f4f96a2f4f401d06fa5f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c14ec55597d9413a93670c0523baf8d3",
      "placeholder": "​",
      "style": "IPY_MODEL_3197e91e6e234c18b5b9822de3d1f231",
      "value": " 174443/174443 [01:22&lt;00:00, 2405.22ex/s]"
     }
    },
    "d089b50fd8624d0e916a8052786ca6d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0b0ecf86aa14e068d4b963f1505cca6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d155c2fc2e05415aa4a86ca814998457": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2343705bce74db382439479af490d7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d23ef3166bec4aeba9bf4ff6a51d0f4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6ddfdd4787a467381cf8401d1264fa8",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8894c6cd80ec4a9990aabba67edfda5d",
      "value": 3
     }
    },
    "d3f7d48f24ee4efa91018c2ad43c619b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1930c66806644318d3893a34286939a",
      "placeholder": "​",
      "style": "IPY_MODEL_9528cd36b35a41fbb8eb21910f172b4a",
      "value": "Generating train split: "
     }
    },
    "d4cd39762dbe4ae5b2ad9d943c7f4151": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5ee389e7e344fc48ed328810fe91f51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d089b50fd8624d0e916a8052786ca6d5",
      "placeholder": "​",
      "style": "IPY_MODEL_ae23eecee81348999c8862c4c929e5a8",
      "value": " 140k/140k [00:00&lt;00:00, 344kB/s]"
     }
    },
    "d8bea9ed290e4e9093280ecf30e40e76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_11208e71ca224360bb3a19428d1666e0",
       "IPY_MODEL_e811ab05b4004f8c9ca5db7d5653d365",
       "IPY_MODEL_8b28cd538fdc4de8bd1715d7da4f9660"
      ],
      "layout": "IPY_MODEL_2abcc7810447406c8a0fa08dc0a4fb3e"
     }
    },
    "d8cfbf6182774960a9b1024519afc926": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da4d46bd68264cd29bb8ec539b7bb9ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e440076625164c259a1467279035e1c5",
       "IPY_MODEL_a20a447b39f84312a0eff1887bb9ed44",
       "IPY_MODEL_d082f074220f4f96a2f4f401d06fa5f9"
      ],
      "layout": "IPY_MODEL_e2127613ba9c407b84c9e3ddd48d9313"
     }
    },
    "da8728572b1d401abe579368f65155ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_60ce6bd2a23e416fa16bfa98376a31bd",
       "IPY_MODEL_1664592b61e14e4b9c87150580e573b6",
       "IPY_MODEL_6727165b8edc4733a4f7ce08fc7b9da5"
      ],
      "layout": "IPY_MODEL_2a2636cc0c4647bbbbf1d0d261376f35"
     }
    },
    "db082aaeff6e4f9db14c580993e859f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd7047324fc1435098c471496b2285e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e2127613ba9c407b84c9e3ddd48d9313": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2fcba7e77094c9ab93783addebbca12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e440076625164c259a1467279035e1c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb24103b1bbf48289de19c0f05acea1a",
      "placeholder": "​",
      "style": "IPY_MODEL_72ad75a111624935ae72eb30d8e2243a",
      "value": "100%"
     }
    },
    "e5fbd6b53bb94051b4235a57853591f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6d5e11a804e4fd7a01fc7c1e7700a5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "e811ab05b4004f8c9ca5db7d5653d365": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_462ccf11cf0246f28046ff219bf0db8e",
      "max": 13603614,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5521232d155e463e8c6d79503423895b",
      "value": 13603614
     }
    },
    "e88cf4d0dd0f4decb820965372afa113": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e8d9d6b9ad2e42819beaa2dfdd52a63f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e99eafed62824d1aa6cc17c5c3f6a8c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eb24103b1bbf48289de19c0f05acea1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec3867f8184a447bb304ef26a53095fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec5bac39bb254f7d8690d7aef2984b86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1a4244327be4edfab6c20a940b40579": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc864feb25be4f798b18ad5af700e260",
      "placeholder": "​",
      "style": "IPY_MODEL_96ea1ea254e64e618df32643a7c923dd",
      "value": "Downloading data: 100%"
     }
    },
    "f1be400a1273449ea6179e6a3229b136": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f796fe15ff6a4127a6288fad1f0c5331",
      "placeholder": "​",
      "style": "IPY_MODEL_3d562f2f6cdf4dfdaffac3e0eb9bf213",
      "value": " 184k/184k [00:00&lt;00:00, 344kB/s]"
     }
    },
    "f6309d69d64d4ac9998481790cbca063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f696be6a65924089b85589ecc2cddf61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "f6c2958faa4e45c784eb60165f674a71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f796fe15ff6a4127a6288fad1f0c5331": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa7d7885b28446f58bbc264b8a305a4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd00a0a4c0724f3080a5de179c3ca880": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff5a3824f6e94689be3bf83eed88302e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
