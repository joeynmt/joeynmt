name: "my_experiment"

data: # specify your data here
    src: "de"                       # src language: expected suffix of train files, e.g. "train.de"
    trg: "en"                       # trg language
    train: "test/data/toy/train"    # training data
    dev: "test/data/toy/dev"        # development data for validation
    test: "test/data/toy/test"      # test data for testing final model; optional
    level: "word"                   # segmentation level: either "word", "bpe" or "char"
    lowercase: True                 # lowercase the data, also for validation
    max_sent_length: 30             # filter out longer sentences from training (src+trg)
    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary
    src_voc_limit: 101              # src vocabulary only includes this many most frequent tokens, default: unlimited
    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary
    trg_voc_limit: 102              # trg vocabulary only includes this many most frequent tokens, default: unlimited
    #src_vocab: "my_model/src_vocab.txt"  # if specified, load a vocabulary from this file
    #trg_vocab: "my_model/trg_vocab.txt"  # one token per line, line number is index

testing:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)
    beam_size: 5                    # size of the beam for beam search
    alpha: 1.0                      # length penalty for beam search

training:                           # specify training details here
    #load_model: "my_model/50.ckpt" # if given, load a pre-trained model from this checkpoint
    random_seed: 42                 # set this seed to make training deterministic
    optimizer: "adam"               # choices: "sgd", "adam", "adadelta", "adagrad", "rmsprop", default is SGD
    adam_betas: [0.9, 0.98]         # beta parameters for Adam. These are the defaults. Typically these are different for Transformer models.
    learning_rate: 0.001            # initial learning rate, default: 3.0e-4
    learning_rate_min: 0.000001     # stop learning when learning rate is reduced below this threshold, default: 1.0e-8
    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)
    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)
    #clip_grad_val: 1.0             # for Transformer do not clip (so leave commented out)
    #clip_grad_norm: 1.0            # for Transformer do not clip (so leave commented out)
    weight_decay: 0.                # l2 regularization, default: 0
    batch_size: 250                 # mini-batch size as number of sentences (when batch_type is "sentence"; default) or total number of tokens (when batch_type is "token")
    batch_type: "token"             # create batches with sentences ("sentence", default) or tokens ("token")
    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches
    scheduling: "noam"              # learning rate scheduling, optional, if not specified stays constant, options: "plateau", "exponential", "decaying"
    epochs: 50                      # train for this many epochs
    validation_freq: 100            # validate after this many updates (number of mini-batches), default: 1000
    logging_freq: 10                # log the training progress after this many updates, default: 100
    eval_metric: "bleu"             # validation metric, default: "bleu", other options: "chrf", "token_accuracy", "sequence_accuracy"
    early_stopping_metric: "loss"   # when a new high score on this metric is achieved, a checkpoint is written, when "eval_metric" (default) is maximized, when "loss" or "ppl" is minimized
    model_dir: "my_trans_model"     # directory where models and validation results are stored, required
    overwrite: False                # overwrite existing model directory, default: False. Do not set to True unless for debugging!
    shuffle: True                   # shuffle the training data, default: True
    use_cuda: True                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.
    max_output_length: 31           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length
    print_valid_sents: [0, 1, 2]    # print this many validation sentences during each validation run, default: [0, 1, 2]
    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5
    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)

model:                              # specify your model architecture here
    initializer: "xavier"           # initializer for all trainable weights (xavier, zeros, normal, uniform)
    bias_initializer: "zeros"       # initializer for bias terms (xavier, zeros, normal, uniform)
    embed_initializer: "normal"     # initializer for embeddings (xavier, zeros, normal, uniform)
    embed_init_weight: 0.05         # weight to initialize; for uniform, will use [-weight, weight]
    tied_embeddings: False          # tie src and trg embeddings, only applicable if vocabularies are the same, default: False
    encoder:
        type: "transformer"          # encoder type: "recurrent" for LSTM or GRU, or "transformer" for a Transformer
        num_layers: 3               # number of layers
        num_heads: 4                # number of transformer heads
        embeddings:
            embedding_dim: 64       # size of embeddings (for Transformer set equal to hidden_size)
            scale: True             # scale the embeddings by sqrt of their size, default: False
            freeze: False           # if True, embeddings are not updated during training
        hidden_size: 64             # size of hidden layer; must be divisible by number of heads
        ff_size: 128                # size of position-wise feed-forward layer
        dropout: 0.1                # apply dropout to the inputs to the RNN, default: 0.0
        freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)
    decoder:
        type: "transformer"         # decoder type: "recurrent" for LSTM or GRU, or "transformer" for a Transformer
        num_layers: 3               # number of layers
        num_heads: 4                # number of transformer heads
        embeddings:
            embedding_dim: 64
            scale: True
            freeze: False           # if True, embeddings are not updated during training
        hidden_size: 64             # size of hidden layer; must be divisible by number of heads
        ff_size: 128                # size of position-wise feed-forward layer
        dropout: 0.1
        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)
