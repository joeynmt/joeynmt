=========
Resources
=========

If you want to learn more about neural machine translation, check out the following resources.

Toolkits
========
A comprehensive list of NMT toolkits, ordered by deep learning backend can be found `here <https://github.com/jonsafari/nmt-list>`_.

Tutorials
=========
- `The Annotated Transformer <http://nlp.seas.harvard.edu/2018/04/03/attention.html>`_ by Alexander Rush.
- `The Annotated Encoder-Decoder <https://bastings.github.io/annotated_encoder_decoder/>`_ by Joost Bastings.
- Graham Neubig: `Neural Machine Translation and Sequence-to-sequence Models: A Tutorial.<https://arxiv.org/pdf/1703.01619.pdf>`_
- Philipp Koehn: `Neural Machine Translation.<https://arxiv.org/pdf/1709.07809.pdf>`_
- `Video recording <https://www.youtube.com/watch?v=IxQtK2SjWWM>`_ of Chris Manning's lecture on "NMT and Models with Attention" at Stanford (2017).

Papers
======
- NMT papers in the `ACL anthology <https://aclweb.org/anthology/search/?q=neural+machine+translation>`_
- statmt.org `survey of NMT publications <http://www.statmt.org/survey/Topic/NeuralNetworkModels>`_
- `THUNLP-MT MT reading list <https://github.com/THUNLP-MT/MT-Reading-List>`_
